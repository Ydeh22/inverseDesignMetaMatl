{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae,cvae w/ mech dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrBu1z+PIw5KNAfQEvU13u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taechanha/inverseDesignMetaMatl/blob/main/vae%2Ccvae_w_mech_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smPa0_iVL4nm"
      },
      "source": [
        "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
        "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
        "\n",
        "a = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12.691395, 8.735944, 8.666785]\n",
        "\n",
        "edge_mat = np.zeros(shape=(27,27))\n",
        "for i in range(0, 27):\n",
        "  for j in range(i+1, 27):\n",
        "    edge_mat[i][j] = a.pop(0)\n",
        "\n",
        "test = edge_mat + edge_mat.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "481Jl_xsPhml",
        "outputId": "00722ec5-7100-4b14-8878-022041011100"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(edge_mat)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MhvjsH8qQ-y7",
        "outputId": "8da12198-bb07-4e11-f448-28891d29084b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(edge_index[5])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYyoUqTeT4xb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d28fd2-dac6-481c-ecab-30f28e3bc612"
      },
      "source": [
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = open(\"/content/dataset_2.txt\", 'r')\n",
        "data = []\n",
        "length = 0\n",
        "for i in f:\n",
        "  new = []\n",
        "  new.append(i)\n",
        "  data.append(new)\n",
        "\n",
        "  # length of dataset\n",
        "  length += 1\n",
        "\n",
        "f.close()\n",
        "\n",
        "# create dataset from data\n",
        "dataset = []\n",
        "for i in range(length):\n",
        "  new = []\n",
        "  for j in data[i][0].split(','):\n",
        "    new.append(float(j))\n",
        "  dataset.append(new)\n",
        "\n",
        "\n",
        "# trim out label from dataset\n",
        "# Ex Ey Ez\n",
        "label = []\n",
        "new = []\n",
        "for line in dataset:\n",
        "  tmp = []\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.reverse()\n",
        "  label.append(tmp)\n",
        "\n",
        "print(\"label.shape: \", np.array(label).shape)\n",
        "\n",
        "\n",
        "# create edge_index\n",
        "edge_mat = np.zeros(shape=(27,27))\n",
        "edge_index = []\n",
        "\n",
        "for e in range(len(dataset)):\n",
        "  for i in range(0, 27):\n",
        "    for j in range(i+1, 27):\n",
        "      edge_mat[i][j] = dataset[e].pop(0)\n",
        "  edge_index.append(edge_mat + edge_mat.T)\n",
        "\n",
        "\n",
        "# whole dataset to Tensor & train/test split\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "edge_index = torch.FloatTensor(edge_index)\n",
        "label = torch.FloatTensor(label)\n",
        "\n",
        "split = int(length * 0.8)\n",
        "\n",
        "x_train = edge_index[:split]\n",
        "y_train = label[:split]\n",
        "x_val = edge_index[split:]\n",
        "y_val = label[split:]\n",
        "\n",
        "print(\"x_train, y_train, x_val shape: \", x_train.shape, y_train.shape, x_val.shape)\n",
        "\n",
        "# create torch dataset\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, label, dataset, transform=None, target_transform=None):\n",
        "        self.labels = label\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return data, label\n",
        "\n",
        "# create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_data   = CustomDataset(label=y_train, dataset=x_train)\n",
        "test_data       = CustomDataset(label=y_val, dataset=x_val)\n",
        "train_loader    = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=4, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label.shape:  (2000, 3)\n",
            "x_train, y_train, x_val shape:  torch.Size([1600, 27, 27]) torch.Size([1600, 3]) torch.Size([400, 27, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "SAw3shjuITCX",
        "outputId": "7c304dd0-8c03-4868-b4fe-b1d657964ac3"
      },
      "source": [
        "x, y = next(iter(train_loader))\n",
        "print(\"tr: \", x.shape, y.shape)\n",
        "\n",
        "x, y = next(iter(test_loader))\n",
        "print(\"te: \", x.shape, y.shape, y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-46daf9160007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tr: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"te: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsJFL7_HpfY8"
      },
      "source": [
        "#x_train, val_x = torch.utils.data.random_split(dataset, [1400, 350])\n",
        "#y_train, val_y = torch.utils.data.random_split(label, [1400, 350])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udQ5sGFWSMwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815f6288-7427-46ae-f2cc-5a50a13d3306"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1  = nn.Linear(729, 256)\n",
        "        self.fc2  = nn.Linear(256, 128)\n",
        "        self.fc3  = nn.Linear(128, 64)\n",
        "        self.fc4  = nn.Linear(64, 32)\n",
        "        \n",
        "        self.fc51  = nn.Linear(32, 10)\n",
        "        self.fc52  = nn.Linear(32, 10)\n",
        "\n",
        "        self.fc6  = nn.Linear(10, 32)\n",
        "        self.fc7  = nn.Linear(32, 64)\n",
        "        self.fc8  = nn.Linear(64, 128)\n",
        "        self.fc9  = nn.Linear(128, 256)\n",
        "        self.fc10  = nn.Linear(256, 729)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc51.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc52.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc7.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc8.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc9.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc10.weight)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        h1 = F.relu(self.fc2(h1))\n",
        "        h1 = F.relu(self.fc3(h1))\n",
        "        h1 = F.relu(self.fc4(h1))\n",
        "        return self.fc51(h1), self.fc52(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc6(z))\n",
        "        h3 = F.relu(self.fc7(h3))\n",
        "        h3 = F.relu(self.fc8(h3))\n",
        "        h3 = F.relu(self.fc9(h3))\n",
        "        return torch.sigmoid(self.fc10(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 729))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "VAE()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (fc1): Linear(in_features=729, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc51): Linear(in_features=32, out_features=10, bias=True)\n",
              "  (fc52): Linear(in_features=32, out_features=10, bias=True)\n",
              "  (fc6): Linear(in_features=10, out_features=32, bias=True)\n",
              "  (fc7): Linear(in_features=32, out_features=64, bias=True)\n",
              "  (fc8): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc9): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc10): Linear(in_features=256, out_features=729, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAXUWytKnIs4",
        "outputId": "77feaad0-e5b9-4b8a-e6a8-869bf9d1dd68"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in VAE().parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "461933"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52R1zBJi6rXm"
      },
      "source": [
        "val_losses = []\n",
        "train_losses = []\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 729), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        data = data.float()\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "d = []\n",
        "r = []\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            data = data.float()\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "\n",
        "            val_losses.append(loss_function(recon_batch, data, mu, logvar).item())\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 1)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(4, 27, 27)[:n]])\n",
        "                d.append(data)\n",
        "                r.append(recon_batch)\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(100):\n",
        "        train(epoch)\n",
        "        test(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "kfk4XVUQGhlu",
        "outputId": "f93732d0-94ac-43c7-8d1d-9cdfe18ed734"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(val_losses)/4,label=\"val\")\n",
        "plt.plot(np.array(train_losses)/4,label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAFNCAYAAAC5cXZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUVf7H8fehS0eKBVCwY0VFxbrq2vuu9Wdf2xZdddVVLLtiW7ErYkNBUbBgx04RBER6k94hdAgQAqTn/P44d5iZZFqSmdzM5PN6njy3n/udSZvvPc1YaxEREREREZHMUsfvAERERERERCT5lOyJiIiIiIhkICV7IiIiIiIiGUjJnoiIiIiISAZSsiciIiIiIpKBlOyJiIiIiIhkICV7IiJSKcaYH4wxNyT7XD8ZY5YZY85IQbmjjDG3eOvXGGOGJnJuJe6zlzFmmzGmbmVjFRGRzKFkT0SkFvESgcBXqTEmL2T7moqUZa0911o7INnn1kTGmB7GmNER9rcxxhQaYw5NtCxr7SBr7VlJiissObXWrrDWNrXWliSj/DL3ssaY/ZJdroiIpI6SPRGRWsRLBJpaa5sCK4ALQ/YNCpxnjKnnX5Q10kDgBGNM5zL7rwJ+t9bO8iEmERGRmJTsiYgIxphTjTErjTEPGGPWAu8aY1oZY741xmwwxmz21juEXBPaNPFGY8xYY8zz3rlLjTHnVvLczsaY0caYXGPMcGPMa8aYgVHiTiTGJ4wxv3rlDTXGtAk5fp0xZrkxJtsY83C098dauxL4GbiuzKHrgffjxVEm5huNMWNDts80xswzxuQYY/oAJuTYvsaYn734NhpjBhljWnrHPgD2Ar7xambvN8Z08mrg6nnn7GmMGWKM2WSMWWSMuTWk7J7GmMHGmPe992a2MaZbtPcgGmNMC6+MDd57+Ygxpo53bD9jzC/ea9tojPnE22+MMS8ZY9YbY7YaY36vSO2oiIgkRsmeiIgE7A7sCuwN3Ib7H/Gut70XkAf0iXH9ccB8oA3wLNDPGGMqce6HwESgNdCT8glWqERivBr4C9AOaADcB2CMORh4wyt/T+9+ERM0z4DQWIwxBwJdvXgr+l4FymgDfAE8gnsvFgMnhp4CPO3F1wXoiHtPsNZeR3jt7LMRbvExsNK7/jLgf8aY00OOX+Sd0xIYkkjMEbwKtAD2Af6AS4D/4h17AhgKtMK9t696+88CTgEO8K69AsiuxL1FRCQGJXsiIhJQCjxqrS2w1uZZa7OttZ9ba3dYa3OBp3Af5qNZbq192+svNgDYA9itIucaY/YCjgH+a60ttNaOxSUhESUY47vW2gXW2jxgMC5BA5f8fGutHW2tLQD+470H0XzpxXiCt3098IO1dkMl3quA84DZ1trPrLVFwMvA2pDXt8haO8z7nmwAXkywXIwxHXGJ4wPW2nxr7XTgHS/ugLHW2u+978MHwBGJlB1yj7q4pqwPWmtzrbXLgBcIJsVFuAR4Ty+GsSH7mwEHAcZaO9dau6Yi9xYRkfiU7ImISMAGa21+YMMY09gY85bXNG8rMBpoaaKP9BiapOzwVptW8Nw9gU0h+wCyogWcYIxrQ9Z3hMS0Z2jZ1trtxKhd8mL6FLjeq4W8Bni/AnFEUjYGG7ptjNnNGPOxMWaVV+5AXA1gIgLvZW7IvuVA+5Dtsu9NI1Ox/pptgPpeuZHucT+udnKi10z0JgBr7c+4WsTXgPXGmL7GmOYVuK+IiCRAyZ6IiATYMtv3AgcCx1lrm+Oa3UFIn7IUWAPsaoxpHLKvY4zzqxLjmtCyvXu2jnPNAFyTwzNxNVPfVDGOsjEYwl/v/3Dfl8O8cq8tU2bZ71mo1bj3slnIvr2AVXFiqoiNBGvvyt3DWrvWWnurtXZP4K/A68Yb0dNa29taezRwMK4557+TGJeIiKBkT0REomuG63u2xRizK/Boqm9orV0OTAZ6GmMaGGOOBy5MUYyfARcYY04yxjQAHif+/8UxwBagL/CxtbawinF8BxxijPmzV6N2J67vZEAzYBuQY4xpT/mEaB2ur1w51tosYBzwtDGmkTHmcOBmXO1gZTXwympkjGnk7RsMPGWMaWaM2Ru4J3APY8zlIQPVbMYlp6XGmGOMMccZY+oD24F8YjehFRGRSlCyJyIi0bwM7IKrvRkP/FhN970GOB7XpPJJ4BOgIMq5lY7RWjsbuB03wMoaXDKyMs41Ftd0c29vWaU4rLUbgcuBXrjXuz/wa8gpjwFHATm4xPCLMkU8DTxijNlijLkvwi3+D+iEq+X7Etcnc3gisUUxG5fUBr7+AvwTl7AtAcbi3s/+3vnHABOMMdtwfS/vstYuAZoDb+Pe8+W41/5cFeISEZEIjPu/JSIiUjN5w/XPs9amvGZRREQkk6hmT0REahSvid++xpg6xphzgIuBr/yOS0REJN1UZMQtERGR6rA7rrlia1yzyr9ba6f5G5KIiEj6UTNOERERERGRDKRmnCIiIiIiIhlIyZ6IiIiIiEgGSus+e23atLGdOnXyOwwRERERERFfTJkyZaO1tm2kY2md7HXq1InJkyf7HYaIiIiIiIgvjDHLox1TM04REREREZEMpGRPREREREQkAynZExERERERyUBp3WdPRERERERqt6KiIlauXEl+fr7foaRUo0aN6NChA/Xr10/4GiV7IiIiIiKStlauXEmzZs3o1KkTxhi/w0kJay3Z2dmsXLmSzp07J3ydmnGKiIiIiEjays/Pp3Xr1hmb6AEYY2jdunWFay+V7ImIiIiISFrL5EQvoDKvUcmeiIiIiIhINWnatGm13UvJnoiIiIiISAbSAC2psHIytOgAzXb3OxIREREREUmhHj160LFjR26//XYAevbsSb169Rg5ciSbN2+mqKiIJ598kosvvrjaY1Oyl2w9W5Tf17wDXDEAOnSr/nhERERERCRlrrzySu6+++6dyd7gwYP56aefuPPOO2nevDkbN26ke/fuXHTRRdXetzClyZ4xZhmQC5QAxdbabsaYXYFPgE7AMuAKa+1m4175K8B5wA7gRmvt1FTGlwolDZpTt3Br+M6tK+GdP4bv2/NIuGko1GtQfcGJiIiIiGSwx76ZzZzVW+OfWAEH79mcRy88JOrxI488kvXr17N69Wo2bNhAq1at2H333fnXv/7F6NGjqVOnDqtWrWLdunXsvnv1tvyrjpq906y1G0O2ewAjrLW9jDE9vO0HgHOB/b2v44A3vGVa2XfrmzRnGzMb3Rb7xNXT4Mm24fuu/hSatoM9u6YuQBERERERSarLL7+czz77jLVr13LllVcyaNAgNmzYwJQpU6hfvz6dOnXyZdJ3P5pxXgyc6q0PAEbhkr2LgfettRYYb4xpaYzZw1q7xocYK+3zv5/ApW+Mo1P+h2H7l/3nOPjiNlg8IvrFH15eft/Vg2G/M6GOxtIREREREYklVg1cKl155ZXceuutbNy4kV9++YXBgwfTrl076tevz8iRI1m+fLkvcaU62bPAUGOMBd6y1vYFdgtJ4NYCu3nr7YGskGtXevvSKtk7eu9WLOt1Pp16fBe2/59Dsnjp6s+oVzckaZv7DXxybewCP7wifHvPo+Dy96DV3skJWEREREREquSQQw4hNzeX9u3bs8cee3DNNddw4YUXcthhh9GtWzcOOuggX+JKdbJ3krV2lTGmHTDMGDMv9KC11nqJYMKMMbcBtwHstddeyYs0xb6ZsZpvZqxmWa/zgzu7XAg9c6C0BAq2wqtHw47s2AWtngqvHB7cbt4Bjr4BTrgTCnJh4wLodGJqXoSIiIiIiET0+++/71xv06YNv/32W8Tztm3bVl0hpXaePWvtKm+5HvgSOBZYZ4zZA8BbrvdOXwV0DLm8g7evbJl9rbXdrLXd2rZtW/ZwjTd4chaupWqIOnVhl1Zw/xKX/P1rDhzyZ2jePn6BW1fCyKfgqd3g+f3gvfMgPyc1wYuIiIiISNpIWbJnjGlijGkWWAfOAmYBQ4AbvNNuAL721ocA1xunO5CTbv31EnH/ZzO56b1JsU9q0R4ufxfumQMPrYYrB0LnPyR+k157uSkglo+DHZuqFrCIiIiIiKSlVDbj3A340ptLoh7wobX2R2PMJGCwMeZmYDkQ6JT2PW7ahUW4qRf+ksLYfDVy/obET27QxDX37HIhFGyDwu0w5nmY2Df+te+eG1y/qA8sGu6aezZpDa06VThuERERERFJH6Zck8I00q1bNzt58mS/w4io7AAtkcz471m0aFy/ajf6/TP4/OaKX9dTTT1FREREJP3NnTuXLl26+B1GtYj0Wo0xU6y13SKdr/H8ffTokFlVL+SwyyqXuPVsAYMuh23r3cAuIiIiIiKSUZTs+eir6asZPCkr/omJ+O8mNyVDRSwcCs/vD093cMmfiIiIiIhkDCV7Prv/85nMWpWEJpV16sIhf4I7p8N9i4L7D7448TJ6toBXulY9FhERERGRWmLLli28/vrrFb7uvPPOY8uWLSmIKEjJXg3w/ND5ySts187QtC3cvxQeWgNXvA8XvJz49ZuXwhPtYMFQGPUM/PIsFOUnLz4RERERkQwSLdkrLi6Oed33339Py5YtUxUWkPpJ1SUBa7akIJlqvGtwvdtf3Bck1lyzpAA+vDy4PfJ/cPZTsN8Z0PbA5MYpIiIiIpLGevToweLFi+natSv169enUaNGtGrVinnz5rFgwQIuueQSsrKyyM/P56677uK2224DoFOnTkyePJlt27Zx7rnnctJJJzFu3Djat2/P119/zS677FLl2FSzVwPMX5fLG6MWV8/N9jujEhdZ+OkheO1YWDTCJYx9T0t6aCIiIiIi6aZXr17su+++TJ8+neeee46pU6fyyiuvsGDBAgD69+/PlClTmDx5Mr179yY7O7tcGQsXLuT2229n9uzZtGzZks8//zwpsalmr4Z45sd5/O0P++DNS5g6135etcFYBv7ZLVdPTU48IiIiIiLJ8kMPWPt7csvc/TA4t1fCpx977LF07tx553bv3r358ssvAcjKymLhwoW0bt067JrOnTvTtasbO+Poo49m2bJlVY8bJXs1ytqt+ezRourVtXFd2BuKC+DIa6CkCJ7Zu3Ll/PIsHHNLeJNREREREZFarEmTJjvXR40axfDhw/ntt99o3Lgxp556Kvn55btwNWzYcOd63bp1ycvLS0osSvZqkMXrt1dPsnf0DeHbPXOgpBieaB35/GhGPuW+/vYr7HYIpLpWUkREREQklgrUwCVLs2bNyM2NPG91Tk4OrVq1onHjxsybN4/x48dXa2zqs1eDXNtvgn83r1sPrv8aDji34te+eSIMuiz5MYmIiIiI1HCtW7fmxBNP5NBDD+Xf//532LFzzjmH4uJiunTpQo8ePejevXu1xmastdV6w2Tq1q2bnTx5st9hRNSpx3eVvnbO42fTuIGPla6Pt4bS2EPFxnTjd9ByL/clIiIiIpJCc+fOpUuXLn6HUS0ivVZjzBRrbbdI56tmrwaaviK1kyvG9bexVbv+vfPh5cNg4tvJiUdERERERCpMyV4N5Htda7su8PBauKaKQ75+fx8URG6/LCIiIiIiqaVkTyKrvwvsfwbcPrFq5ayeDjkrYV7lm7WKiIiIiEjFKdmrga55ZwIj5q7zOwyn7YFw00+Vv37ABfDSIfDx1TD7q+TFJSIiIiLiSedxSBJVmdeoZK+GunlADRp4Zq/ucNaTcNzfq1bOpzfEP0dEREREpAIaNWpEdnZ2Rid81lqys7Np1KhRha7TPHuSmBP+6Zbn9oKeLSpfzo5N8FsfOO1hqFM3ObGJiIiISK3VoUMHVq5cyYYNG/wOJaUaNWpEhw4dKnSNkj2puHOfhR/ur9y1z3Z2y4bN4aS7g/tLS8DU0cTsIiIiIlIh9evXp3Pnzn6HUSOpGWcN9uqIhfQbu9TvMMo77q/w0Go4qgrNMoc/CtmLYf1cKNgGj+8Ko59PXowiIiIiIrWckr0a7IVhC3ji2zl+hxFZgyaw72lVK+PVo+D17pC3yW1Pea/KYYmIiIiIiKNkTyrvkD/BPXOrXs7QR7yVzO1UKyIiIiJS3ZTsSdU03xPu/h26Xlv5MuZ8nbx4REREREQEULInydByL7jkNTj4kqqVk781OfGIiIiIiIiSPUmiKwbAbb9U/vrCXMjb4taLC6CkKDlxiYiIiIjUQkr2JLn27ApHXV/565/ZG+YMgSfbwWvHJi8uEREREZFaRsmeJF+LjlW7fvB1brlpSdVjERERERGppZTspYFr3hnPrFU5foeRuJPu8TsCEREREZFaT8leGvh1UTaPfDXL7zASV7ceXPKma855fxUnhd+8HIb3BKtpGUREREREKkLJXpqoY/yOoIK6/h9c9Co03hUOv6ry5bxyOIx9CdbNTl5sIiIiIiK1gJK9NDF1xRa/Q6i8P78FzdtXsRDV7ImIiIiIVISSPakepoo/am+eBDs2ufWNi8KbdZaWQt7mqpUvIiIiIpJhlOxJ9TBJaIc663NYMgr6HA3TPwzuH/0cPNMJtq2v+j1ERERERDKEkj2pHodeFlw/5pbKlfH9ffD+xW59zfTg/rlD3HLbusqVKyIiIiKSgZTsSfU4/T/Q7mC3XtV5+AByVlW9DBERERGRDKZkT6pHnTpw7Rfwx0fh+NurXt7871yTzoGXgi2tenkiIiIiIhmmnt8BSC3SfA842Ztw/c7psGYGfHpD5csLNOlssVfVYxMRERERyTCq2Usj2dsK/A4heXbtDIdcAg8noZ9dus1BKCIiIiJSDZTspZHeIxb6HULy1W9U9TK2rKh6GSIiIiIiGUbJXhoZ8Ntyvpmx2u8wku+qD+OfU1GrpysJFBEREZFaTclemrn/s5l+h5B8B50Pf/kR6tSvWjmfh0zp0PcP8PJhVStPRERERCSNKdmTmmHv46HLhVUrY8O85MQiIiIiIpIBlOxJzVHZydZDDX8MeraoejkiIiIiImlOyV6aySsqYfCkLHLzi/wOJfmSMVjL2BfDt2cOrnqZIiIiIiJpSMleGrr/85k8/OUsv8NIvpadkl/mF7e6+fxERERERGoZJXtpamMmzbkX0KR1asot3JGackVEREREarCUJ3vGmLrGmGnGmG+97c7GmAnGmEXGmE+MMQ28/Q297UXe8U6pji2dzVqV43cIqXXzsCQWZpNYloiIiIhIeqiOmr27gLkh288AL1lr9wM2Azd7+28GNnv7X/LOkyi25hf7HUJqdTzW7whERERERNJaSpM9Y0wH4HzgHW/bAKcDn3mnDAAu8dYv9rbxjv/RO19qqz/1hYv6+B2FiIiIiEhaSnXN3svA/UCpt90a2GKtDVRLrQTae+vtgSwA73iOd77UVkdcCUddBy33rlo51sKkfrAly23nb4Wi/KrHJyIiIiJSg6Us2TPGXACst9ZOSXK5txljJhtjJm/YsCGZRUtN0KoznHxv+L7L3q1amZ/fAt/dA+9f5LZ7dYR3zqhamSIiIiIiNVy9FJZ9InCRMeY8oBHQHHgFaGmMqefV3nUAVnnnrwI6AiuNMfWAFkB22UKttX2BvgDdunXTyBuZ5q7p5fd1OBoaNIPC3MqVmbvaLTctCe5b93vlyhIRERERSRMpq9mz1j5ore1gre0EXAX8bK29BhgJXOaddgPwtbc+xNvGO/6ztVbJnDh3ToVrPot/Xjw5q+KfIyIiIiKSAfyYZ+8B4B5jzCJcn7x+3v5+QGtv/z1ADx9ik5qqaTvY+4Sql/PSwVUvQ0REREQkDaSyGedO1tpRwChvfQlQblx9a20+cHl1xJMphsxYzUVH7Ol3GNWnQRO/IxARERERSRt+1OxJktz50TS/Q6h+jdskr6yXD4PFI2HMC8krU0RERESkhlCyJ+kl0gAulbVlBXxwCYx4PHllioiIiIjUEEr2JL00bAYHnud3FCIiIiIiNZ6SPUk/l7/ndwQiIiIiIjWekj1JP/Ua+h2BiIiIiEiNp2RPREREREQkAynZS3PFJaV+h5AZSvU+ioiIiEhmUbKX5kbO3+B3CJlhwptuuWAorJ/rbywiIiIiIklQLZOqS+oUFJf4HYK/ul4L0wdWvZyfnwBbAkMfcds9c6pepoiIiIiIj1Szl+bu+LAWTqwOcPtEuGMKXPJacsor2hFM9EREREREMoCSvQyweXuh3yFUv7YHQpv9Un+frIkw4+PU30dEREREJMmU7GWAW96f7HcI/rpvEdw0NDVl9zsTvvxrasoWEREREUkhJXsZYMryzX6H4K+mbWGv45JbZu8jk1ueiIiIiEg1U7InEsmmJX5HICIiIiJSJUr2JHMccA7s0TV55VmbvLJERERERKqZpl6QzHH1J27Zs0Vyypv6fnLKERERERHxgWr2MsSsVZoXLum+udPvCEREREREKk3JXob43/dz/Q4hs22Y75bbNsDGhf7GIiIiIiKSACV7Iol47Vi3fOVw6NPN31hERERERBKgZC9DaCyREPcthH/NSU3ZRTtSU66IiIiISJIp2csQFmV7OzVtBy3aw00/+R2JiIiIiIhvlOxliNmrtvodQs1Tt77fEYiIiIiI+EbJXobILSj2OwQREREREalBlOyJVMbKyX5HICIiIiISk5I9yVymbnLLKy0Nrm9dldyyRURERESSTMmeZK49jkhueUP+mdzyRERERERSSMmeZC5jklve9IHJLU9EREREJIWU7IlUhiY2FBEREZEaTsmeSGV8dhP0bBHej09EREREpAZRsie1R48VySvLlrjlut9hwVAoLUle2SIiIiIiSaBkL4Mc+9RwZq/O8TuMmunEu6FRi+SXO/cb+PByGN4Ttq5JfvkiIiIiIpWkZC+DrM8t4M1flvgdRs1y8etw01A487HUlD/6Obcc1xtePCg19xARERERqYR6fgcgklJHXuN3BCIiIiIivlDNXoYp1SiRIiIiIiKCkr3Mo1wvtvOeh7P/53cUIiIiIiIpp2Qvw1hle7Edeyscf7vfUYiIiIiIpJySvQwzfM56v0Oo3Xq2gOzFfkchIiIiIqJkL9MUlmiSb9+tnOR3BCIiIiIiSvakFjvg3BQVbFJUroiIiIhI4pTsZaDDHv3J7xBqvkfWw1WD/I5CRERERCRlNM9eBsotKPY7hJqvXkO/IxARERERSSnV7EntduS1fkcgIiIiIpISSvakdrv4Nbi0X3LLNOqzJyIiIiL+U7IncthlfkcgIiIiIpJ0SvYylLWaXN0/BmZ/Cds3+h2IiIiIiNRiSvYy1JAZq/0Oofbavh4+vRE+vsZtZ03UROsiIiIiUu1SluwZYxoZYyYaY2YYY2YbYx7z9nc2xkwwxiwyxnxijGng7W/obS/yjndKVWy1Qf9fl/kdQnpp0DR5Zf30kFtmjXfLfmfCq0clr3wRERERkQSksmavADjdWnsE0BU4xxjTHXgGeMlaux+wGbjZO/9mYLO3/yXvPKmkGVlb/A4hvdwxye8IRERERESSKmXJnnW2eZv1vS8LnA585u0fAFzirV/sbeMd/6MxGtZQqknzPaFnTvLLXTkl+WWKiIiIiCQgpX32jDF1jTHTgfXAMGAxsMVaG5j1eyXQ3ltvD2QBeMdzgNYRyrzNGDPZGDN5w4YNqQxfpOreOT24vnik67tnLayf619MIiIiIlIrpDTZs9aWWGu7Ah2AY4GDklBmX2ttN2ttt7Zt21Y5RpGIWu6d/DI/uMT13Zs5GF7vDvN/TP49REREREQ8CSV7xpgmxpg63voBxpiLjDH1E72JtXYLMBI4HmhpjKnnHeoArPLWVwEdvXvUA1oA2YneQyQp7l/qvs55OnX3WPe7W26cn7p7iIiIiEitl2jN3migkTGmPTAUuA54L9YFxpi2xpiW3vouwJnAXFzSF5jF+gbga299iLeNd/xnq8nipLo13tV9iYiIiIikuXrxTwHAWGt3GGNuBl631j7r9cWLZQ9ggDGmLi6pHGyt/dYYMwf42BjzJDAN6Oed3w/4wBizCNgEXFXhVyOSLNXxnEHPMkREREQkhRJO9owxxwPXEJwqoW6sC6y1M4EjI+xfguu/V3Z/PnB5gvGIpJYt9TsCEREREZEqSbQZ593Ag8CX1trZxph9cM0xpQabsnyz3yGksRTWuq2f55aaWUREREREUiihZM9a+4u19iJr7TPeQC0brbV3pjg2qaLHv53jdwjpK5VNLBcNc8uSwtTdQ0RERERqvURH4/zQGNPcGNMEmAXMMcb8O7WhifhJ/elEREREJL0l2ozzYGvtVuAS4AegM25ETpHM1O4QvyMQEREREamSRJO9+t68epcAQ6y1RajqQzJZ2wNSf4/C7W75a2+Y8Fb4sYXDIGdl6mMQERERkYyVaLL3FrAMaAKMNsbsDWxNVVCSHDOytvgdQnq7fSJc92Xqys9eDPlbYdh/4If7w48NugzeOiV19xYRERGRjJfQ1AvW2t5A75Bdy40xp6UmJEmmnLwiWuxS3+8w0lPbA91XylgY+Ofoh3dkp/DeIiIiIpLpEh2gpYUx5kVjzGTv6wVcLZ/UcAPHL/c7BIlm7jewclJwO3ct7NjkXzwiIiIiklESbcbZH8gFrvC+tgLvpiookVrphQPh2c5+RyEiIiIiGSLRZG9fa+2j1tol3tdjwD6pDEySY01Ont8hZI7rv4b9z/I7ChERERGRhCSa7OUZY04KbBhjTgSURaSBgeNX+B1C5tjnVLjmU7+jEBERERFJSKLJ3t+A14wxy4wxy4A+wF9TFpVITXbe835HICIiIiISV0LJnrV2hrX2COBw4HBr7ZHA6SmNTKSm2KNr+PYxt/gTh4iIiIhIBSQ09UKAtTZ0br17gJeTG45IDXTDN5C7JrhtjH+xiIiIiIgkKNFmnJHoE2+ayNlR5HcI6a1R8/Lz7TVomrr7TXondWWLiIiISK1RlWTPJi0KSakPxi/zO4TMc+3n0HT31JT93b2pKVdEREREapWYyZ4xJtcYszXCVy6wZzXFKFLz7NUdLlUNnIiIiIjUXDH77Flrm1VXIJI6E5dt9jsEqaz8HGjUwu8oRERERCQNVaUZp6SJ0Qs2+B1CZjLV8OvzTKfU30NEREREMpKSPZHK2qt76u9hS1N/DxERESS/uSIAACAASURBVBHJSEr2UqRR/ToaoT/T1anrdwQiIiIiIlEp2UuR6f89i7mPn+N3GJJqPXPg+Dv8jkJEREREpBwleynSqH5dGtWvyw93ncyY+0/zOxy2FxSzInuH32FkptAq3DoxxzwSEREREak2SvZSrMsezem4a2P+eso+9L3uaN/iuPqdCZzy3Ejf7p/RbMiUk//NTn75W1cnv0wRERERyXhK9qrJg+d14cyDdwNg/3ZNq/3+M7K2VPs9JUle7OJ3BCIiIiKShpTsVSNjDJMfOYM3rvWvhk/S1MBLYftGv6MQERERkTSiZK+atWnakP3aNaXvdUfz5CWHevsaANCsUer7exUUl6T8HpICi4bDc/u65eppbt+qqdCzBWxa4m9sIiIiIlIjKdnzyVmH7M55h+1Bm6YNeO8vxzL/yXM4ttOuKb/v2pz8lN9DUmjgpdD3VLc+fZBbLhrhWzgiIiIiUnNp6EAf7dqkAZMfOdPvMEREREREJAOpZq+WMWim96Q75T633HXf6r1v6CigIiIiIiJlKNmrQS48Ys+U38Mo10u+XVrBo1vgn1Oq7569j4QFPwW3hz+mvnsiIiIiEkbJXg1yyZHtWfr0eXRotUtK77MmJ4+ZKzUVQ1IZU72Z9KYlsHWlW5/3LYx9ET66uvruLyIiIiI1nvrs1TCmGhKGE3v9TKmFZb3OT/m9pBosGeWWpUW+hiEiIiIiNYtq9mqhUnX1qh6HXV799/y1N3x4ZfTjfU+FCX2rLRwRERER8Y+SvRqobp3U1e6d/OzIlJUtZXS5qPrvOew/sODH6MdXT4Mf/l31+wy63M3xJ+VN/wjm/+B3FCIiIiJK9mqid288hr+eso/fYUhVdbmweu8XbXTOgm3JH7lz4dDklpdJvvobfHSV31GIiIiIKNmrifZp25QHz+uS8vvkF5VwXb8JTF2xOeX3qpWMgcOu8Ofe390Hb50C2zbA0+3dAC4iIiIiUqso2avFjnpiGGMWbuTOj6ZFPL6toJj3fl2K1XxulXfBS3DZu9Vzr02Lg+uT3oY1MyB3tdueXE0xiIiIiEiNoWSvFttRWBLz+GNDZtPzmzmMWbixmiLKEP8YD5e86dYbNoVD/+xfLCvGu2VOln8xpJq1sGqq31GIiIiI1DhK9iSqzTvcUP75RbGTQimjXRfo+n+Rj/XMqd5Y1v4eXJ/5afXeu7pMeRfePg0WqB+hiIiISCglezVY/brVN0l39rYCOvX4jkETlie9bGsty7O3J71cSUDovI1DH07smmkDYd3sit8rbzPs2FTx66pq/Vy33Ly0+u+dLAuHwdLRfkchIiIiGUbJXg32w10n8+Qlh1bLvVZs2gHA4MkrAdi0vZDhc9cBcNsHU+g31n2Qnrd2Kyuyd1So7HfGLOUPz41i9upqrtUSmPp+cL0wJOHOWQVjXw6O0jn+DXj7j7B1DXx9O7xxQvQy86N8H5/pBM92rnLIFVa2T+n2FDc7thbyt5bfP+BCGHx95cocdJm7XkRERCSJlOzVYPu1a8a13fdO+X1Wbs6j7BAsfX5eFLbd30v2znl5DKc8V7G5+iYtc7U9WZsqliRmlH9MgDsm+xtD4bbg+ifXwPBHIdsb1OXHHrBqMrx4UPTrsybBE22h117lj8UbxGfNDMhdm1icRXmRk6moAvc2sOxXeG5fmDMk/JRt62HjwtjFFG6HL/8ev3ZywpvQqyNsWRG+f+lomPN1cDtvM3x4FWzPTuhViIiIiCSbkr00MOSOE3nlqq7Vcq8ZWVv4bXFyP5ya6muNWnO1Owja7O93FEGrvRFYf30p9nlF+TC5PxTkwrjeUFIY+bxFIyLvz5oEs75w00C8fHjse5UUuxrGV45wyVRFGRN8XYGBaQJe7AJ9usGambBpSeTrp7wHMz6EX56NfZ+537rl5jhNnie9Awt+gPGvxQ1dREREJBWU7KWBwzu05OKu7VN6jzsGBUcz/L+3x8dN0EpKKz4dg2Zw8Dy0Guo29DsKZ+WU6MeKC10/v2//BU93gHL1vyF2lGk6mb3Y1c71OwM++4vbV1IQO5bpA10N47Z1EcrfBD1bwPg3yx8L+8EKWQ/05QMoLXbLt06G3kfGjsMYyFkJxdHiTfAHWT/vIiIi4rOUJXvGmI7GmJHGmDnGmNnGmLu8/bsaY4YZYxZ6y1befmOM6W2MWWSMmWmMOSpVsUl5q3PyK3R+RWr/DC5z1GdfT4MmsN8f/Y7C2TAXxrwQ+diTbV2tXiy562DdHKDM04FXj6p47VxBbvh2SZFLRgu2ueQLYNoH0a9fPS2Y+M37Fl7vDrM+j3/fonx3XeDakkJ46RD46u/lz83fGjwvbpW1DZafSZ7dF57c3T0MEBERkRotlTV7xcC91tqDge7A7caYg4EewAhr7f7ACG8b4Fxgf+/rNuCNFMYmVVTifeDdsqOQwuLSsGObtxey/8PfM2GJSwjVjDMCWxr/nOoy4vHox0LjjFQ1+8KB8Mbx4d/kWH3j+p3tmmtGMuzR8O0N8+Cd0+Hzm4P71s2KXvb0QcH1LV4Ty9DavUjyc+Cp3WBUr+C+EjflCPN/CD/30xtdArtiXOwyA3YOfpNhzTh3bITiPJj5Sezzpg0Kf18z2fp58Zv1ioiI+CBlyZ61do21dqq3ngvMBdoDFwMDvNMGAJd46xcD71tnPNDSGLNHquJLR5MfOYPfHjy9Wu5VNj9btSWPN39ZXO68ro8P45b3wwcemZa1maISG/F88TRu7XcESRIyOEpAn27RT88aD9uiDNRiy8znuPhnt1w1hbB64cKyA/2EHItU8xdrGonAyJ0zPy7/VKJscjv7yzIXx3mKEYg/lm0bose3cBj89DCUxnkwkLcZRj8X/7zKKMoL1qqWVfb7VdbX/4BRTyc/ppro9ePglTh9UpOlcHvNH/Qnb4tL9Es1R6uIiN+qpc+eMaYTcCQwAdjNWrvGO7QW2M1bbw9khVy20tsnnjZNG7JHi12q5V6RauN6/TBv57oN+SA8esGGiGWUrQcKXLJ+az55hVX7EFBaarn1/ck7aw/TTr0a0mcvWbavT/zcV4+Gr/4R/7xh//XKLvvzVeYnKzQp27igfDmxppHYyYQ0zwz8WfS2Ny9zU1VEUlwI398f+cN3/pbg+pgXIteMvn5c5PiKC9x0DL/1iZ80fncv/Pxk5PO2b4TBN1RwdNMQH1/tmrRGlGCVfWlJ8uYQXD/PTQ8Sz4yPa05CtCXL9Tdd+3tyynvrFHhun+SUlSo/PugS/bK1434b+TSsmOB3FCIi1SrlyZ4xpinwOXC3tTbsE4d1GUOFunIZY24zxkw2xkzesCFykiFVV1QS+9uytkwfv3s+mc4XUyPXAJRNHI/93wi6/PfHKo36uXlHIcPmrONvA2MMMFKTHXG1W/5zauzzapKyfepC/fRQ4uUU54c3uUxE6Nx+thSWjU3suoqMChQoM/ADG7j2lSPgpYPLn28MzB0CE9+K//pHPF5+qgaAHVF+B0pDmrqWFkWJ91dYOiY4f2KkkVJHPwdzvoJpA2PHF02sRDNa++yCXFfbGDDmBTeH4JJfKhdDqNePiz49SO466H+OGwH2y7/CpzdU/X7JMP97t5wyILhv9TT45NroTZpjyV4U/xy/FXk/k9F+dv3ySy/of5bfUYiIVKuUJnvGmPq4RG+QtfYLb/e6QPNMbxmoElgFhI7o0MHbF8Za29da281a261t27apCz5NNG5QNyXlvjduWczjPb74PaxG74tpq7hn8Iywc8p+FLRl8vq+oyvfzDNQUp107RDY8RjomQOt94UuF/kdTWKWJuHDelkzPglPDKIJnXD86Q7w3vkw7ztvRxKG/tm81E2TALBouFvGGz109bRg085EPtSWbfaYNSnGuTZ8ffTz5Wvn3jsPBlxA1Bq2paOD/Q+jvUfrZsOIJyo5VG6U+z5/IDzTKbgdqG2NNMpqZfXaC949P3zfpLdhxW9uHsRk3y9Rq6fDhvnh+yIN6PPZTTD3m2DfUvHPliz4tbffUYiIpEwqR+M0QD9grrX2xZBDQ4DAI9cbgK9D9l/vjcrZHcgJae4pEYz+92mMfaB6+vBFcn3/iTGPB3tzeaNxJvh5srTUlhv0pdw5XmEmXZO9UFfGGGEyk62bA1/e5hKDsS9X/PpofclCRfv5KClyNU6f31L+WKQauEh+esiN+hlN2R/4stNG9Dsj8nVLR8MHfwpuz/8efn7CTYMRU8j91v7uEuTJ/dx2tIni+58DY553tYOf/gU++HOwn1VeSDPU4gKY/qFrjhhPoFYnkuJCeLyNKyfe5PWx5OfA8ji1uxVJYK2FH3q4mtKFwysfV98/wGvHli3cW2bA36pEpdM8Ox9eAcP+k9jfExGRNJTKmr0TgeuA040x072v84BewJnGmIXAGd42wPfAEmAR8DaQQKee2m2v1o3ZtUkDv8MIU1AcXnsxa1UOP89zlbdbdhQydUX8WpzHvpnNAY/8QGmMufwCnyXq+PT5yVrL3DUV7wf1+ZSV5ZrA1lpvHB9cH/5o9POi2bHJTcsQS6R5+QBGPuWaVq5OUjPaSB9ui/PCtye+5WrSBl8PX98evaxPb4SVIQ9SAs0zV06OeHrEhHZcn/DtMc9HvjZQ82cMzP4CFo9wzQshvM/hx1dHnoqiorIXBWtBJ/WD6R95A/D4LG8zTHjD1ZQOuhRePix5ZSc8VUcmSsFrLkly09BA8/SaNEJyRZSWwkuHwszBfkciqTT1fVgbYzRqkRhSORrnWGutsdYebq3t6n19b63Nttb+0Vq7v7X2DGvtJu98a6293Vq7r7X2MGttlE82UpOVTWQueHUseUUuAfzP17P58+vhw9Zv2VHIrFU5Yfs+GO+aNsV6Nhys2atiwJXUb+xSzn1lDJOWJV47kZtfxL2fzuCad8anMLJa5Jde8HR7mPJe9HMKo/QzHPtScmOJNAF7pBrC9XNhztex+9CV+zDr/ZCvnxP5nC3euFahCefMj2OGW17IL9L872FVmSR4UYTartIiN/jLxjh9yELjCm0KPPJJ+Opv8HY1tU5YMsp9YArI3wo/POBGHP2xR/i5W1a4Bwllax9LS9yH6ngjn25dE3KvJNXsrZuTWG3o0tGVrzUtKa5cP8Ko4tTwbVpasVFk18+DJ9pEGBm3FivOg5wsGHJn5a7PXev+Lr3SFbIXu9+TN09OflItVTPkn/DmieH7SopgWwUGR5Naq1pG45Ta4w/PjeLbGa717YpNZYfID1dQXErXx4dxwatjWbKhfA3Nmpw8Vm/JY97a8jVopTtr9sI/QC3duJ0xC4N9CWdkbWGfB79j/daq16ZZa9maX8SqLXk8+Z2bvy0rzmsMi9n7TLMhN05fMKk54s3TFzD/u/jnQPicgdEUlPl5j5S4/RTSpHOdN8pjaZEbGTRWX8BQhTuI+mH87dPiX5810Q3+8u3dwX0xEwVTPqkqa8emyIllRUR7AvT+xe4DU8CYF1z/vsn9I88Z2LsrPNs5fN/k/vDFrTClf+wYBl3m7jWuj5tCA0JGeS1j7SyY/VXs8sDVhL8TpelvQGmJa777fiX7AT/ZDnofWblrQyXyFG7DfPcej30h8XLXTHfLWKN8bs+OMD1LDOnU5DSiwHtdydfxwoHwenfXb3liX/dzu3YmbI0yAnFNU1yYAd/DSvrq7/D8/olNcVJSHP4+9T0Nntwt+vk1kbVu5OlE/y/LTkr20tBnfzueIXecGP9En3wxzf2TWLIhRt8dYEfI9Aubthd612zbmcid8uxITuj1M+e8PGbneeMWb+S3xdk7m3iWTfZOe34U1/WbSL5Xm/jeuGWUWhi9cGPVXhSuxvHwnkO5uM+vO/dV5n9MLf23lJ5e716990v0Q+qyMeX3jXjcNRWN1hcw1LSB8L893MioAKsq05DC+90L7ds36n/lT9s5SmiMn/xxfdwHloGXuq91c2DxSNe3b0Jfd862Soy+nL0Yfn0lfN/WNTDz0+CAOaVREtTAlB8bF8Ksz10/wcC+7XH+nmxa4pZDH4YlI916tATozRMTHzl0U5xBrQJ/kAJzN058O7w2Mx5bAjkJ9lkta+UUl2gWF8T+w1hS5D6s/XC/214+Lvq5lfHcPvD8AW6Kis3LoSjfjdQal3Ej8s75Ov6pn1zrppCpCarSvKW650HM3wpbVyevvJxV8GRb9xCmNgrUcMdrglywDZ5oDb88E9y3emrw73+6KNjqRpjuf47fkaQdJXtpqFunXTm8Q0u/w6iySB8HruwbbOIYqcve1W9P4P/eHl+uG8yi9bn87YNg35/7P5vJp5OD0zbe92n4SKHbCop5+Mvf2V6QWJOlhetyef4nN8rexm3Ra+ZmrtzCRxMr+WFJ5H97VP7aQIIRTd9TXc1QSXH5PoOrp5U/P1AjFc+6kPnjIvW/mzvELX//LHoZQx+GGR+5xApcLdYHl7j1QLL2zh/LX1dc6KZ0CO2jGJq8vXe+m68xdOqOFw+CL24JJtbxPij16eZGz/wypM9iaDKTu7b8NdFq8RK1eblLUOb/AJPeSfy6stOGfH9feG1mNAuHRX4d0eRtdt/P0CTqndNdE9Jhof1vIyQiPz3sHqIsGZX4/SqqMBfWzHAT3X9yDbxwQGLXvXe+61NblBf8mel3thsNN9Tcb1z/0xXjqzagTyS568J/XsE9tIiXFJcdwXf5uPJJ95oZ4bUi3/87ehmp8OZJ8GKX5JUXmIbE72a9S0cnPrBXIj65Ft45M3nlBfpfV+TBT01mS91Dg4GX1ry5PGuoen4HILXXiuxgzV9JqaVTj+hN4a59ZwJjFwWfpgf67K3cnEd+UQkPfP47U5YHB38ZMmM1Q2as5ui9W0Usr9+YpQyasILdmjfizj/uHzfWM19KbFLoi7xavzoG/nRkBxrUK/+hL9HnsIfnv83MRrcmeLbUCj1bwHF/C++/l6hAQjf7i/LHAhPYh/r+vtjlzfiwYvdf+FPs49EGrdm60g2YEunD1M9PwLgyw+YHPvhlL4SGzaPfr9BrOp7owBzzv4NWndz6L73gtAfd+nsXRDg5wm95pARwYt/I93rl8PDt3Q+PfN6WLGjZ0fVB3bAALn4t8nmh8nNcDUvLkJmOBl0GLTpGv6asV492c0Q2aQuXvgP1GwePbV3Jztc/+jk3tUydkNeeVabPcqy5HKMpynM1dvUbxT830Cy4IknzW6e4KUN65rh4s8bDKRF+H/qf7ZY9c8ofq4jFI93P1q6dXWK6Syt4YFnw+KtHueVDa6CB915vWurOXzi0fHmzPndNxi9+DQ67wjXRbHuge12h8c74qOKxWut+F1vtHfz5S1S8qUbGvwn7ng5tD3APE0wdOPTPkc9dNTU42JMxrp/E1lVQt4F7qHXdl9Bmv8Rjq4oBF0Kd+vDfqrcgAtzDhOpirXugduS10KRN9d23qj74M2yY636/q/r7VwuoZk98s3lHsAP4kBmxm3aEJnoQTPYAPpmUxarNeWUvAQhLAAHWb82nuKSUEq8D3ZqcPHLyihi3aCMTl7pBDWatyqHPzwt3XvNNnNhy8orIyQvvzP7A57/z2kj31PHDCSs48vGh9Bvral4CkX88cQUn9vqZ7G0FHJA/gMeLrgt/jbVpqHZJ3IQoI4wmqijy70qVbc9OXW1NtKfm8ZpTxjov0E+vIqMwjo+QTEWqUY2U2BkDKyaEz5eY6PeyX4Sn/It/hpcPdcnt8J4u+d5ZQ2ujD9Ly1inuuoDAAA85WZHPnzbIPWQIrfnbke2W2ze4vpCR4gNYNys4f2UyzR1Svj8luCa60Xx3b3A93iA0gbkhY5UXascm9x5VNHEt2OZqST+4xPVfDMjb7N7vsnNrBmr+Z3/lzl/wk6uJBMLaymxe6pablrh+sq8d65o8VkS0Wr7fXnMPI0Y87n6Oyv7O5+e4qV/WzXG17vGUFMOoXi5Z+/EB1wIBXLL62V8iX9P/HNeveOTTwX2/vuTiGf2sa4o88a34906mROZazVkFs8o8bMua5GoGwf1tTsXAOLFqbFdOdqNhJ2O05UTN+KT8+xBgrUv6CyIMrhb6OrZVoCVCZWxaGn+07zSimj2pEQZNqFgTiNCmko8OmZ3QNVvzizj2fyO4rvvetGxc3ysnix9mrWWLl3gu63U+F7zq5u+64/T9GTJjNXd9HKGJmyc3v4gjHhu689pQgX6ID33pmrn1/jl81MIeX7j9BcWlFFKf/iXn8G1Jd5qZHexnVrONxqw68Smm5rbkwpkxhuoXqYhAX6lk+via2HMOpkq82sXAYDeDLot+TmVbrpUUw7T3g33/QkV6TrM9G/qf5WoeWnSIXOa4PtDhmMTuHxiG/dMbQ+4RMjJfYAoNcHMk7nYo7HE4bF4WXk6kfql5W6BhM6hTF6YPcvuyF0Gz3ePHNfcbaLFXcHtnn804Fv/sYut2k+v3t2oK7H1C8PjU98MnrC8K6d+6ZYVrvhtIFOKZ8REcdV38835+IuR+Xv+mSLWJgYFjfn0FOp/q+mnue3r8/nRPt49+7IUDodkecO+88P0lxcH7rf09/Nj8H+GjK+HEu4L7VvzmlisTHLgp1OblLrG76SfYy/s5We71Wf/9U7dcNxv2OTV4zQd/Cm/OfdWHcFD4/8YwT7T24vP6DceapzMg8JrWBLpnmGDCFBidGOOSg4JcaFSmhv/Hh6DzKbD/We73t279+PdMhLWutv7QS90AVp1OhEYhc5P2P8clogdd4EZRLcoP9rHumQNP7Q4dys7TGed+kWxe7mqyz3gsvFY9khKvW0qqEpuifHeP0Pfhy9vcsuVe7m/N/me4ftQdurm/Mz8+4B4UXdwncplA3DZSUwa4Wud9Kznac++u7m/xLUlupu0TJXuSlt4es7TC1wyb7fqXjJi7jqaNgj/6W3ZEfpJmreXOj6InegA9v4ndnK4kQsfD3PzisH5/xSXBodnX04r1thWLrfsQcOKIzhhKuTCB1koiCUlFp3w/Er2KiNWfcfRzlStz4J+CHzATMd2bbqOkMHI8iUxYHypSTcL4N4LrofcIPLUPbe60caFrLhiopQv1zN7Q/XY4J8KAO4kIHeQlZ5VLQg48Bya8FfIBPcQn1wabru3IdiPuAfx9HOx2iFuP1Pdw+kew3xkVnxcx7HfA+/sbqYY3kOiCe0+K8+HeBeXPCwyIs2SUq6396UG4/D1YMNQlXu0OCp67Yrz74NukXflyVpRp4pq7xvVLDfXjAy4Rh/ApX6x1iR4E+74COz8UJzIIUOgInMYEa+0CzVVvnxRMMHbWtpvgdouO5fvtfnw13Pg9tN43fP+OTeEJQLTfpfytrrYwVnPRsKTai29yf/dAYNoHcOd01+Q1YPxr7uvwq9xox49uccnR4VeWTwwjKdwO/c6CC3tDh5CBehaNcA/TJr7tmpHvfzZc+jaMexX+0MNr4ow7Z8q7cG1I7Vbgexk6x2ppiatJDZ3ztCjf/c06pUx/y9AYBnpNX9scAPsmMLIyVH6gnxUToPkeLnELWD0NdtnVNfV9548ucYvU1DLQD7tnjutHvUsruNDrox2pL160GIf9171/5z7j+mLXawTfeFORPLIB6lVyPurKPCCpodSMU2qNe71BWowxLFgX+SnWLQOCoxJuza/YfFOFxeU/LOz70PcRz+32ZPBp0TM/zot4ToClDittSFv6LhdWKC4RiaGkklOhxEr0yg6wkQrbIyRpgRofcIlCWaE1AX26wXP7lj8nYPxr4XMoWutq35ZGGAk2lqEPuyRkyJ3Ra5VD+ygFEj2AN05wiWI0X/0NPry8YvEELBzuXlOgqeT412OfH0gQP7mm/LGhjwTXsya45aLhrua57HQr/c92tanbI4wuO+2D8vvKDp406Z1gYhrWbDTkexvvgc5Xt0fuIzvv22ASl7u2fH/Y16LUOo/8n0u4Q2uZQ713npu7L6Bwh2uG+8MDseME11wz0Ox4cn8YdEXIQe81r5oG2WUeoJQWBd/P0AcfoSOQBqa1WTbG9VEOHbBm9TT3AGbjQvjqH8H15ePcsXWzYNh/wu856FK3DPzubVoCr5/gkrM5XwW/bzMHu2Vosv5h6OvyDO/p7jEmZHqS8a/BmOfdz2to8tPvbOjZMpjoAQy5A146JKTACMlS4G/C8l/dQE1rZrrtni2CD6B6tnDlR9L/rPIPW/qeGux3vK4CE8HnbQ7Gs2OjW09k3s9fX3EPWUpLXVPn7+4JHhv1dPi5pSVuoKCJb7vtpWPgqT1dAh/JBu/hzvq58MJBMOKJyOfVcKrZk1pn1ZbofZaGzw2OLjdvTfn5/WJ5ZUSEJ74J+O73+P1CLil4gsmNvKfzFZmEWEQyU6T+g/GSzIo+qe5zNHQMaeb5wZ8qdn2oqQMqd91HV8YegCHSSLLxzBzsalDOf8GN3AmJ95+MN43DnChzJn53nxskJSBS4jttYPl9/c8qv6/sdCIQnsgHksBVU1wNU1mBWubQgXXKGnBh4k1wA0P6R3vtEN7EODD9wqS3I58bWjsYGHFzw3z49l/h5wVG3i3IcV8QuWnjjmx4dl+49vPIg1EF+jHnhfRznek1U13wY7CGt0+38Oui/j6FJFWB2rz5IQ9+A01VQ+8Xqb9z2WQbggli6PempKj8wEehAoMU5W1299nn1OCx0GQs0Nz9ypCfwxwv/qzxLvGqWy8YR92QGrM3ToS//+qaT+6MK8EH5mujJISjernBsO5bBCvGuZrKWALJdOioo1tXudrOTie5JqMT3nTNn7+/z/V9XfGba8r7/b9dwrffGS7JDnjtGNfC4A2vSfmY5+GPZZL8NKBkTyQKU8FmDYvXhz8ZKo40d0QlbSTY3KV/0RncRIKTeIuIBERqQhlP4EPkjI+TG0tFzE1yM+FAU7nQQVsSleg8hIF5IQMfpssmNksTGLwkntCBiyL1HY03YFKsGsBoiV7ZETV/ejD2PSLpE2mOwpD/ixfELQAAIABJREFUlwMuKn84kHTEFeH/7qIRrqZo3KuR3/dA7e7CoW4wp9KQ+SYDtXCRRHuPAg8QQmMJnY80INoIxLGEJrOBWsov/xr7mm/vdsui7W5ApfNfDB77sUeE80Nqxt4OmfLmoyvh6BvdKLz9z4aTQpLvdbPgm7tgynvBfYH+mOAGPPn9s8ij2r4ZOm90yOv7pZdbblnhBiLaxRtdvSDag/iQObkCxawY7wbjarYn5JYZbG/52OB6IKGP9MDijRPK70szSvZEkuTH2eGjQ305LdF/Tom5sfDfbLLNmTlnN340/2Fww/RsTiAiPok3nUYs0yPUOlWXSE0na7rQqUZiJQxVEVpbVhkVGYk2oDLTviQiNJbCSN0sEnx4uijCgBqB2t96DSNf88UtwfWyTZvXzox9v1gPUMLmPkzSBPajn3XLGR+z8z0JzGWaqNBmjpGE/lyFjnq5aHj4+zv2pfDrQhO9sgKjzcbrhhLpZ/Idb5CVvM3lj4X6zRvQJXSu1cDDibKJXi2jZE8kih2FFeuzV1Z+UXKbW44qPXLn+lQbPjfgStuGDiZJc/yIiEjyfKH5UuMqjfP/dlK/ype90RvFNXTAnWQJzF0YT7KnpQlMr5FufogysEzAZzdVvuzQ5pcSRgO0iERx47s1dySm4jLPaVaURhjZTUREJBPMjzzYWY22abHfEdQ8FRnBWJJGyZ5IBphs43RcFhEREZFaR8meSJrKKm27c31qqZI9EREREQmnZE8kTZ1c+Ar/LXIT5S6zuwHwc0lXP0MSERERyVzxpl+pgZTsiaSx90vO4oj8viyze7BP/kBuLqrCaHsiIiIiEt2QO/yOoMKU7ImkNUMOTQEopQ42yq90jo0xea6IiIiIxLdwqN8RVJiSPRERERERkQykZE8kQz1YdPPO9d7Ff/IxEhERERHxg5I9kQz1Uckf+aj4ND4pPpV+JecndM1yzdcnIiIikjHqxT9FRNLVg8W3Vuj8PxS+zGv1X+b8uhNTFJGIiIiIVBfV7GWQrh1bcuvJnf0OQ9LANyXdua/orxGP/VJ6RDVHIyIiIiKpoGQvQ7RsXJ+vbj+R0w5UM7zarlv+G5xc8FK5/TNK92FC6UGcXvA89xb9nZml+0S8fnDJqRye33fn9t2F/0hZrCIiIiKSOmrGmQEG3nwc+7Zr4ncYUkNspAXYFuX2X1z4ZNj2AtshSgmGrTTl0oJHeaL+e/xaemi5Mz4tPoXL641OQrQiIiIikipK9jLASfu38TsESUuGY/Jfp6NZzxcNe5Y7OsUeyHmFT9OQwrD9Jxe8RJZtV/Fkb5dWkLe5CvGKiIiISEWoGWeGOaJjS/Zv15SLu+7pdyiSBjbQkqn2AD4pPpXHiq6LeE4BDRhXcvDO7Sy7G2AqfrNWnSoXpIiIiIhUipK9DNOkYT2G3fMHjujQ0u9QJI08UHwb75acG/X4ehL/ecpt2SXygWMjDwgjIiIiIqmhZE9E4vqh5DgA8m39nfsOz+/LNYUP7tx+quhqAHJ27x65kHoNoGcOnP0/uGUEPLK+YkGc9WT8c0RERERkJyV7IhLXuNJDALg/ZLqGrTTl19LDdm5br2lng3p14eR7yxeyqzf65/G3Q4duUK9h+XNMyJ+kbjcF16/6ELpXYVTQqz+t/LUiIiIiaUoDtIhIXLk0plP+hxGPPVB0K9ttI/JoAIBpdxCcciscfwcsGQmfeUnbnkcmdrNHNkBxPhTlweT+bt+B54EJ6SfYMwc+vAoW/BC/vH1OhQPOgn/NgZe8vod7dIU10xOLR0RERCRNqWYvQ7VvtQsAx3RqBUDjBnX9DEcy2Cclp/Ft6fGMKD2a8wueIvegq9yBxrtCaYlbP+zyxAqr38Q192zUHJrtBvufDd1uDk/0AnYLDhpD49ZwzC3hx6/7Mny7RfuQdW/aidb7Bff96a3IMTUMmcbigpfg5uGxX0O9RrGPh7r+6/Bt9WsUERGRJFKyl6HOPmR3Pr6tO4P/ejwz/nsWh3coP++aSLLNtp3DE7POf4A69aH73yNfcO5zwfVD/gy3jgg/fs1guODF4HazPeE4r6yT/gWH/MmtmzrQpF3igR57m1s23S2477AroG0XuLQfPLQmuL/+LsH1gy6AjscEtxtHmPbkH7/B0TdGv/cxtwbX441Q2qBp+HbD5rHPD9jrhMTOExERkYymZC+Ddd+nNcYYWjSuj7V+RyO1UrPd4L8bof3RkY8fd5tL8gBO7QFtD4xd3r1z4dxebr1hM7ioj1s/+GKX/LXcO3huIBFrG2F00EBCGtpHsE4duH08HHYZNGgMJ9zp9p90d/nrH1rtEsLL+oXv3+c0aLEXXPBy9Ndw/vNe/C3KJ3MQfD+gfL/GB7OilxvqpgSat8bSMcogO4k44v+qdu+K2vuk1JRbtqZYREQkDSnZExF/XfQqXPVR/EQvkoZN4d+L4ZxnXPPPo0LmCtzjcPjLD3Dm48F9dbzRRG2pWxoDfx0TTBpDnfUE3L/U1Uo2bu3t9JLEBk1cQrjPqa7/4Il3w8WvwfVfQd0yXaH/ORVuG+VqMf812+27dz7cPROalKkZbL0v/OnNkB0VmM+wbLPVC3sH1x9eC6c9XIGyvgiu3zUT7plb/pxbfo58bZsD4D8bE79XRdz4Xfh2h2OABJ5kdToZrni/YvdqF2UKERERkTSiZK+WUMWe1FgNm8JB51X++iZtggnWcX+Ho26AHl4N2N4nuCQw4J45cMeUYLKHcUnhUZEnlOf/27vzMCmqcw3g7zf7DLMvDDADzDAMy7ANAwzDvskOgnMRQZFFFEEQFVFxRxOFxOUaTdSYXI16vWqiJpJoEpe4J4JoABFRUfGqV0HFKERFwHP/OKfoqurq7ll66GXe3/P0M7V3dZ+umvrqfOdURr7+O+8RnZrpDs4s468C+s9zTus2WfcCWlChO6cZvMTXVjCrHZBunl14+ee6U5r5G3SKZ1IqMPte4KyX4Xnk2lM5u473BbDiOp0PWOAbTk5Ho6v3z9uhg8S8zkB2B2DVTuf8ggrbsK3tIxSQmKwDLACYeZtzvRm/cI7b23Oe+gfnvHFX+oaPvwUoc9XinfK74J+r53TfdqpmBF7OS9+TGrd8MOduD9+2rNTlUKzPHszYy5q3Ly1lyIqmr1s5wTme1b55+xKMVxo3EVGUYbBHRPEjNRM4/mbdwYuXzLZAYVeg42CgXR9nrV8wHaqB6T/z7ijGiwhw8gO6F9BQEpN1QNpllE4lBYCq43XNUvlI/+Urxui/PaYB8x4Cxl2ux3M6em8/zQSVVoA7ao13m8IznnGO55Q42ytmuy6aRXQq6/lv+TrisbPaQya4ajqPBtqGPaiqGANMvUEPV04ARqwC+poOf2rm+7+HVxqsZeQFunZz8k/1oz7s0nL140GG2VJ0l75kPlcCMOsuve3+HjcB5twPlNi2t/wVjze3/U461gG5trJZuUWnCI/2PaMSJ97tGx51UeDPBPh/f5bTnwaW/V3fKKicCJz03/7LdKjRHR5ZUrLM7prfXUPbhAI6eL5in2/cHfik5zd8W5ntnOPFvZ3juZ18wymZzlprN3uwt/Yr4Pydgee7eX3+nscHXn7u/c7xYOnPw89rXOdNjVHi+n2f+ULgZd3nFK9zTEMVNiEbozGsG0ZeLv00fO9zzrbmrV81M/C8szY2b9vBJLdpuW2Tk/XoqBjFYK+1YNUekU9KG2DpizqIi2Yzb/cPJup/rdsEWhfzQ1cCq99x1rRZLv7Yl4J5NHU1QQeuq3bq4OCC94CLdgNtTe+mKZn6+wklMUWnsma1c9aMWjVtXoHxeTuAinHOaRXjnG0tu5taXiu4q/+lvmi3b+MowdGT24I/6c+69CW9/NjLdM3s4DP992XNB8C4K3SNrKVdb73elV8Cvev1Osff4pu/aiew8HFdC23vSKioGzDrTucF9sI/+YaHuJ4PmV+uU4TtNcG9Zvreq/tk5/JLX3SO24Mqe0DXri9Q3EvfKDjlt3paxThg6o163079AzDvYd3h0dqv9GvgacDIC/Xv5LK9QFEP3/bS85zv667pnH0PkJCov+8p1+ta8wvf982f97BveObt8GP/HUy/ybsmsri33kd7p0bLNwa/6RLqhky7vr7h1Gznfgxe6ly283B9rATi7mBp1IWBl01po7+r3E46uO0y2nu5joOBc18P3ha1xzTn+GzbzYJBp+tshWDGXeEb7juncZ1bdZvkGx64yDdcMiBwandTuVPT7ew3ogDvZ7s2VF5n582Xxiqp8Q13GuIbrhgHtO3hHbTOvkcfd8Es+GPwsln9tnPc3j7d3vYbcB6PgPMGSmPlhFj3sr2B57lviIS6uWUXqj24O3tjyXMN37ab+7jPLvFeLkYw2COisOJ9hTBKTtPBhF1Sir7IOtrJjOgaSwBY9g9numVqpg7IAGewB+iauuJeQJsCfWGfnKYDgEuCXIAs+rNOTV2+yXmxNXwVMOwcM2J+AdZFtf2iIqdEv9Z+pWuFLv9C12YufdEXxGV30PMDpSHmlPh6ZJUE3+dKSNLrtuvtvR6gL+ga03bRHjhktwfKhnkv1/s/9AW2FTAXVIbedlZ7vbwVsPU/VdeSdujva9sJAG176SDk7Nd0jWP/U/T0jAL9HZ32V526nJjs/x6nPgIMWqz3rWKMLy3ZkpQCjL1U/0aSUp3fXdtewIjVoT9Hu95ArUk/zsjXPd3WLfe1wU1KB6rn+gePpz7i68iosBsw8VrfvF4zdXA572H9uBO7nFIcrTnNLAZOus9Xew14dwaVb7sRMuYS/Terg+7wyNHJktI1RoXmmOs3x/md2YPZzsP1cWeljEui8/dyvu1CPLczUH2Kzio493Vg9Vv6sSv27Y2/Wv8G5m8IfSFuDy4AZwq3VTMeyIxbdWCUbVLK88r0cW2Z8OPA605a7/uux12pf5fDV+nxieuA0gAdcQH6e738cx0sWrXobmJ7RFRyG/2b7myOubkP+C+/5n/1za+5D/oHf3bBamcto9f4ht01y4GyJgB9LrKnkp/yO9+w1fbZ67E+VTP0+dnLmS/oc2D5SF/qunVDzGq3POxc//XtN/xySoE2Rb7xzrZz15AVwPgfOdedeqNz3KudtuWMEEF9Uqr/9i19XY9hstcsB3vPop66PbuVjQD41yzPuNU57r6Z2+/kwNt3c2cbpOd6LxcjGOy1EoqX4ESxa+K6wBdIdsVV/umWlm4T9d+uY5u+H52H6tRUd2c6IkC1qamy7igPWaEvWjrV6UdwDFjkXCch0dfWMi3b+RzEUCat8wWKVnpNalbwdQB9QeeufQl2IWdJTPWfNmoNkOZ6pM28R3RnP1nFzgstLwmJ+jEdVlAromtJAV/bTkB/xsk/0RdyveuBH0xwm1eu/3aq06nLDU0xbjDlvJh01yYFMuU6YNK18KWymv89XimSAxbq2o2CCmeAk5wO1N/h+z6UK03Y+qwV44Ce04Bztuqa0dJaZ82dJSHRObzwMWCJSVu2B6FVM/R7z7lf13L2mOrcjj0Nc5GrsyB7sNFltP4NWM7dpm9EuNkDyWHnADN+rm+6eLHSmQH/WgZ7kGTp5qohnnoDsHqXL6W4q6nRTM3UQei8R/QFtdezPmffA1y6R3dW1dk81qVirC6H467UN246DfZfz/5M0uR0Hbyd/KC+QXDyb3Xt9gl36Pl1Zzkf0ZNngpv6X+nApnKi//bTcnTw0H0ScPh7//mWSeuc46PWeC9nWfaSfl+LvYZx4rW69hXQN1qs3qEtCR43Xexq5uusiEDmPeKsmT3+FuCUh4ClL+gbAWXDdSBoz0qw2H9juZ2cAZf99znxGuDIId94UprvRhWg22Bnd9A34Fa86p+K6j7vAf6p1fZzh9UGt/5XwIDTnMt1HqazC07/m+9GnydzHln6vK9WfOgK3/F7wXuBg2dA/0btN8TKRznnT71Bf78Wd1OQkUFq7WNAUuhFiIgootypgE3RsTbIP9IwKOrm3H5Cgu+iZcEG73WaSsQXKE69QV+kB6vRC2bZS8B3XweeP/FafWHrNuZi/bLLbu9LaZ33CPDgPOeda/cFRjBnbQQ+2eo/3frc9mdEhkvnYcDmO20TTFA1ZIVODz1rI/D9Af82kF6s2iar1mPBBuBn/VzLiPMCbcVmvX03vzahrkAyPVdfRFupvyv/CXzxrm/xTnXA52/72mC6O/qZfY+usWrXR48XdtXpom7u9qfWZwikZj7wWoheYOdvAL7x6L3WfvOjep6vBrtXva71/L0tKPPar5PuBX5kUn7Pfs0/zXvyT3Uw296USddxvgDw4o+Bhxfrci93pSDWzNe1c/ZgNsEj2ASczyR16zbRdwOqny1FuKAr8KdzfTW0OSX+gY29RtRy5KBzfNDpwCu/1n9zSnVb2ZtNTY9XYOrWdzaw+0WdVl1YqQPE59Z7t7vsXQ88ZTqT8qpht99sP/4WZ3p46SDgI5OqP+NWXxlYUjOByvF6uIvH+WPJc8Ado4Bi0wY9vwuwZ7uuSd5hOryy3yiwWOfmtlX6xsrB/f7LeN2As87xfecA2x5wTvvjSt9yVnvqqpk6uJx4jf+2AH38jA2SbTHhGuCJS32///wuulb809d1DexHr+hjLFjNLqB/oz2nA6/d7du/2iXApjv0jayBi/W+DFkB/OPnzo6d2vfT71W3HHj5F97bj3IM9ogoLMoKMrD7i28ivRvU2qS08W/n1hhpOd53qi1Dljdtu+376hodyxX70KhHabTtoV9+263WKVe96/3nNVefWfqC56HTdJBmXdgmZ/j2qaGsIMi6SMsr03ffr+vi37OrpTBA+qsVTFht6o5uO0DGSn4XZ4cKU67X6XbFVd7LN7Sn1oRE/agX+/taNTkdB/tqia2UOfeFvRevC3hAl/E7TwDffqlvnPxg3tOqUVu5Bdj/CbD3TZ2K7WYFHRmF3u15k9N8wZZbaqaugfMi4gz0QulQA/zfaw1f3qr18gpgp92kswrc6ciAr7Zt1EW6necO6waT+a3kl+NoG98y2w2Ys237tvAx50X+8bbaquHn6XTnmgXANtd3Y6+VTkjUtdhpDUz7W/Rn4LHzdRBS1YB0UzfrWEhM1gGPvXa0cqI+7ryeFVvUXWdHWDePPrOlHXdvQO/YU67zBXuWs172nUut79G6geLl0j3e04//ObDB1AZWTjDBnutYt7Y79UZgzGW+5gpuAxb5ai0rxwNXfAk8u04Heq/epae37en7Ho9bq2uc88t92zjzef130rUM9oiIiCiAQLUfjSWi2+G1lKMXu0pfNH/zhb7QbSwrtdDebqZNQdNql2vPBA585qsBqJygO64Y2sBHNCSlBg70GqLreGDXk7q20v34leQ03eY0r1wHSedsDd2BRUOkZupam5/11TWSRw4BW+7ztRvNL9cvK63Sy+pd+rMfS6c/DdxbDxw05Xz6U4177EufE4G9O4CRHu1F7R3CuPWYqm8mWIFv/3m6ZtxqowkAK18D3ntWBzh1ZwFv/MEZCLtrfO2S03ztkqtmAh9udKaAp2QB35vasYs+cK4b7PEficm6Xer4qxqWiu5W3EfXRtUu8Z/XpkD/HgO+ty0EKOqmyy67xNcG3G7+BmeQ69Xjtf3ZpOUjdJpraa3/clanU4FSlmtO1cFej2n+N438PkNy8JsP029yjickBK9JTEyOy2esimrss5eiyMCBA9XmzZsjvRsxYdZtf8fmD76M9G5QHLNq9p4+fxQqioLkzhNR9PrhCPD4at1Oymo31VQfbtK1de7OWWLNoW+BA3v8e9+MJl9/olMZo2EfjxwCDn0TvMY8likFfP9vZwryvveBT7cFriX+7QKd/lodokfJlrTWlEe40vmv66oD2aVBHvXRVAf3686d/vUBcEuN7oxlxaaGrfvRq8C+d3UqbjAbfwn8+ULdKdHQs/3nf7pd155babQA8OYfdQdO7nbrUUBEXlVKeebYM9hrJRjsUUtjsEdERBSl9r2va8i80nqjlVLA01fp2u1w7/eRwzqVc8DCAG0tY0uwYK/FeuMUkTtFZK+IbLdNyxeRJ0XkHfM3z0wXEblZRHaJyDYRqQm8ZWqK2A3pKdbE8P0jIiKi+JRfHluBHmB6fF3bMvudmKQfGxMHgV4oLfnohd8AmOSatgbA00qpSgBPm3EAmAyg0ryWALitBfeLiFqAhL3rdyIiIiJqjhYL9pRSzwPY55o8A4Dp9xR3A5hpm36P0l4GkCsiQVq0UmNZ6brXn9gPl00N3fj07tNq8dSqkSGXIyIiIiKi6HSsH6perJT6xAx/CsDqQqcEwIe25T4y0yjMygszcPqILiGXG9WtCGUFbY7BHhERERERUUs41sHeUUpXNTW6dY+ILBGRzSKy+bPPPmuBPYtPTWlGxbQ8IiIiIqLYdayDvT1Weqb5u9dM/xiA7YElKDXT/Cil7lBKDVRKDSwqKmrRnY1PzQ/g+pbmoF12GvqUxGm3ytQkvDVAREREFF2OdbC3AcACM7wAwKO26fNNr5x1AL6ypXtSGCQ2s5ZuZnUHnD22KwCgNC8dL18yDu1zAjwQk1qF2rJ8xzg74SQiIiKKLi356IX7AfwDQHcR+UhEFgNYD2C8iLwD4DgzDgCPA3gPwC4AvwJwVkvtV2t105xqLBpWhv4dc5u4fn90b5cFAJAm1uGkJCZgQlVx6AUp6u1ePxWLR5RHejeIiIiIKIikltqwUmpugFnjPJZVAJa31L4QUJqXgSun9zo6npWahP0HDwddJ9xpeW9fMxkAULbmsTBvmUIpzk7Fnq8PhnWbgZ+nxzo+IiIiomgQsQ5aKLKeXDUKDy6pi/RuUAvLSkvCixeNwcpxlS3+XmyzR0RERBRdGOy1Uu1y0jC4S0GkdyPunTy4E7q2zYzoPpTmZTQ59dbLjOoOAAC/ZqCM9oiIiIiiSoulcVJsuP7EfnjvswO49dl3Qy4bOG3v2Hnl0uNw4OBhjLn+2UjvSoNce0IfAPGTurr9qolIS+I9IiIiIqJYwKu2Vm7WgFJcOKmH57yAHXia6S0Z+3UvzvKcXpSVivJCPuw9UjJTk5CUyNMGERERUSzgVRtFzE//oy9uOLGf57yFw8rw5tWTkJUaucrn+poSXD6tKmLvH06qBULzaKjpJSIiIqLAGOxR45mL/OZe7M8e1DHgs/oEQHpKYvPeoJlunF2NxcNj5/EC6+v74L7TBzdqnaSE5je0a5ety3BM97bN3hYRERERhQ+DPWqwZj6XvYlvqv+sHNsV3Yq9OzrpEOLh7mO6F2HluEqM6lbkmD6oLC8su3isrKvvg3l1nQLOn1PbCcO6FjqmhSqyW0+pwaPLhzVrv/qW5mD3+qmoNs9wZI0fERERUXRgsEcOI7sVYcGQzgAAiUh0p7njhcUjuuCJ80b5Lffm1ZPwzAWjg24rJSkBq8Z3Q6qrY5F19X2x80eTQu7L2WO74penDvCcN3tgKdZOr8KAzi0XOF4wsTsWDy/HnEEd8eOZfRq1bqi4KzM1Cf1MkNZcEfy5EBEREZEH9sZJDvecVhtwnrvGZlBZHp56c0+L7MfR9woRrdhTPRNEL37J5J7okJuO/d8dwppHXvd77MDSURXY+enXKCvIaFBnI+dP6I6vvj3kOS81KRELh5Xjsdc/CbmdQHq0y8LOT/cHnD+oLB+15flBt5HS1B4yAwRomy4Zh8M/KAxd/zcAwNzajhhU5r0PrMgjIiIiik4M9qjxTICwZGQXTOzVDj9+7M1GBX23zO2Pqg7ZjXvLELVGDy0dgva56SjJTT867XFXADajugRP7NiDk2s7oVNBRqPeP1REU9elAK/s/jLg/AGd83Dgu8N4a48O6u5aOAidCjKQn5GCPfu/w6SbXvBbZ/mYCtR1KQgZ6O24emKDn6OXnpyIbw8dgUjwdMu22c7U2HX1ff2WYU0eERERUXRjGicBaFpHHSKCssI2uGlONf7njIZ3DDK9XwdUFJn2dwHetrG9Rw4sy3cEes791H+n9m2P3eunNj7QA5CUGPz7Ofe4bnjugtEBew99eNlQ/PW8kWiTkojqjrkY06MtKooykdcmBckBaheTEhIworLIc55dRkpSgzuzmdm/BC+tGYtaU0tnBYkXTeoRsLOcQNg2j4iIiCi6sWaPAADb1k7wnJ6SmIA1k72fw2fJTE3C0IpCrKvvg9uefRd3LhyID/d9i88PHMTnB77HT/6yE1lp0f9TG9g5D7s+O4B/feOfstkmNQn/c8ZgHPjuMJbc+6rf/MQEQeeCNijKSsX+g4cDvscbV4duIxhKeWEbvP/5vwPOv+e0WhRkpmDqzS8ejaXtgVlJbrpfKL1sdAWWja7we/j7uvo+yMtICbo/rOAjIiIiik7RfwVOx0RGivdP4e1rJjd4G3NrO2Fure4tsmtb30PRFw0rC7lucXYq9nx9EIuGleGul3ajZ/vgaZ4/ntkb3x/+Iegyja15emjZUADAo1s+RopHbdvQikK8/N4XAIB+pTmo6pCN8yd0cyxz3xmD8dKuL7D6d1sb/L4Bn10fYMaGFcOw/7vDR9vTuY3sVoSvTMDq/grc2wyVimmVJxERERHFHgZ71OLSkgOnGKYm6Xn9O+bhdtPj5dljK5HfRtcmXT6tCpc9uh0Zrm3Mq+vc4PdvbNuyGdUlIZdJTU70bMfWPicdswaUNirYKy9sg+VjKpCTnoxrH98ZcvmstGRkpSUHXyjEZ67rUoBN7+9DcXbjUjeJiIiIKHYw2KMGG19VjOn9OuCSKcHTOhujplMu1k6vwgn9S49OswI9QD94ffagjmF7v+ZKMJFjOB5GbhERXDCxB/6yXXcoM7g8H9npyVg4tKzpG1VBR3HOuErMqin1a7/4wJI6fLjvmya95YSqdth6xQRkxkDKLhEREVFrwKsyarC05ETcMrd/WLcpIlg4rDys27SM7dEWx/UsxsWTewZd7oULx+Ct41StAAAKV0lEQVTjf33boG0O6JyH04eXY/GI8O+zlXaam5GMX546MCzbdIek1nhignh2VFPXpQB1XQoatO0M0ymMFZynJCU0/REQRERERBR2DPYobqWnJOLXC0IHTR3zM9Axv2E9dCYmCC6bVtXcXQuqoY9RiLQRlYW45oTemNmAtFciIiIiOvYY7BFFidHd22JM9yJcMiV4TaRl6agKfPhlw1IurUctTOjVrsn75yYiOGVww9tOEhEREdGxxWCPKEqkpyTirkW1DV4+2CMxrHTKwSYls3u7LOxeP7V5O0hEREREMYXBHlEcSk9JxJPnjURpXuMfIE9ERERE8YG9KRC1gFHdiiK9C6gszkJ6SuDHXhARERFRfGPNHlEL+PWCgfj20JFI7wYRERERtWIM9ohaQHJiApITWXFORERERJHDq1EiIiIiIqI4xGCPiIiIiIgoDjHYIyIiIiIiikMM9oiIiIiIiOIQgz0iIiIiIqI4xGCPiIiIiIgoDjHYIyIiIiIiikMM9oiIiIiIiOIQgz0iIiIiIqI4xGCPiIiIiIgoDolSKtL70GQi8hmADyK9Hx4KAXwe6Z2gRmGZxR6WWWxhecUellnsYZnFFpZX7InWMuuslCrymhHTwV60EpHNSqmBkd4PajiWWexhmcUWllfsYZnFHpZZbGF5xZ5YLDOmcRIREREREcUhBntERERERERxiMFey7gj0jtAjcYyiz0ss9jC8oo9LLPYwzKLLSyv2BNzZcY2e0RERERERHGINXtERERERERxiMFemInIJBF5S0R2iciaSO9PayYiu0XkdRHZIiKbzbR8EXlSRN4xf/PMdBGRm025bRORGtt2Fpjl3xGRBZH6PPFIRO4Ukb0ist02LWxlJCIDzG9gl1lXju0njD8BymytiHxsjrUtIjLFNu9i8/2/JSITbdM9z5UiUi4iG830B0Uk5dh9uvgjIh1F5BkR2SEib4jIOWY6j7MoFaTMeJxFIRFJE5FNIrLVlNdVZrrndywiqWZ8l5lfZttWo8qRmiZImf1GRN63HWPVZnpsnxeVUnyF6QUgEcC7ALoASAGwFUBVpPertb4A7AZQ6Jr2UwBrzPAaAD8xw1MA/BmAAKgDsNFMzwfwnvmbZ4bzIv3Z4uUFYCSAGgDbW6KMAGwyy4pZd3KkP3OsvwKU2VoAqz2WrTLnwVQA5eb8mBjsXAngtwDmmOHbASyL9GeO5ReA9gBqzHAWgLdNufA4i9JXkDLjcRaFL/O7zzTDyQA2muPB8zsGcBaA283wHAAPNrUc+Qp7mf0GwCyP5WP6vMiavfCqBbBLKfWeUup7AA8AmBHhfSKnGQDuNsN3A5hpm36P0l4GkCsi7QFMBPCkUmqfUupLAE8CmHSsdzpeKaWeB7DPNTksZWTmZSulXlb6zHuPbVvURAHKLJAZAB5QSh1USr0PYBf0edLzXGnufI4F8JBZ317+1ARKqU+UUq+Z4f0A3gRQAh5nUStImQXC4yyCzLFywIwmm5dC4O/Yfuw9BGCcKZNGlWMLf6y4FqTMAonp8yKDvfAqAfChbfwjBD9BU8tSAJ4QkVdFZImZVqyU+sQMfwqg2AwHKjuW6bEXrjIqMcPu6dQyVpj0ljutlEA0vswKAPxLKXXYNZ3CwKSL9Ye+i83jLAa4ygzgcRaVRCRRRLYA2At9wf8uAn/HR8vFzP8Kukx4HXIMuctMKWUdY9eYY+w/RSTVTIvp8yKDPYpnw5VSNQAmA1guIiPtM83dFnZHG8VYRjHjNgAVAKoBfALghsjuDrmJSCaAhwGcq5T62j6Px1l08igzHmdRSil1RClVDaAUuiauR4R3iUJwl5mI9AZwMXTZDYJOzbwogrsYNgz2wutjAB1t46VmGkWAUupj83cvgN9Dn4D3mOp1mL97zeKByo5leuyFq4w+NsPu6RRmSqk95h/nDwB+BX2sAY0vsy+g02OSXNOpGUQkGTpouE8p9YiZzOMsinmVGY+z6KeU+heAZwAMQeDv+Gi5mPk50GXC65AIsJXZJJNCrZRSBwHchaYfY1F1XmSwF16vAKg0PTClQDe83RDhfWqVRKSNiGRZwwAmANgOXR5Wb0kLADxqhjcAmG96XKoD8JVJcforgAkikmdSZiaYadRywlJGZt7XIlJn2kPMt22LwsgKGowToI81QJfZHNP7XDmASuhG657nSlPD9AyAWWZ9e/lTE5jf/n8BeFMpdaNtFo+zKBWozHicRScRKRKRXDOcDmA8dDvLQN+x/dibBeBvpkwaVY4t/8niV4Ay22m7ASbQbezsx1jsnhe9em3hq1k9/EyB7jnrXQCXRnp/WusLuteqreb1hlUW0HnxTwN4B8BTAPLNdAHwC1NurwMYaNvWadANpXcBWBTpzxZPLwD3Q6cjHYLOaV8czjICMBD6ZP0ugJ8DkEh/5lh/BSize02ZbIP+p9jetvyl5vt/C7beyAKdK82xu8mU5e8ApEb6M8fyC8Bw6BTNbQC2mNcUHmfR+wpSZjzOovAFoC+Af5py2Q7gimDfMYA0M77LzO/S1HLkK+xl9jdzjG0H8N/w9dgZ0+dFMTtEREREREREcYRpnERERERERHGIwR4REREREVEcYrBHREREREQUhxjsERERERERxSEGe0RERERERHGIwR4REcU9Efm7+VsmIieHeduXeL0XERFRpPHRC0RE1GqIyGgAq5VS0xqxTpJS6nCQ+QeUUpnh2D8iIqJwYs0eERHFPRE5YAbXAxghIltE5DwRSRSR60TkFRHZJiJnmuVHi8gLIrIBwA4z7Q8i8qqIvCEiS8y09QDSzfbus7+XaNeJyHYReV1ETrJt+1kReUhEdorIfSIi1vZEZIfZl+uP5XdERETxJynSO0BERHQMrYGtZs8EbV8ppQaJSCqAl0TkCbNsDYDeSqn3zfhpSql9IpIO4BUReVgptUZEViilqj3eqx5ANYB+AArNOs+bef0B9ALwfwBeAjBMRN4EcAKAHkopJSK5Yf/0RETUqrBmj4iIWrMJAOaLyBYAGwEUAKg08zbZAj0AWCkiWwG8DKCjbblAhgO4Xyl1RCm1B8BzAAbZtv2RUuoHAFsAlAH4CsB3AP5LROoBfNPsT0dERK0agz0iImrNBMDZSqlq8ypXSlk1e/8+upBu63ccgCFKqX4A/gkgrRnve9A2fASA1S6wFsBDAKYB+Esztk9ERMRgj4iIWpX9ALJs438FsExEkgFARLqJSBuP9XIAfKmU+kZEegCos807ZK3v8gKAk0y7wCIAIwFsCrRjIpIJIEcp9TiA86DTP4mIiJqMbfaIiKg12QbgiEnH/A2An0GnUL5mOkn5DMBMj/X+AmCpaVf3FnQqp+UOANtE5DWl1Cm26b8HMATAVgAKwIVKqU9NsOglC8CjIpIGXeO4qmkfkYiISOOjF4iIiIiIiOIQ0ziJiIiIiIjiEIM9IiIiIiKiOMRgj4iIiIiIKA4x2CMiIiIiIopDDPaIiIiIiIjiEIM9IiIiIiKiOMRgj4iIiIiIKA4x2CMiIiIiIopD/w+j0KGlQSl9twAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkBo2ud7TvrH",
        "outputId": "ba2ccf86-bf62-4d65-f9e1-1f74373054a0"
      },
      "source": [
        "np.array(train_losses) / 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([506.62463379, 506.29690552, 506.65631104, ..., 152.94316101,\n",
              "       139.41781616, 130.01306152])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq9KTQ46dFOR"
      },
      "source": [
        "test_loader     = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "\n",
        "data, y = next(iter(test_loader))\n",
        "recon_batch, mu, logvar = model(data.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DptFPfO1V3-9"
      },
      "source": [
        "keepx = data\n",
        "keepy = y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ivEf-JadU7K",
        "outputId": "66c0eb15-eb36-4999-daaa-465c4dc4f7ec"
      },
      "source": [
        "data[0].shape, recon_batch.view(4, 27, 27)[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([27, 27]), torch.Size([27, 27]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 570
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "vxoUBWP_K1p_",
        "outputId": "8a28cf87-6a53-4a4b-8857-6691699b9f4b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(data[1])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "qa-yAt9TLEHH",
        "outputId": "e351e291-879d-46ae-9ad0-fa6f4f980a89"
      },
      "source": [
        "recon_batch = recon_batch.cpu()\n",
        "plt.imshow(recon_batch.view(4, 27, 27)[1].detach().numpy())\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLUzpm9hnoUs"
      },
      "source": [
        "##########################################################################################\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "                                      ##### AE #####\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "##########################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgbehlMDnvc7",
        "outputId": "617f88c1-6a99-4f30-95cc-33f4d7b83abd"
      },
      "source": [
        "class AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        self.fc1  = nn.Linear(729, 512)\n",
        "        self.fc2  = nn.Linear(512, 256)\n",
        "        self.fc3  = nn.Linear(256, 128)\n",
        "        self.fc4  = nn.Linear(128, 64)        \n",
        "            \n",
        "        self.fc5  = nn.Linear(64, 2)\n",
        "        self.fc6  = nn.Linear(2, 64)\n",
        "\n",
        "        self.fc7  = nn.Linear(64, 128)\n",
        "        self.fc8  = nn.Linear(128, 256)\n",
        "        self.fc9  = nn.Linear(256, 512)\n",
        "        self.fc10  = nn.Linear(512, 729)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc7.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc8.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc9.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc10.weight)\n",
        "    \n",
        "    def encode(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "\n",
        "        return self.fc5(x)\n",
        "    \n",
        "    def decode(self, x):\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.relu(self.fc8(x))\n",
        "        x = F.relu(self.fc9(x))\n",
        "        \n",
        "        return torch.sigmoid(self.fc10(x))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x.view(-1, 729))\n",
        "        return self.decode(z)\n",
        "\n",
        "ae = AE().cuda()\n",
        "ae"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AE(\n",
              "  (fc1): Linear(in_features=729, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc5): Linear(in_features=64, out_features=2, bias=True)\n",
              "  (fc6): Linear(in_features=2, out_features=64, bias=True)\n",
              "  (fc7): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc8): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc9): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (fc10): Linear(in_features=512, out_features=729, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv4iUGUAou6m"
      },
      "source": [
        "optimizer = optim.Adam(ae.parameters(), lr=1e-4)\n",
        "\n",
        "def loss_function(recon_x, x):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 729), reduction='sum')\n",
        "    return BCE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQlxHS2pcxL"
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train(epoch):\n",
        "    ae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon = ae(x)\n",
        "        loss = loss_function(recon, x)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        if batch_idx % 10  == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(x)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test():\n",
        "    ae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "            \n",
        "            recon = ae(x)\n",
        "            loss = loss_function(recon, x)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            val_losses.append(loss_function(recon, x).item())\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "Kj2AHIqBqHOm",
        "outputId": "74e75481-1048-4859-9f07-d8c161011670"
      },
      "source": [
        "# train\n",
        "for epoch in range(0, 100):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-367-bb04a30128be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-366-7b6c1fd1d1af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "a3YjWzf2wKIr",
        "outputId": "00535f46-46d2-4db0-af6a-c326715e21ba"
      },
      "source": [
        "x, _ = next(iter(test_loader))\n",
        "r = ae(x.cuda())\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(x[0])\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "b1cck5LlwTX5",
        "outputId": "aa62946d-c74a-4483-c00f-7297ce9ebe50"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(r.view(4, 27, 27)[0].cpu().detach().numpy())\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oS_-1Eb-YMZ"
      },
      "source": [
        "##########################################################################################\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "######################################CVAE################################################\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "##########################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwQHiXB70tW7",
        "outputId": "abc5b313-586b-423a-be5b-a4b52fbc5fd6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class CVAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, c_dim):\n",
        "        super(CVAE, self).__init__()\n",
        "        \n",
        "        # encoder part\n",
        "        self.fc1 = nn.Linear(x_dim + c_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc51 = nn.Linear(64, z_dim)\n",
        "        self.fc52 = nn.Linear(64, z_dim)\n",
        "        # decoder part\n",
        "        self.fc6 = nn.Linear(z_dim + c_dim, 64)\n",
        "        self.fc7 = nn.Linear(64, 128)\n",
        "        self.fc8 = nn.Linear(128, 256)\n",
        "        self.fc9 = nn.Linear(256, 512)\n",
        "        self.fc10 = nn.Linear(512, x_dim)\n",
        "    \n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc51.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc52.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc7.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc8.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc9.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc10.weight)\n",
        "\n",
        "    def encoder(self, x, c):\n",
        "        concat_input = torch.cat([x, c], 1)\n",
        "        h = F.relu(self.fc1(concat_input))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        h = F.relu(self.fc3(h))\n",
        "        h = F.relu(self.fc4(h))\n",
        "        return self.fc51(h), self.fc52(h)\n",
        "    \n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add(mu) # return z sample\n",
        "    \n",
        "    def decoder(self, z, c):\n",
        "        concat_input = torch.cat([z, c], 1)\n",
        "        h = F.relu(self.fc6(concat_input))\n",
        "        h = F.relu(self.fc7(h))\n",
        "        h = F.relu(self.fc8(h))\n",
        "        h = F.relu(self.fc9(h))\n",
        "        return torch.sigmoid(self.fc10(h))\n",
        "    \n",
        "    def forward(self, x, c):\n",
        "        mu, log_var = self.encoder(x.view(-1, 729), c)\n",
        "        z = self.sampling(mu, log_var)\n",
        "        return self.decoder(z, c), mu, log_var\n",
        "\n",
        "# build model\n",
        "cond_dim = 3\n",
        "cvae = CVAE(x_dim=729, h_dim1=512, h_dim2=256, z_dim=10, c_dim=cond_dim)\n",
        "if torch.cuda.is_available():\n",
        "    cvae.cuda()\n",
        "cvae"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CVAE(\n",
              "  (fc1): Linear(in_features=732, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc51): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (fc52): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (fc6): Linear(in_features=13, out_features=64, bias=True)\n",
              "  (fc7): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc8): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc9): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (fc10): Linear(in_features=512, out_features=729, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 596
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G8sDR6T-dMx"
      },
      "source": [
        "optimizer = optim.Adam(cvae.parameters(), lr=1e-5)\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 729), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJNlYIufWZyA"
      },
      "source": [
        "train_loader    = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=4, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gl2Wz1E-rJB"
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train(epoch):\n",
        "    cvae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, cond) in enumerate(train_loader):\n",
        "        #print(cond[0])\n",
        "        data, cond = data.cuda(), cond.cuda()#F.one_hot(cond[0].long(), num_classes=3)\n",
        "        data = data.float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, log_var = cvae(data, cond)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        if batch_idx % 10  == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test():\n",
        "    cvae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, cond in test_loader:\n",
        "            data, cond = data.cuda(), cond.cuda()\n",
        "            data = data.float()\n",
        "            \n",
        "            recon, mu, log_var = cvae(data, cond)\n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "            \n",
        "            val_losses.append(loss_function(recon, data, mu, log_var).item())\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3NdwZ_o-vee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb93f65-9660-43e9-cc82-c69d330fbffb"
      },
      "source": [
        "# train\n",
        "for epoch in range(0, 100):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/1400 (0%)]\tLoss: 519.615479\n",
            "Train Epoch: 0 [40/1400 (3%)]\tLoss: 521.170288\n",
            "Train Epoch: 0 [80/1400 (6%)]\tLoss: 517.401672\n",
            "Train Epoch: 0 [120/1400 (9%)]\tLoss: 513.259399\n",
            "Train Epoch: 0 [160/1400 (11%)]\tLoss: 508.790833\n",
            "Train Epoch: 0 [200/1400 (14%)]\tLoss: 499.673737\n",
            "Train Epoch: 0 [240/1400 (17%)]\tLoss: 507.425903\n",
            "Train Epoch: 0 [280/1400 (20%)]\tLoss: 496.188385\n",
            "Train Epoch: 0 [320/1400 (23%)]\tLoss: 488.579681\n",
            "Train Epoch: 0 [360/1400 (26%)]\tLoss: 485.035400\n",
            "Train Epoch: 0 [400/1400 (29%)]\tLoss: 479.876556\n",
            "Train Epoch: 0 [440/1400 (31%)]\tLoss: 468.217346\n",
            "Train Epoch: 0 [480/1400 (34%)]\tLoss: 464.519470\n",
            "Train Epoch: 0 [520/1400 (37%)]\tLoss: 452.675903\n",
            "Train Epoch: 0 [560/1400 (40%)]\tLoss: 470.287872\n",
            "Train Epoch: 0 [600/1400 (43%)]\tLoss: 452.590271\n",
            "Train Epoch: 0 [640/1400 (46%)]\tLoss: 443.306305\n",
            "Train Epoch: 0 [680/1400 (49%)]\tLoss: 446.575378\n",
            "Train Epoch: 0 [720/1400 (51%)]\tLoss: 441.921478\n",
            "Train Epoch: 0 [760/1400 (54%)]\tLoss: 417.456512\n",
            "Train Epoch: 0 [800/1400 (57%)]\tLoss: 462.426605\n",
            "Train Epoch: 0 [840/1400 (60%)]\tLoss: 428.053070\n",
            "Train Epoch: 0 [880/1400 (63%)]\tLoss: 402.638153\n",
            "Train Epoch: 0 [920/1400 (66%)]\tLoss: 408.161560\n",
            "Train Epoch: 0 [960/1400 (69%)]\tLoss: 384.851776\n",
            "Train Epoch: 0 [1000/1400 (71%)]\tLoss: 404.849182\n",
            "Train Epoch: 0 [1040/1400 (74%)]\tLoss: 391.634033\n",
            "Train Epoch: 0 [1080/1400 (77%)]\tLoss: 418.354248\n",
            "Train Epoch: 0 [1120/1400 (80%)]\tLoss: 385.981476\n",
            "Train Epoch: 0 [1160/1400 (83%)]\tLoss: 369.843781\n",
            "Train Epoch: 0 [1200/1400 (86%)]\tLoss: 300.276978\n",
            "Train Epoch: 0 [1240/1400 (89%)]\tLoss: 427.140381\n",
            "Train Epoch: 0 [1280/1400 (91%)]\tLoss: 337.773621\n",
            "Train Epoch: 0 [1320/1400 (94%)]\tLoss: 336.282867\n",
            "Train Epoch: 0 [1360/1400 (97%)]\tLoss: 266.131683\n",
            "====> Epoch: 0 Average loss: 433.5475\n",
            "====> Test set loss: 306.8696\n",
            "Train Epoch: 1 [0/1400 (0%)]\tLoss: 264.384369\n",
            "Train Epoch: 1 [40/1400 (3%)]\tLoss: 349.134644\n",
            "Train Epoch: 1 [80/1400 (6%)]\tLoss: 260.894928\n",
            "Train Epoch: 1 [120/1400 (9%)]\tLoss: 256.776245\n",
            "Train Epoch: 1 [160/1400 (11%)]\tLoss: 310.274139\n",
            "Train Epoch: 1 [200/1400 (14%)]\tLoss: 248.711548\n",
            "Train Epoch: 1 [240/1400 (17%)]\tLoss: 301.286682\n",
            "Train Epoch: 1 [280/1400 (20%)]\tLoss: 294.912231\n",
            "Train Epoch: 1 [320/1400 (23%)]\tLoss: 281.191711\n",
            "Train Epoch: 1 [360/1400 (26%)]\tLoss: 249.653778\n",
            "Train Epoch: 1 [400/1400 (29%)]\tLoss: 247.160629\n",
            "Train Epoch: 1 [440/1400 (31%)]\tLoss: 237.629822\n",
            "Train Epoch: 1 [480/1400 (34%)]\tLoss: 242.515869\n",
            "Train Epoch: 1 [520/1400 (37%)]\tLoss: 221.095993\n",
            "Train Epoch: 1 [560/1400 (40%)]\tLoss: 241.958466\n",
            "Train Epoch: 1 [600/1400 (43%)]\tLoss: 222.117737\n",
            "Train Epoch: 1 [640/1400 (46%)]\tLoss: 235.213776\n",
            "Train Epoch: 1 [680/1400 (49%)]\tLoss: 215.259216\n",
            "Train Epoch: 1 [720/1400 (51%)]\tLoss: 186.346664\n",
            "Train Epoch: 1 [760/1400 (54%)]\tLoss: 288.215973\n",
            "Train Epoch: 1 [800/1400 (57%)]\tLoss: 196.238297\n",
            "Train Epoch: 1 [840/1400 (60%)]\tLoss: 197.963242\n",
            "Train Epoch: 1 [880/1400 (63%)]\tLoss: 262.352936\n",
            "Train Epoch: 1 [920/1400 (66%)]\tLoss: 185.446274\n",
            "Train Epoch: 1 [960/1400 (69%)]\tLoss: 219.163132\n",
            "Train Epoch: 1 [1000/1400 (71%)]\tLoss: 209.447876\n",
            "Train Epoch: 1 [1040/1400 (74%)]\tLoss: 204.040695\n",
            "Train Epoch: 1 [1080/1400 (77%)]\tLoss: 202.047943\n",
            "Train Epoch: 1 [1120/1400 (80%)]\tLoss: 196.339081\n",
            "Train Epoch: 1 [1160/1400 (83%)]\tLoss: 185.514511\n",
            "Train Epoch: 1 [1200/1400 (86%)]\tLoss: 209.009491\n",
            "Train Epoch: 1 [1240/1400 (89%)]\tLoss: 188.123901\n",
            "Train Epoch: 1 [1280/1400 (91%)]\tLoss: 193.248932\n",
            "Train Epoch: 1 [1320/1400 (94%)]\tLoss: 245.057617\n",
            "Train Epoch: 1 [1360/1400 (97%)]\tLoss: 177.427887\n",
            "====> Epoch: 1 Average loss: 232.0919\n",
            "====> Test set loss: 195.0034\n",
            "Train Epoch: 2 [0/1400 (0%)]\tLoss: 218.059753\n",
            "Train Epoch: 2 [40/1400 (3%)]\tLoss: 169.346039\n",
            "Train Epoch: 2 [80/1400 (6%)]\tLoss: 177.760864\n",
            "Train Epoch: 2 [120/1400 (9%)]\tLoss: 207.340195\n",
            "Train Epoch: 2 [160/1400 (11%)]\tLoss: 160.027298\n",
            "Train Epoch: 2 [200/1400 (14%)]\tLoss: 195.049347\n",
            "Train Epoch: 2 [240/1400 (17%)]\tLoss: 192.686752\n",
            "Train Epoch: 2 [280/1400 (20%)]\tLoss: 181.656586\n",
            "Train Epoch: 2 [320/1400 (23%)]\tLoss: 205.935913\n",
            "Train Epoch: 2 [360/1400 (26%)]\tLoss: 215.595490\n",
            "Train Epoch: 2 [400/1400 (29%)]\tLoss: 197.926346\n",
            "Train Epoch: 2 [440/1400 (31%)]\tLoss: 159.488312\n",
            "Train Epoch: 2 [480/1400 (34%)]\tLoss: 255.222687\n",
            "Train Epoch: 2 [520/1400 (37%)]\tLoss: 170.029526\n",
            "Train Epoch: 2 [560/1400 (40%)]\tLoss: 186.051361\n",
            "Train Epoch: 2 [600/1400 (43%)]\tLoss: 204.541000\n",
            "Train Epoch: 2 [640/1400 (46%)]\tLoss: 179.460205\n",
            "Train Epoch: 2 [680/1400 (49%)]\tLoss: 168.817596\n",
            "Train Epoch: 2 [720/1400 (51%)]\tLoss: 163.149918\n",
            "Train Epoch: 2 [760/1400 (54%)]\tLoss: 163.761703\n",
            "Train Epoch: 2 [800/1400 (57%)]\tLoss: 173.841507\n",
            "Train Epoch: 2 [840/1400 (60%)]\tLoss: 156.264618\n",
            "Train Epoch: 2 [880/1400 (63%)]\tLoss: 211.464279\n",
            "Train Epoch: 2 [920/1400 (66%)]\tLoss: 152.913559\n",
            "Train Epoch: 2 [960/1400 (69%)]\tLoss: 165.173889\n",
            "Train Epoch: 2 [1000/1400 (71%)]\tLoss: 191.065445\n",
            "Train Epoch: 2 [1040/1400 (74%)]\tLoss: 180.502441\n",
            "Train Epoch: 2 [1080/1400 (77%)]\tLoss: 161.904205\n",
            "Train Epoch: 2 [1120/1400 (80%)]\tLoss: 160.817856\n",
            "Train Epoch: 2 [1160/1400 (83%)]\tLoss: 160.089600\n",
            "Train Epoch: 2 [1200/1400 (86%)]\tLoss: 218.535492\n",
            "Train Epoch: 2 [1240/1400 (89%)]\tLoss: 195.657928\n",
            "Train Epoch: 2 [1280/1400 (91%)]\tLoss: 179.209641\n",
            "Train Epoch: 2 [1320/1400 (94%)]\tLoss: 196.715530\n",
            "Train Epoch: 2 [1360/1400 (97%)]\tLoss: 195.802826\n",
            "====> Epoch: 2 Average loss: 192.6873\n",
            "====> Test set loss: 186.7056\n",
            "Train Epoch: 3 [0/1400 (0%)]\tLoss: 226.952942\n",
            "Train Epoch: 3 [40/1400 (3%)]\tLoss: 169.731827\n",
            "Train Epoch: 3 [80/1400 (6%)]\tLoss: 219.123642\n",
            "Train Epoch: 3 [120/1400 (9%)]\tLoss: 190.101273\n",
            "Train Epoch: 3 [160/1400 (11%)]\tLoss: 195.329208\n",
            "Train Epoch: 3 [200/1400 (14%)]\tLoss: 158.678772\n",
            "Train Epoch: 3 [240/1400 (17%)]\tLoss: 179.252350\n",
            "Train Epoch: 3 [280/1400 (20%)]\tLoss: 195.837799\n",
            "Train Epoch: 3 [320/1400 (23%)]\tLoss: 148.673950\n",
            "Train Epoch: 3 [360/1400 (26%)]\tLoss: 200.268250\n",
            "Train Epoch: 3 [400/1400 (29%)]\tLoss: 153.992615\n",
            "Train Epoch: 3 [440/1400 (31%)]\tLoss: 225.226059\n",
            "Train Epoch: 3 [480/1400 (34%)]\tLoss: 205.481216\n",
            "Train Epoch: 3 [520/1400 (37%)]\tLoss: 257.504578\n",
            "Train Epoch: 3 [560/1400 (40%)]\tLoss: 206.591660\n",
            "Train Epoch: 3 [600/1400 (43%)]\tLoss: 180.247223\n",
            "Train Epoch: 3 [640/1400 (46%)]\tLoss: 216.714279\n",
            "Train Epoch: 3 [680/1400 (49%)]\tLoss: 167.042938\n",
            "Train Epoch: 3 [720/1400 (51%)]\tLoss: 192.246475\n",
            "Train Epoch: 3 [760/1400 (54%)]\tLoss: 167.594193\n",
            "Train Epoch: 3 [800/1400 (57%)]\tLoss: 181.896408\n",
            "Train Epoch: 3 [840/1400 (60%)]\tLoss: 171.815811\n",
            "Train Epoch: 3 [880/1400 (63%)]\tLoss: 164.267166\n",
            "Train Epoch: 3 [920/1400 (66%)]\tLoss: 206.040283\n",
            "Train Epoch: 3 [960/1400 (69%)]\tLoss: 185.448471\n",
            "Train Epoch: 3 [1000/1400 (71%)]\tLoss: 191.091812\n",
            "Train Epoch: 3 [1040/1400 (74%)]\tLoss: 147.972397\n",
            "Train Epoch: 3 [1080/1400 (77%)]\tLoss: 198.327393\n",
            "Train Epoch: 3 [1120/1400 (80%)]\tLoss: 163.148163\n",
            "Train Epoch: 3 [1160/1400 (83%)]\tLoss: 200.542557\n",
            "Train Epoch: 3 [1200/1400 (86%)]\tLoss: 187.180908\n",
            "Train Epoch: 3 [1240/1400 (89%)]\tLoss: 263.504364\n",
            "Train Epoch: 3 [1280/1400 (91%)]\tLoss: 169.240891\n",
            "Train Epoch: 3 [1320/1400 (94%)]\tLoss: 194.713333\n",
            "Train Epoch: 3 [1360/1400 (97%)]\tLoss: 148.700073\n",
            "====> Epoch: 3 Average loss: 185.8106\n",
            "====> Test set loss: 182.6297\n",
            "Train Epoch: 4 [0/1400 (0%)]\tLoss: 170.418915\n",
            "Train Epoch: 4 [40/1400 (3%)]\tLoss: 179.320663\n",
            "Train Epoch: 4 [80/1400 (6%)]\tLoss: 205.083099\n",
            "Train Epoch: 4 [120/1400 (9%)]\tLoss: 175.510452\n",
            "Train Epoch: 4 [160/1400 (11%)]\tLoss: 228.831604\n",
            "Train Epoch: 4 [200/1400 (14%)]\tLoss: 197.166702\n",
            "Train Epoch: 4 [240/1400 (17%)]\tLoss: 152.162354\n",
            "Train Epoch: 4 [280/1400 (20%)]\tLoss: 176.205795\n",
            "Train Epoch: 4 [320/1400 (23%)]\tLoss: 275.119934\n",
            "Train Epoch: 4 [360/1400 (26%)]\tLoss: 162.581741\n",
            "Train Epoch: 4 [400/1400 (29%)]\tLoss: 147.725433\n",
            "Train Epoch: 4 [440/1400 (31%)]\tLoss: 184.186737\n",
            "Train Epoch: 4 [480/1400 (34%)]\tLoss: 181.364502\n",
            "Train Epoch: 4 [520/1400 (37%)]\tLoss: 185.455338\n",
            "Train Epoch: 4 [560/1400 (40%)]\tLoss: 148.551559\n",
            "Train Epoch: 4 [600/1400 (43%)]\tLoss: 182.496628\n",
            "Train Epoch: 4 [640/1400 (46%)]\tLoss: 153.671158\n",
            "Train Epoch: 4 [680/1400 (49%)]\tLoss: 183.173508\n",
            "Train Epoch: 4 [720/1400 (51%)]\tLoss: 187.041885\n",
            "Train Epoch: 4 [760/1400 (54%)]\tLoss: 205.469574\n",
            "Train Epoch: 4 [800/1400 (57%)]\tLoss: 165.979416\n",
            "Train Epoch: 4 [840/1400 (60%)]\tLoss: 195.365341\n",
            "Train Epoch: 4 [880/1400 (63%)]\tLoss: 212.217407\n",
            "Train Epoch: 4 [920/1400 (66%)]\tLoss: 174.918701\n",
            "Train Epoch: 4 [960/1400 (69%)]\tLoss: 225.045303\n",
            "Train Epoch: 4 [1000/1400 (71%)]\tLoss: 160.877625\n",
            "Train Epoch: 4 [1040/1400 (74%)]\tLoss: 172.708801\n",
            "Train Epoch: 4 [1080/1400 (77%)]\tLoss: 134.417984\n",
            "Train Epoch: 4 [1120/1400 (80%)]\tLoss: 155.900177\n",
            "Train Epoch: 4 [1160/1400 (83%)]\tLoss: 165.841690\n",
            "Train Epoch: 4 [1200/1400 (86%)]\tLoss: 157.794052\n",
            "Train Epoch: 4 [1240/1400 (89%)]\tLoss: 151.295486\n",
            "Train Epoch: 4 [1280/1400 (91%)]\tLoss: 223.677536\n",
            "Train Epoch: 4 [1320/1400 (94%)]\tLoss: 234.865509\n",
            "Train Epoch: 4 [1360/1400 (97%)]\tLoss: 178.601578\n",
            "====> Epoch: 4 Average loss: 179.6596\n",
            "====> Test set loss: 175.6509\n",
            "Train Epoch: 5 [0/1400 (0%)]\tLoss: 155.106430\n",
            "Train Epoch: 5 [40/1400 (3%)]\tLoss: 210.637711\n",
            "Train Epoch: 5 [80/1400 (6%)]\tLoss: 232.106583\n",
            "Train Epoch: 5 [120/1400 (9%)]\tLoss: 167.813339\n",
            "Train Epoch: 5 [160/1400 (11%)]\tLoss: 212.001892\n",
            "Train Epoch: 5 [200/1400 (14%)]\tLoss: 187.515060\n",
            "Train Epoch: 5 [240/1400 (17%)]\tLoss: 150.455643\n",
            "Train Epoch: 5 [280/1400 (20%)]\tLoss: 149.092468\n",
            "Train Epoch: 5 [320/1400 (23%)]\tLoss: 146.862656\n",
            "Train Epoch: 5 [360/1400 (26%)]\tLoss: 164.650101\n",
            "Train Epoch: 5 [400/1400 (29%)]\tLoss: 159.173340\n",
            "Train Epoch: 5 [440/1400 (31%)]\tLoss: 192.743240\n",
            "Train Epoch: 5 [480/1400 (34%)]\tLoss: 200.706543\n",
            "Train Epoch: 5 [520/1400 (37%)]\tLoss: 136.378799\n",
            "Train Epoch: 5 [560/1400 (40%)]\tLoss: 208.645020\n",
            "Train Epoch: 5 [600/1400 (43%)]\tLoss: 205.168381\n",
            "Train Epoch: 5 [640/1400 (46%)]\tLoss: 158.583618\n",
            "Train Epoch: 5 [680/1400 (49%)]\tLoss: 149.694916\n",
            "Train Epoch: 5 [720/1400 (51%)]\tLoss: 199.924484\n",
            "Train Epoch: 5 [760/1400 (54%)]\tLoss: 153.332855\n",
            "Train Epoch: 5 [800/1400 (57%)]\tLoss: 228.259659\n",
            "Train Epoch: 5 [840/1400 (60%)]\tLoss: 157.536514\n",
            "Train Epoch: 5 [880/1400 (63%)]\tLoss: 168.094131\n",
            "Train Epoch: 5 [920/1400 (66%)]\tLoss: 191.025345\n",
            "Train Epoch: 5 [960/1400 (69%)]\tLoss: 192.784668\n",
            "Train Epoch: 5 [1000/1400 (71%)]\tLoss: 145.129425\n",
            "Train Epoch: 5 [1040/1400 (74%)]\tLoss: 172.875443\n",
            "Train Epoch: 5 [1080/1400 (77%)]\tLoss: 159.241562\n",
            "Train Epoch: 5 [1120/1400 (80%)]\tLoss: 149.419205\n",
            "Train Epoch: 5 [1160/1400 (83%)]\tLoss: 144.052841\n",
            "Train Epoch: 5 [1200/1400 (86%)]\tLoss: 181.591019\n",
            "Train Epoch: 5 [1240/1400 (89%)]\tLoss: 167.813873\n",
            "Train Epoch: 5 [1280/1400 (91%)]\tLoss: 137.742035\n",
            "Train Epoch: 5 [1320/1400 (94%)]\tLoss: 238.396179\n",
            "Train Epoch: 5 [1360/1400 (97%)]\tLoss: 191.032990\n",
            "====> Epoch: 5 Average loss: 171.4830\n",
            "====> Test set loss: 166.8949\n",
            "Train Epoch: 6 [0/1400 (0%)]\tLoss: 166.673187\n",
            "Train Epoch: 6 [40/1400 (3%)]\tLoss: 169.218842\n",
            "Train Epoch: 6 [80/1400 (6%)]\tLoss: 164.760712\n",
            "Train Epoch: 6 [120/1400 (9%)]\tLoss: 159.065842\n",
            "Train Epoch: 6 [160/1400 (11%)]\tLoss: 140.952469\n",
            "Train Epoch: 6 [200/1400 (14%)]\tLoss: 173.779694\n",
            "Train Epoch: 6 [240/1400 (17%)]\tLoss: 166.282425\n",
            "Train Epoch: 6 [280/1400 (20%)]\tLoss: 161.718826\n",
            "Train Epoch: 6 [320/1400 (23%)]\tLoss: 137.424088\n",
            "Train Epoch: 6 [360/1400 (26%)]\tLoss: 162.194168\n",
            "Train Epoch: 6 [400/1400 (29%)]\tLoss: 175.305908\n",
            "Train Epoch: 6 [440/1400 (31%)]\tLoss: 140.705566\n",
            "Train Epoch: 6 [480/1400 (34%)]\tLoss: 179.541916\n",
            "Train Epoch: 6 [520/1400 (37%)]\tLoss: 172.522476\n",
            "Train Epoch: 6 [560/1400 (40%)]\tLoss: 152.193756\n",
            "Train Epoch: 6 [600/1400 (43%)]\tLoss: 163.622971\n",
            "Train Epoch: 6 [640/1400 (46%)]\tLoss: 157.730484\n",
            "Train Epoch: 6 [680/1400 (49%)]\tLoss: 188.360168\n",
            "Train Epoch: 6 [720/1400 (51%)]\tLoss: 170.288361\n",
            "Train Epoch: 6 [760/1400 (54%)]\tLoss: 143.202499\n",
            "Train Epoch: 6 [800/1400 (57%)]\tLoss: 174.015381\n",
            "Train Epoch: 6 [840/1400 (60%)]\tLoss: 162.180405\n",
            "Train Epoch: 6 [880/1400 (63%)]\tLoss: 172.601913\n",
            "Train Epoch: 6 [920/1400 (66%)]\tLoss: 152.655182\n",
            "Train Epoch: 6 [960/1400 (69%)]\tLoss: 147.091812\n",
            "Train Epoch: 6 [1000/1400 (71%)]\tLoss: 191.152390\n",
            "Train Epoch: 6 [1040/1400 (74%)]\tLoss: 153.521088\n",
            "Train Epoch: 6 [1080/1400 (77%)]\tLoss: 166.003250\n",
            "Train Epoch: 6 [1120/1400 (80%)]\tLoss: 156.994690\n",
            "Train Epoch: 6 [1160/1400 (83%)]\tLoss: 149.224426\n",
            "Train Epoch: 6 [1200/1400 (86%)]\tLoss: 167.318466\n",
            "Train Epoch: 6 [1240/1400 (89%)]\tLoss: 190.569763\n",
            "Train Epoch: 6 [1280/1400 (91%)]\tLoss: 157.702087\n",
            "Train Epoch: 6 [1320/1400 (94%)]\tLoss: 141.867828\n",
            "Train Epoch: 6 [1360/1400 (97%)]\tLoss: 151.635757\n",
            "====> Epoch: 6 Average loss: 162.5051\n",
            "====> Test set loss: 160.1026\n",
            "Train Epoch: 7 [0/1400 (0%)]\tLoss: 160.609970\n",
            "Train Epoch: 7 [40/1400 (3%)]\tLoss: 160.273285\n",
            "Train Epoch: 7 [80/1400 (6%)]\tLoss: 143.049332\n",
            "Train Epoch: 7 [120/1400 (9%)]\tLoss: 151.982681\n",
            "Train Epoch: 7 [160/1400 (11%)]\tLoss: 159.237320\n",
            "Train Epoch: 7 [200/1400 (14%)]\tLoss: 142.202179\n",
            "Train Epoch: 7 [240/1400 (17%)]\tLoss: 171.841522\n",
            "Train Epoch: 7 [280/1400 (20%)]\tLoss: 135.456558\n",
            "Train Epoch: 7 [320/1400 (23%)]\tLoss: 141.241074\n",
            "Train Epoch: 7 [360/1400 (26%)]\tLoss: 137.662537\n",
            "Train Epoch: 7 [400/1400 (29%)]\tLoss: 150.475891\n",
            "Train Epoch: 7 [440/1400 (31%)]\tLoss: 161.525742\n",
            "Train Epoch: 7 [480/1400 (34%)]\tLoss: 209.150497\n",
            "Train Epoch: 7 [520/1400 (37%)]\tLoss: 148.101959\n",
            "Train Epoch: 7 [560/1400 (40%)]\tLoss: 143.415070\n",
            "Train Epoch: 7 [600/1400 (43%)]\tLoss: 158.117920\n",
            "Train Epoch: 7 [640/1400 (46%)]\tLoss: 149.392151\n",
            "Train Epoch: 7 [680/1400 (49%)]\tLoss: 148.010468\n",
            "Train Epoch: 7 [720/1400 (51%)]\tLoss: 141.014801\n",
            "Train Epoch: 7 [760/1400 (54%)]\tLoss: 140.958633\n",
            "Train Epoch: 7 [800/1400 (57%)]\tLoss: 140.466278\n",
            "Train Epoch: 7 [840/1400 (60%)]\tLoss: 146.426941\n",
            "Train Epoch: 7 [880/1400 (63%)]\tLoss: 169.290894\n",
            "Train Epoch: 7 [920/1400 (66%)]\tLoss: 160.451523\n",
            "Train Epoch: 7 [960/1400 (69%)]\tLoss: 178.685532\n",
            "Train Epoch: 7 [1000/1400 (71%)]\tLoss: 152.431717\n",
            "Train Epoch: 7 [1040/1400 (74%)]\tLoss: 187.770721\n",
            "Train Epoch: 7 [1080/1400 (77%)]\tLoss: 155.274139\n",
            "Train Epoch: 7 [1120/1400 (80%)]\tLoss: 123.892403\n",
            "Train Epoch: 7 [1160/1400 (83%)]\tLoss: 202.661453\n",
            "Train Epoch: 7 [1200/1400 (86%)]\tLoss: 181.601517\n",
            "Train Epoch: 7 [1240/1400 (89%)]\tLoss: 156.346024\n",
            "Train Epoch: 7 [1280/1400 (91%)]\tLoss: 172.872650\n",
            "Train Epoch: 7 [1320/1400 (94%)]\tLoss: 179.005219\n",
            "Train Epoch: 7 [1360/1400 (97%)]\tLoss: 143.351379\n",
            "====> Epoch: 7 Average loss: 156.1437\n",
            "====> Test set loss: 154.7862\n",
            "Train Epoch: 8 [0/1400 (0%)]\tLoss: 143.955597\n",
            "Train Epoch: 8 [40/1400 (3%)]\tLoss: 179.681763\n",
            "Train Epoch: 8 [80/1400 (6%)]\tLoss: 166.938034\n",
            "Train Epoch: 8 [120/1400 (9%)]\tLoss: 152.748093\n",
            "Train Epoch: 8 [160/1400 (11%)]\tLoss: 160.321671\n",
            "Train Epoch: 8 [200/1400 (14%)]\tLoss: 148.173981\n",
            "Train Epoch: 8 [240/1400 (17%)]\tLoss: 131.281982\n",
            "Train Epoch: 8 [280/1400 (20%)]\tLoss: 160.730225\n",
            "Train Epoch: 8 [320/1400 (23%)]\tLoss: 174.112015\n",
            "Train Epoch: 8 [360/1400 (26%)]\tLoss: 157.331329\n",
            "Train Epoch: 8 [400/1400 (29%)]\tLoss: 155.674240\n",
            "Train Epoch: 8 [440/1400 (31%)]\tLoss: 183.650665\n",
            "Train Epoch: 8 [480/1400 (34%)]\tLoss: 148.601364\n",
            "Train Epoch: 8 [520/1400 (37%)]\tLoss: 143.475784\n",
            "Train Epoch: 8 [560/1400 (40%)]\tLoss: 134.550613\n",
            "Train Epoch: 8 [600/1400 (43%)]\tLoss: 162.291306\n",
            "Train Epoch: 8 [640/1400 (46%)]\tLoss: 145.410400\n",
            "Train Epoch: 8 [680/1400 (49%)]\tLoss: 158.705734\n",
            "Train Epoch: 8 [720/1400 (51%)]\tLoss: 183.217804\n",
            "Train Epoch: 8 [760/1400 (54%)]\tLoss: 167.785965\n",
            "Train Epoch: 8 [800/1400 (57%)]\tLoss: 150.976166\n",
            "Train Epoch: 8 [840/1400 (60%)]\tLoss: 140.959686\n",
            "Train Epoch: 8 [880/1400 (63%)]\tLoss: 140.028015\n",
            "Train Epoch: 8 [920/1400 (66%)]\tLoss: 153.824646\n",
            "Train Epoch: 8 [960/1400 (69%)]\tLoss: 164.935944\n",
            "Train Epoch: 8 [1000/1400 (71%)]\tLoss: 152.868698\n",
            "Train Epoch: 8 [1040/1400 (74%)]\tLoss: 154.325653\n",
            "Train Epoch: 8 [1080/1400 (77%)]\tLoss: 152.879471\n",
            "Train Epoch: 8 [1120/1400 (80%)]\tLoss: 155.434891\n",
            "Train Epoch: 8 [1160/1400 (83%)]\tLoss: 176.170990\n",
            "Train Epoch: 8 [1200/1400 (86%)]\tLoss: 153.670197\n",
            "Train Epoch: 8 [1240/1400 (89%)]\tLoss: 208.287430\n",
            "Train Epoch: 8 [1280/1400 (91%)]\tLoss: 152.316040\n",
            "Train Epoch: 8 [1320/1400 (94%)]\tLoss: 172.121246\n",
            "Train Epoch: 8 [1360/1400 (97%)]\tLoss: 162.314880\n",
            "====> Epoch: 8 Average loss: 153.5015\n",
            "====> Test set loss: 152.8304\n",
            "Train Epoch: 9 [0/1400 (0%)]\tLoss: 155.264023\n",
            "Train Epoch: 9 [40/1400 (3%)]\tLoss: 154.926117\n",
            "Train Epoch: 9 [80/1400 (6%)]\tLoss: 151.947250\n",
            "Train Epoch: 9 [120/1400 (9%)]\tLoss: 147.111572\n",
            "Train Epoch: 9 [160/1400 (11%)]\tLoss: 147.142029\n",
            "Train Epoch: 9 [200/1400 (14%)]\tLoss: 153.043839\n",
            "Train Epoch: 9 [240/1400 (17%)]\tLoss: 139.948044\n",
            "Train Epoch: 9 [280/1400 (20%)]\tLoss: 156.109543\n",
            "Train Epoch: 9 [320/1400 (23%)]\tLoss: 164.870377\n",
            "Train Epoch: 9 [360/1400 (26%)]\tLoss: 158.087387\n",
            "Train Epoch: 9 [400/1400 (29%)]\tLoss: 164.789734\n",
            "Train Epoch: 9 [440/1400 (31%)]\tLoss: 136.977676\n",
            "Train Epoch: 9 [480/1400 (34%)]\tLoss: 178.409927\n",
            "Train Epoch: 9 [520/1400 (37%)]\tLoss: 150.585571\n",
            "Train Epoch: 9 [560/1400 (40%)]\tLoss: 141.972183\n",
            "Train Epoch: 9 [600/1400 (43%)]\tLoss: 152.697281\n",
            "Train Epoch: 9 [640/1400 (46%)]\tLoss: 187.667831\n",
            "Train Epoch: 9 [680/1400 (49%)]\tLoss: 161.408569\n",
            "Train Epoch: 9 [720/1400 (51%)]\tLoss: 164.582779\n",
            "Train Epoch: 9 [760/1400 (54%)]\tLoss: 142.027557\n",
            "Train Epoch: 9 [800/1400 (57%)]\tLoss: 155.463531\n",
            "Train Epoch: 9 [840/1400 (60%)]\tLoss: 144.029037\n",
            "Train Epoch: 9 [880/1400 (63%)]\tLoss: 135.929733\n",
            "Train Epoch: 9 [920/1400 (66%)]\tLoss: 146.634644\n",
            "Train Epoch: 9 [960/1400 (69%)]\tLoss: 152.520767\n",
            "Train Epoch: 9 [1000/1400 (71%)]\tLoss: 132.790436\n",
            "Train Epoch: 9 [1040/1400 (74%)]\tLoss: 149.423157\n",
            "Train Epoch: 9 [1080/1400 (77%)]\tLoss: 149.144058\n",
            "Train Epoch: 9 [1120/1400 (80%)]\tLoss: 190.436234\n",
            "Train Epoch: 9 [1160/1400 (83%)]\tLoss: 122.534393\n",
            "Train Epoch: 9 [1200/1400 (86%)]\tLoss: 146.143250\n",
            "Train Epoch: 9 [1240/1400 (89%)]\tLoss: 153.038254\n",
            "Train Epoch: 9 [1280/1400 (91%)]\tLoss: 143.229645\n",
            "Train Epoch: 9 [1320/1400 (94%)]\tLoss: 131.476700\n",
            "Train Epoch: 9 [1360/1400 (97%)]\tLoss: 146.126709\n",
            "====> Epoch: 9 Average loss: 150.9688\n",
            "====> Test set loss: 151.7086\n",
            "Train Epoch: 10 [0/1400 (0%)]\tLoss: 146.236099\n",
            "Train Epoch: 10 [40/1400 (3%)]\tLoss: 139.967697\n",
            "Train Epoch: 10 [80/1400 (6%)]\tLoss: 152.399323\n",
            "Train Epoch: 10 [120/1400 (9%)]\tLoss: 191.134903\n",
            "Train Epoch: 10 [160/1400 (11%)]\tLoss: 118.523956\n",
            "Train Epoch: 10 [200/1400 (14%)]\tLoss: 124.113556\n",
            "Train Epoch: 10 [240/1400 (17%)]\tLoss: 164.164032\n",
            "Train Epoch: 10 [280/1400 (20%)]\tLoss: 146.181412\n",
            "Train Epoch: 10 [320/1400 (23%)]\tLoss: 131.553513\n",
            "Train Epoch: 10 [360/1400 (26%)]\tLoss: 158.158539\n",
            "Train Epoch: 10 [400/1400 (29%)]\tLoss: 131.544449\n",
            "Train Epoch: 10 [440/1400 (31%)]\tLoss: 143.698868\n",
            "Train Epoch: 10 [480/1400 (34%)]\tLoss: 132.677155\n",
            "Train Epoch: 10 [520/1400 (37%)]\tLoss: 136.211975\n",
            "Train Epoch: 10 [560/1400 (40%)]\tLoss: 142.559845\n",
            "Train Epoch: 10 [600/1400 (43%)]\tLoss: 134.459229\n",
            "Train Epoch: 10 [640/1400 (46%)]\tLoss: 143.356140\n",
            "Train Epoch: 10 [680/1400 (49%)]\tLoss: 150.922852\n",
            "Train Epoch: 10 [720/1400 (51%)]\tLoss: 134.970337\n",
            "Train Epoch: 10 [760/1400 (54%)]\tLoss: 142.840500\n",
            "Train Epoch: 10 [800/1400 (57%)]\tLoss: 145.140030\n",
            "Train Epoch: 10 [840/1400 (60%)]\tLoss: 133.398575\n",
            "Train Epoch: 10 [880/1400 (63%)]\tLoss: 157.377579\n",
            "Train Epoch: 10 [920/1400 (66%)]\tLoss: 144.777756\n",
            "Train Epoch: 10 [960/1400 (69%)]\tLoss: 147.071350\n",
            "Train Epoch: 10 [1000/1400 (71%)]\tLoss: 140.197678\n",
            "Train Epoch: 10 [1040/1400 (74%)]\tLoss: 136.285934\n",
            "Train Epoch: 10 [1080/1400 (77%)]\tLoss: 164.201584\n",
            "Train Epoch: 10 [1120/1400 (80%)]\tLoss: 170.570419\n",
            "Train Epoch: 10 [1160/1400 (83%)]\tLoss: 138.102615\n",
            "Train Epoch: 10 [1200/1400 (86%)]\tLoss: 156.586136\n",
            "Train Epoch: 10 [1240/1400 (89%)]\tLoss: 146.991333\n",
            "Train Epoch: 10 [1280/1400 (91%)]\tLoss: 182.173538\n",
            "Train Epoch: 10 [1320/1400 (94%)]\tLoss: 137.000687\n",
            "Train Epoch: 10 [1360/1400 (97%)]\tLoss: 162.656906\n",
            "====> Epoch: 10 Average loss: 149.6641\n",
            "====> Test set loss: 151.0895\n",
            "Train Epoch: 11 [0/1400 (0%)]\tLoss: 152.871704\n",
            "Train Epoch: 11 [40/1400 (3%)]\tLoss: 144.089188\n",
            "Train Epoch: 11 [80/1400 (6%)]\tLoss: 156.716873\n",
            "Train Epoch: 11 [120/1400 (9%)]\tLoss: 153.830490\n",
            "Train Epoch: 11 [160/1400 (11%)]\tLoss: 146.310516\n",
            "Train Epoch: 11 [200/1400 (14%)]\tLoss: 139.965744\n",
            "Train Epoch: 11 [240/1400 (17%)]\tLoss: 142.456970\n",
            "Train Epoch: 11 [280/1400 (20%)]\tLoss: 192.850159\n",
            "Train Epoch: 11 [320/1400 (23%)]\tLoss: 144.963806\n",
            "Train Epoch: 11 [360/1400 (26%)]\tLoss: 146.311874\n",
            "Train Epoch: 11 [400/1400 (29%)]\tLoss: 157.721268\n",
            "Train Epoch: 11 [440/1400 (31%)]\tLoss: 130.289078\n",
            "Train Epoch: 11 [480/1400 (34%)]\tLoss: 150.719345\n",
            "Train Epoch: 11 [520/1400 (37%)]\tLoss: 145.523117\n",
            "Train Epoch: 11 [560/1400 (40%)]\tLoss: 161.782593\n",
            "Train Epoch: 11 [600/1400 (43%)]\tLoss: 160.823730\n",
            "Train Epoch: 11 [640/1400 (46%)]\tLoss: 141.435593\n",
            "Train Epoch: 11 [680/1400 (49%)]\tLoss: 153.478195\n",
            "Train Epoch: 11 [720/1400 (51%)]\tLoss: 175.176071\n",
            "Train Epoch: 11 [760/1400 (54%)]\tLoss: 136.288803\n",
            "Train Epoch: 11 [800/1400 (57%)]\tLoss: 142.602417\n",
            "Train Epoch: 11 [840/1400 (60%)]\tLoss: 134.413849\n",
            "Train Epoch: 11 [880/1400 (63%)]\tLoss: 132.045807\n",
            "Train Epoch: 11 [920/1400 (66%)]\tLoss: 154.870514\n",
            "Train Epoch: 11 [960/1400 (69%)]\tLoss: 157.704865\n",
            "Train Epoch: 11 [1000/1400 (71%)]\tLoss: 162.567719\n",
            "Train Epoch: 11 [1040/1400 (74%)]\tLoss: 157.934341\n",
            "Train Epoch: 11 [1080/1400 (77%)]\tLoss: 155.679810\n",
            "Train Epoch: 11 [1120/1400 (80%)]\tLoss: 189.951126\n",
            "Train Epoch: 11 [1160/1400 (83%)]\tLoss: 156.048721\n",
            "Train Epoch: 11 [1200/1400 (86%)]\tLoss: 140.661270\n",
            "Train Epoch: 11 [1240/1400 (89%)]\tLoss: 138.365005\n",
            "Train Epoch: 11 [1280/1400 (91%)]\tLoss: 130.596420\n",
            "Train Epoch: 11 [1320/1400 (94%)]\tLoss: 150.747208\n",
            "Train Epoch: 11 [1360/1400 (97%)]\tLoss: 158.845551\n",
            "====> Epoch: 11 Average loss: 148.7842\n",
            "====> Test set loss: 150.3276\n",
            "Train Epoch: 12 [0/1400 (0%)]\tLoss: 158.892853\n",
            "Train Epoch: 12 [40/1400 (3%)]\tLoss: 161.570496\n",
            "Train Epoch: 12 [80/1400 (6%)]\tLoss: 137.769958\n",
            "Train Epoch: 12 [120/1400 (9%)]\tLoss: 140.312775\n",
            "Train Epoch: 12 [160/1400 (11%)]\tLoss: 147.655457\n",
            "Train Epoch: 12 [200/1400 (14%)]\tLoss: 160.204376\n",
            "Train Epoch: 12 [240/1400 (17%)]\tLoss: 140.837250\n",
            "Train Epoch: 12 [280/1400 (20%)]\tLoss: 135.302170\n",
            "Train Epoch: 12 [320/1400 (23%)]\tLoss: 155.704742\n",
            "Train Epoch: 12 [360/1400 (26%)]\tLoss: 141.388474\n",
            "Train Epoch: 12 [400/1400 (29%)]\tLoss: 148.280075\n",
            "Train Epoch: 12 [440/1400 (31%)]\tLoss: 125.955078\n",
            "Train Epoch: 12 [480/1400 (34%)]\tLoss: 128.735184\n",
            "Train Epoch: 12 [520/1400 (37%)]\tLoss: 133.165787\n",
            "Train Epoch: 12 [560/1400 (40%)]\tLoss: 111.044464\n",
            "Train Epoch: 12 [600/1400 (43%)]\tLoss: 148.042847\n",
            "Train Epoch: 12 [640/1400 (46%)]\tLoss: 142.006378\n",
            "Train Epoch: 12 [680/1400 (49%)]\tLoss: 151.231537\n",
            "Train Epoch: 12 [720/1400 (51%)]\tLoss: 170.060791\n",
            "Train Epoch: 12 [760/1400 (54%)]\tLoss: 148.947784\n",
            "Train Epoch: 12 [800/1400 (57%)]\tLoss: 123.251953\n",
            "Train Epoch: 12 [840/1400 (60%)]\tLoss: 142.865295\n",
            "Train Epoch: 12 [880/1400 (63%)]\tLoss: 142.665878\n",
            "Train Epoch: 12 [920/1400 (66%)]\tLoss: 142.959503\n",
            "Train Epoch: 12 [960/1400 (69%)]\tLoss: 132.060272\n",
            "Train Epoch: 12 [1000/1400 (71%)]\tLoss: 141.907028\n",
            "Train Epoch: 12 [1040/1400 (74%)]\tLoss: 148.303162\n",
            "Train Epoch: 12 [1080/1400 (77%)]\tLoss: 158.833160\n",
            "Train Epoch: 12 [1120/1400 (80%)]\tLoss: 140.328293\n",
            "Train Epoch: 12 [1160/1400 (83%)]\tLoss: 144.307709\n",
            "Train Epoch: 12 [1200/1400 (86%)]\tLoss: 151.853775\n",
            "Train Epoch: 12 [1240/1400 (89%)]\tLoss: 140.481949\n",
            "Train Epoch: 12 [1280/1400 (91%)]\tLoss: 138.158813\n",
            "Train Epoch: 12 [1320/1400 (94%)]\tLoss: 145.149780\n",
            "Train Epoch: 12 [1360/1400 (97%)]\tLoss: 156.729630\n",
            "====> Epoch: 12 Average loss: 148.1645\n",
            "====> Test set loss: 149.1405\n",
            "Train Epoch: 13 [0/1400 (0%)]\tLoss: 184.715439\n",
            "Train Epoch: 13 [40/1400 (3%)]\tLoss: 133.051208\n",
            "Train Epoch: 13 [80/1400 (6%)]\tLoss: 149.820755\n",
            "Train Epoch: 13 [120/1400 (9%)]\tLoss: 177.900040\n",
            "Train Epoch: 13 [160/1400 (11%)]\tLoss: 133.840271\n",
            "Train Epoch: 13 [200/1400 (14%)]\tLoss: 158.196686\n",
            "Train Epoch: 13 [240/1400 (17%)]\tLoss: 142.774979\n",
            "Train Epoch: 13 [280/1400 (20%)]\tLoss: 154.487091\n",
            "Train Epoch: 13 [320/1400 (23%)]\tLoss: 142.298248\n",
            "Train Epoch: 13 [360/1400 (26%)]\tLoss: 127.287811\n",
            "Train Epoch: 13 [400/1400 (29%)]\tLoss: 166.231522\n",
            "Train Epoch: 13 [440/1400 (31%)]\tLoss: 165.997177\n",
            "Train Epoch: 13 [480/1400 (34%)]\tLoss: 160.995972\n",
            "Train Epoch: 13 [520/1400 (37%)]\tLoss: 141.073212\n",
            "Train Epoch: 13 [560/1400 (40%)]\tLoss: 145.401413\n",
            "Train Epoch: 13 [600/1400 (43%)]\tLoss: 158.946579\n",
            "Train Epoch: 13 [640/1400 (46%)]\tLoss: 144.458740\n",
            "Train Epoch: 13 [680/1400 (49%)]\tLoss: 166.699356\n",
            "Train Epoch: 13 [720/1400 (51%)]\tLoss: 160.479553\n",
            "Train Epoch: 13 [760/1400 (54%)]\tLoss: 160.861816\n",
            "Train Epoch: 13 [800/1400 (57%)]\tLoss: 131.982010\n",
            "Train Epoch: 13 [840/1400 (60%)]\tLoss: 139.865204\n",
            "Train Epoch: 13 [880/1400 (63%)]\tLoss: 133.381042\n",
            "Train Epoch: 13 [920/1400 (66%)]\tLoss: 145.520798\n",
            "Train Epoch: 13 [960/1400 (69%)]\tLoss: 182.141022\n",
            "Train Epoch: 13 [1000/1400 (71%)]\tLoss: 166.089951\n",
            "Train Epoch: 13 [1040/1400 (74%)]\tLoss: 141.682007\n",
            "Train Epoch: 13 [1080/1400 (77%)]\tLoss: 143.983200\n",
            "Train Epoch: 13 [1120/1400 (80%)]\tLoss: 138.936646\n",
            "Train Epoch: 13 [1160/1400 (83%)]\tLoss: 150.872208\n",
            "Train Epoch: 13 [1200/1400 (86%)]\tLoss: 139.656097\n",
            "Train Epoch: 13 [1240/1400 (89%)]\tLoss: 140.284271\n",
            "Train Epoch: 13 [1280/1400 (91%)]\tLoss: 156.090744\n",
            "Train Epoch: 13 [1320/1400 (94%)]\tLoss: 142.839752\n",
            "Train Epoch: 13 [1360/1400 (97%)]\tLoss: 160.855499\n",
            "====> Epoch: 13 Average loss: 147.3371\n",
            "====> Test set loss: 149.3971\n",
            "Train Epoch: 14 [0/1400 (0%)]\tLoss: 147.739624\n",
            "Train Epoch: 14 [40/1400 (3%)]\tLoss: 150.110107\n",
            "Train Epoch: 14 [80/1400 (6%)]\tLoss: 151.951935\n",
            "Train Epoch: 14 [120/1400 (9%)]\tLoss: 141.976105\n",
            "Train Epoch: 14 [160/1400 (11%)]\tLoss: 184.126526\n",
            "Train Epoch: 14 [200/1400 (14%)]\tLoss: 140.661041\n",
            "Train Epoch: 14 [240/1400 (17%)]\tLoss: 150.666977\n",
            "Train Epoch: 14 [280/1400 (20%)]\tLoss: 145.191589\n",
            "Train Epoch: 14 [320/1400 (23%)]\tLoss: 141.635956\n",
            "Train Epoch: 14 [360/1400 (26%)]\tLoss: 137.912781\n",
            "Train Epoch: 14 [400/1400 (29%)]\tLoss: 119.539093\n",
            "Train Epoch: 14 [440/1400 (31%)]\tLoss: 145.317581\n",
            "Train Epoch: 14 [480/1400 (34%)]\tLoss: 133.743820\n",
            "Train Epoch: 14 [520/1400 (37%)]\tLoss: 167.802795\n",
            "Train Epoch: 14 [560/1400 (40%)]\tLoss: 141.236649\n",
            "Train Epoch: 14 [600/1400 (43%)]\tLoss: 134.841888\n",
            "Train Epoch: 14 [640/1400 (46%)]\tLoss: 150.356430\n",
            "Train Epoch: 14 [680/1400 (49%)]\tLoss: 159.026718\n",
            "Train Epoch: 14 [720/1400 (51%)]\tLoss: 152.927048\n",
            "Train Epoch: 14 [760/1400 (54%)]\tLoss: 127.412544\n",
            "Train Epoch: 14 [800/1400 (57%)]\tLoss: 131.992340\n",
            "Train Epoch: 14 [840/1400 (60%)]\tLoss: 151.724472\n",
            "Train Epoch: 14 [880/1400 (63%)]\tLoss: 138.892395\n",
            "Train Epoch: 14 [920/1400 (66%)]\tLoss: 146.631485\n",
            "Train Epoch: 14 [960/1400 (69%)]\tLoss: 145.307693\n",
            "Train Epoch: 14 [1000/1400 (71%)]\tLoss: 148.253769\n",
            "Train Epoch: 14 [1040/1400 (74%)]\tLoss: 137.535736\n",
            "Train Epoch: 14 [1080/1400 (77%)]\tLoss: 167.814896\n",
            "Train Epoch: 14 [1120/1400 (80%)]\tLoss: 155.709229\n",
            "Train Epoch: 14 [1160/1400 (83%)]\tLoss: 156.676514\n",
            "Train Epoch: 14 [1200/1400 (86%)]\tLoss: 151.980103\n",
            "Train Epoch: 14 [1240/1400 (89%)]\tLoss: 141.003113\n",
            "Train Epoch: 14 [1280/1400 (91%)]\tLoss: 134.306290\n",
            "Train Epoch: 14 [1320/1400 (94%)]\tLoss: 142.300415\n",
            "Train Epoch: 14 [1360/1400 (97%)]\tLoss: 148.036301\n",
            "====> Epoch: 14 Average loss: 146.7263\n",
            "====> Test set loss: 148.3988\n",
            "Train Epoch: 15 [0/1400 (0%)]\tLoss: 164.618835\n",
            "Train Epoch: 15 [40/1400 (3%)]\tLoss: 127.656013\n",
            "Train Epoch: 15 [80/1400 (6%)]\tLoss: 124.099525\n",
            "Train Epoch: 15 [120/1400 (9%)]\tLoss: 148.909256\n",
            "Train Epoch: 15 [160/1400 (11%)]\tLoss: 162.111359\n",
            "Train Epoch: 15 [200/1400 (14%)]\tLoss: 159.620789\n",
            "Train Epoch: 15 [240/1400 (17%)]\tLoss: 156.873535\n",
            "Train Epoch: 15 [280/1400 (20%)]\tLoss: 142.454880\n",
            "Train Epoch: 15 [320/1400 (23%)]\tLoss: 131.663757\n",
            "Train Epoch: 15 [360/1400 (26%)]\tLoss: 120.091911\n",
            "Train Epoch: 15 [400/1400 (29%)]\tLoss: 142.705978\n",
            "Train Epoch: 15 [440/1400 (31%)]\tLoss: 154.747574\n",
            "Train Epoch: 15 [480/1400 (34%)]\tLoss: 145.480591\n",
            "Train Epoch: 15 [520/1400 (37%)]\tLoss: 174.296555\n",
            "Train Epoch: 15 [560/1400 (40%)]\tLoss: 127.842705\n",
            "Train Epoch: 15 [600/1400 (43%)]\tLoss: 149.192719\n",
            "Train Epoch: 15 [640/1400 (46%)]\tLoss: 154.967743\n",
            "Train Epoch: 15 [680/1400 (49%)]\tLoss: 150.370529\n",
            "Train Epoch: 15 [720/1400 (51%)]\tLoss: 145.821228\n",
            "Train Epoch: 15 [760/1400 (54%)]\tLoss: 162.080948\n",
            "Train Epoch: 15 [800/1400 (57%)]\tLoss: 130.861908\n",
            "Train Epoch: 15 [840/1400 (60%)]\tLoss: 158.256943\n",
            "Train Epoch: 15 [880/1400 (63%)]\tLoss: 147.606949\n",
            "Train Epoch: 15 [920/1400 (66%)]\tLoss: 152.880264\n",
            "Train Epoch: 15 [960/1400 (69%)]\tLoss: 129.717438\n",
            "Train Epoch: 15 [1000/1400 (71%)]\tLoss: 138.699783\n",
            "Train Epoch: 15 [1040/1400 (74%)]\tLoss: 141.518311\n",
            "Train Epoch: 15 [1080/1400 (77%)]\tLoss: 143.001114\n",
            "Train Epoch: 15 [1120/1400 (80%)]\tLoss: 151.076996\n",
            "Train Epoch: 15 [1160/1400 (83%)]\tLoss: 173.243942\n",
            "Train Epoch: 15 [1200/1400 (86%)]\tLoss: 140.140335\n",
            "Train Epoch: 15 [1240/1400 (89%)]\tLoss: 152.398178\n",
            "Train Epoch: 15 [1280/1400 (91%)]\tLoss: 141.587952\n",
            "Train Epoch: 15 [1320/1400 (94%)]\tLoss: 132.516098\n",
            "Train Epoch: 15 [1360/1400 (97%)]\tLoss: 136.879196\n",
            "====> Epoch: 15 Average loss: 146.2409\n",
            "====> Test set loss: 148.8073\n",
            "Train Epoch: 16 [0/1400 (0%)]\tLoss: 138.135818\n",
            "Train Epoch: 16 [40/1400 (3%)]\tLoss: 126.716446\n",
            "Train Epoch: 16 [80/1400 (6%)]\tLoss: 131.133255\n",
            "Train Epoch: 16 [120/1400 (9%)]\tLoss: 139.212494\n",
            "Train Epoch: 16 [160/1400 (11%)]\tLoss: 136.464767\n",
            "Train Epoch: 16 [200/1400 (14%)]\tLoss: 148.988052\n",
            "Train Epoch: 16 [240/1400 (17%)]\tLoss: 165.301544\n",
            "Train Epoch: 16 [280/1400 (20%)]\tLoss: 139.812592\n",
            "Train Epoch: 16 [320/1400 (23%)]\tLoss: 148.664505\n",
            "Train Epoch: 16 [360/1400 (26%)]\tLoss: 137.578842\n",
            "Train Epoch: 16 [400/1400 (29%)]\tLoss: 139.606781\n",
            "Train Epoch: 16 [440/1400 (31%)]\tLoss: 141.595001\n",
            "Train Epoch: 16 [480/1400 (34%)]\tLoss: 121.665993\n",
            "Train Epoch: 16 [520/1400 (37%)]\tLoss: 150.029068\n",
            "Train Epoch: 16 [560/1400 (40%)]\tLoss: 153.900238\n",
            "Train Epoch: 16 [600/1400 (43%)]\tLoss: 140.126556\n",
            "Train Epoch: 16 [640/1400 (46%)]\tLoss: 157.790192\n",
            "Train Epoch: 16 [680/1400 (49%)]\tLoss: 132.469376\n",
            "Train Epoch: 16 [720/1400 (51%)]\tLoss: 142.852036\n",
            "Train Epoch: 16 [760/1400 (54%)]\tLoss: 148.873779\n",
            "Train Epoch: 16 [800/1400 (57%)]\tLoss: 154.961899\n",
            "Train Epoch: 16 [840/1400 (60%)]\tLoss: 142.801544\n",
            "Train Epoch: 16 [880/1400 (63%)]\tLoss: 137.733826\n",
            "Train Epoch: 16 [920/1400 (66%)]\tLoss: 138.883301\n",
            "Train Epoch: 16 [960/1400 (69%)]\tLoss: 138.293747\n",
            "Train Epoch: 16 [1000/1400 (71%)]\tLoss: 158.335663\n",
            "Train Epoch: 16 [1040/1400 (74%)]\tLoss: 149.908875\n",
            "Train Epoch: 16 [1080/1400 (77%)]\tLoss: 140.710602\n",
            "Train Epoch: 16 [1120/1400 (80%)]\tLoss: 139.789551\n",
            "Train Epoch: 16 [1160/1400 (83%)]\tLoss: 150.198517\n",
            "Train Epoch: 16 [1200/1400 (86%)]\tLoss: 134.961853\n",
            "Train Epoch: 16 [1240/1400 (89%)]\tLoss: 164.043060\n",
            "Train Epoch: 16 [1280/1400 (91%)]\tLoss: 156.280212\n",
            "Train Epoch: 16 [1320/1400 (94%)]\tLoss: 145.624680\n",
            "Train Epoch: 16 [1360/1400 (97%)]\tLoss: 153.056793\n",
            "====> Epoch: 16 Average loss: 145.7315\n",
            "====> Test set loss: 148.0584\n",
            "Train Epoch: 17 [0/1400 (0%)]\tLoss: 131.792007\n",
            "Train Epoch: 17 [40/1400 (3%)]\tLoss: 155.071838\n",
            "Train Epoch: 17 [80/1400 (6%)]\tLoss: 152.091385\n",
            "Train Epoch: 17 [120/1400 (9%)]\tLoss: 160.169006\n",
            "Train Epoch: 17 [160/1400 (11%)]\tLoss: 142.213882\n",
            "Train Epoch: 17 [200/1400 (14%)]\tLoss: 157.718903\n",
            "Train Epoch: 17 [240/1400 (17%)]\tLoss: 155.416168\n",
            "Train Epoch: 17 [280/1400 (20%)]\tLoss: 136.135162\n",
            "Train Epoch: 17 [320/1400 (23%)]\tLoss: 136.902588\n",
            "Train Epoch: 17 [360/1400 (26%)]\tLoss: 147.016373\n",
            "Train Epoch: 17 [400/1400 (29%)]\tLoss: 140.520706\n",
            "Train Epoch: 17 [440/1400 (31%)]\tLoss: 153.187897\n",
            "Train Epoch: 17 [480/1400 (34%)]\tLoss: 164.002029\n",
            "Train Epoch: 17 [520/1400 (37%)]\tLoss: 135.768539\n",
            "Train Epoch: 17 [560/1400 (40%)]\tLoss: 112.211502\n",
            "Train Epoch: 17 [600/1400 (43%)]\tLoss: 163.710785\n",
            "Train Epoch: 17 [640/1400 (46%)]\tLoss: 150.856262\n",
            "Train Epoch: 17 [680/1400 (49%)]\tLoss: 161.893219\n",
            "Train Epoch: 17 [720/1400 (51%)]\tLoss: 140.216736\n",
            "Train Epoch: 17 [760/1400 (54%)]\tLoss: 147.318985\n",
            "Train Epoch: 17 [800/1400 (57%)]\tLoss: 142.283508\n",
            "Train Epoch: 17 [840/1400 (60%)]\tLoss: 185.336487\n",
            "Train Epoch: 17 [880/1400 (63%)]\tLoss: 175.368103\n",
            "Train Epoch: 17 [920/1400 (66%)]\tLoss: 118.962624\n",
            "Train Epoch: 17 [960/1400 (69%)]\tLoss: 135.123077\n",
            "Train Epoch: 17 [1000/1400 (71%)]\tLoss: 131.766403\n",
            "Train Epoch: 17 [1040/1400 (74%)]\tLoss: 145.899780\n",
            "Train Epoch: 17 [1080/1400 (77%)]\tLoss: 163.579727\n",
            "Train Epoch: 17 [1120/1400 (80%)]\tLoss: 155.626785\n",
            "Train Epoch: 17 [1160/1400 (83%)]\tLoss: 128.728531\n",
            "Train Epoch: 17 [1200/1400 (86%)]\tLoss: 132.891388\n",
            "Train Epoch: 17 [1240/1400 (89%)]\tLoss: 139.842819\n",
            "Train Epoch: 17 [1280/1400 (91%)]\tLoss: 154.587097\n",
            "Train Epoch: 17 [1320/1400 (94%)]\tLoss: 142.795303\n",
            "Train Epoch: 17 [1360/1400 (97%)]\tLoss: 143.580078\n",
            "====> Epoch: 17 Average loss: 145.4699\n",
            "====> Test set loss: 147.8331\n",
            "Train Epoch: 18 [0/1400 (0%)]\tLoss: 126.615669\n",
            "Train Epoch: 18 [40/1400 (3%)]\tLoss: 140.980347\n",
            "Train Epoch: 18 [80/1400 (6%)]\tLoss: 154.201675\n",
            "Train Epoch: 18 [120/1400 (9%)]\tLoss: 150.512207\n",
            "Train Epoch: 18 [160/1400 (11%)]\tLoss: 149.453064\n",
            "Train Epoch: 18 [200/1400 (14%)]\tLoss: 157.520126\n",
            "Train Epoch: 18 [240/1400 (17%)]\tLoss: 131.642197\n",
            "Train Epoch: 18 [280/1400 (20%)]\tLoss: 145.204742\n",
            "Train Epoch: 18 [320/1400 (23%)]\tLoss: 144.064590\n",
            "Train Epoch: 18 [360/1400 (26%)]\tLoss: 149.350998\n",
            "Train Epoch: 18 [400/1400 (29%)]\tLoss: 142.848892\n",
            "Train Epoch: 18 [440/1400 (31%)]\tLoss: 158.951752\n",
            "Train Epoch: 18 [480/1400 (34%)]\tLoss: 140.233261\n",
            "Train Epoch: 18 [520/1400 (37%)]\tLoss: 148.902115\n",
            "Train Epoch: 18 [560/1400 (40%)]\tLoss: 139.661880\n",
            "Train Epoch: 18 [600/1400 (43%)]\tLoss: 122.796341\n",
            "Train Epoch: 18 [640/1400 (46%)]\tLoss: 143.515991\n",
            "Train Epoch: 18 [680/1400 (49%)]\tLoss: 141.079666\n",
            "Train Epoch: 18 [720/1400 (51%)]\tLoss: 164.938446\n",
            "Train Epoch: 18 [760/1400 (54%)]\tLoss: 144.778793\n",
            "Train Epoch: 18 [800/1400 (57%)]\tLoss: 133.418442\n",
            "Train Epoch: 18 [840/1400 (60%)]\tLoss: 165.896835\n",
            "Train Epoch: 18 [880/1400 (63%)]\tLoss: 158.270416\n",
            "Train Epoch: 18 [920/1400 (66%)]\tLoss: 137.751831\n",
            "Train Epoch: 18 [960/1400 (69%)]\tLoss: 164.137985\n",
            "Train Epoch: 18 [1000/1400 (71%)]\tLoss: 150.013596\n",
            "Train Epoch: 18 [1040/1400 (74%)]\tLoss: 132.932236\n",
            "Train Epoch: 18 [1080/1400 (77%)]\tLoss: 153.845352\n",
            "Train Epoch: 18 [1120/1400 (80%)]\tLoss: 163.952057\n",
            "Train Epoch: 18 [1160/1400 (83%)]\tLoss: 136.690796\n",
            "Train Epoch: 18 [1200/1400 (86%)]\tLoss: 152.856934\n",
            "Train Epoch: 18 [1240/1400 (89%)]\tLoss: 140.603546\n",
            "Train Epoch: 18 [1280/1400 (91%)]\tLoss: 143.950150\n",
            "Train Epoch: 18 [1320/1400 (94%)]\tLoss: 167.277100\n",
            "Train Epoch: 18 [1360/1400 (97%)]\tLoss: 162.959518\n",
            "====> Epoch: 18 Average loss: 144.8181\n",
            "====> Test set loss: 147.8982\n",
            "Train Epoch: 19 [0/1400 (0%)]\tLoss: 143.276260\n",
            "Train Epoch: 19 [40/1400 (3%)]\tLoss: 152.632187\n",
            "Train Epoch: 19 [80/1400 (6%)]\tLoss: 184.776459\n",
            "Train Epoch: 19 [120/1400 (9%)]\tLoss: 136.477478\n",
            "Train Epoch: 19 [160/1400 (11%)]\tLoss: 129.454987\n",
            "Train Epoch: 19 [200/1400 (14%)]\tLoss: 148.009048\n",
            "Train Epoch: 19 [240/1400 (17%)]\tLoss: 159.621155\n",
            "Train Epoch: 19 [280/1400 (20%)]\tLoss: 147.614380\n",
            "Train Epoch: 19 [320/1400 (23%)]\tLoss: 143.866943\n",
            "Train Epoch: 19 [360/1400 (26%)]\tLoss: 137.227005\n",
            "Train Epoch: 19 [400/1400 (29%)]\tLoss: 150.884384\n",
            "Train Epoch: 19 [440/1400 (31%)]\tLoss: 134.563187\n",
            "Train Epoch: 19 [480/1400 (34%)]\tLoss: 122.218353\n",
            "Train Epoch: 19 [520/1400 (37%)]\tLoss: 129.556992\n",
            "Train Epoch: 19 [560/1400 (40%)]\tLoss: 143.511444\n",
            "Train Epoch: 19 [600/1400 (43%)]\tLoss: 138.377686\n",
            "Train Epoch: 19 [640/1400 (46%)]\tLoss: 135.384094\n",
            "Train Epoch: 19 [680/1400 (49%)]\tLoss: 184.789062\n",
            "Train Epoch: 19 [720/1400 (51%)]\tLoss: 135.668091\n",
            "Train Epoch: 19 [760/1400 (54%)]\tLoss: 138.776306\n",
            "Train Epoch: 19 [800/1400 (57%)]\tLoss: 141.764481\n",
            "Train Epoch: 19 [840/1400 (60%)]\tLoss: 126.053749\n",
            "Train Epoch: 19 [880/1400 (63%)]\tLoss: 148.489136\n",
            "Train Epoch: 19 [920/1400 (66%)]\tLoss: 133.364929\n",
            "Train Epoch: 19 [960/1400 (69%)]\tLoss: 146.469910\n",
            "Train Epoch: 19 [1000/1400 (71%)]\tLoss: 144.219727\n",
            "Train Epoch: 19 [1040/1400 (74%)]\tLoss: 129.050385\n",
            "Train Epoch: 19 [1080/1400 (77%)]\tLoss: 134.613297\n",
            "Train Epoch: 19 [1120/1400 (80%)]\tLoss: 148.468750\n",
            "Train Epoch: 19 [1160/1400 (83%)]\tLoss: 131.768326\n",
            "Train Epoch: 19 [1200/1400 (86%)]\tLoss: 143.397919\n",
            "Train Epoch: 19 [1240/1400 (89%)]\tLoss: 127.055046\n",
            "Train Epoch: 19 [1280/1400 (91%)]\tLoss: 150.648499\n",
            "Train Epoch: 19 [1320/1400 (94%)]\tLoss: 154.580750\n",
            "Train Epoch: 19 [1360/1400 (97%)]\tLoss: 124.602867\n",
            "====> Epoch: 19 Average loss: 144.7187\n",
            "====> Test set loss: 147.4175\n",
            "Train Epoch: 20 [0/1400 (0%)]\tLoss: 139.154800\n",
            "Train Epoch: 20 [40/1400 (3%)]\tLoss: 143.367004\n",
            "Train Epoch: 20 [80/1400 (6%)]\tLoss: 133.477005\n",
            "Train Epoch: 20 [120/1400 (9%)]\tLoss: 150.037766\n",
            "Train Epoch: 20 [160/1400 (11%)]\tLoss: 139.578278\n",
            "Train Epoch: 20 [200/1400 (14%)]\tLoss: 153.431061\n",
            "Train Epoch: 20 [240/1400 (17%)]\tLoss: 141.248627\n",
            "Train Epoch: 20 [280/1400 (20%)]\tLoss: 139.980026\n",
            "Train Epoch: 20 [320/1400 (23%)]\tLoss: 153.094574\n",
            "Train Epoch: 20 [360/1400 (26%)]\tLoss: 147.327988\n",
            "Train Epoch: 20 [400/1400 (29%)]\tLoss: 152.299820\n",
            "Train Epoch: 20 [440/1400 (31%)]\tLoss: 147.050003\n",
            "Train Epoch: 20 [480/1400 (34%)]\tLoss: 163.052628\n",
            "Train Epoch: 20 [520/1400 (37%)]\tLoss: 158.656860\n",
            "Train Epoch: 20 [560/1400 (40%)]\tLoss: 149.195969\n",
            "Train Epoch: 20 [600/1400 (43%)]\tLoss: 152.688187\n",
            "Train Epoch: 20 [640/1400 (46%)]\tLoss: 170.823929\n",
            "Train Epoch: 20 [680/1400 (49%)]\tLoss: 155.563553\n",
            "Train Epoch: 20 [720/1400 (51%)]\tLoss: 166.828522\n",
            "Train Epoch: 20 [760/1400 (54%)]\tLoss: 173.221344\n",
            "Train Epoch: 20 [800/1400 (57%)]\tLoss: 147.809158\n",
            "Train Epoch: 20 [840/1400 (60%)]\tLoss: 155.148254\n",
            "Train Epoch: 20 [880/1400 (63%)]\tLoss: 141.949890\n",
            "Train Epoch: 20 [920/1400 (66%)]\tLoss: 135.819305\n",
            "Train Epoch: 20 [960/1400 (69%)]\tLoss: 147.877533\n",
            "Train Epoch: 20 [1000/1400 (71%)]\tLoss: 180.198868\n",
            "Train Epoch: 20 [1040/1400 (74%)]\tLoss: 160.064194\n",
            "Train Epoch: 20 [1080/1400 (77%)]\tLoss: 157.669968\n",
            "Train Epoch: 20 [1120/1400 (80%)]\tLoss: 141.932266\n",
            "Train Epoch: 20 [1160/1400 (83%)]\tLoss: 130.808762\n",
            "Train Epoch: 20 [1200/1400 (86%)]\tLoss: 150.461411\n",
            "Train Epoch: 20 [1240/1400 (89%)]\tLoss: 126.475769\n",
            "Train Epoch: 20 [1280/1400 (91%)]\tLoss: 127.963440\n",
            "Train Epoch: 20 [1320/1400 (94%)]\tLoss: 151.340240\n",
            "Train Epoch: 20 [1360/1400 (97%)]\tLoss: 134.383286\n",
            "====> Epoch: 20 Average loss: 144.4220\n",
            "====> Test set loss: 146.5395\n",
            "Train Epoch: 21 [0/1400 (0%)]\tLoss: 149.122375\n",
            "Train Epoch: 21 [40/1400 (3%)]\tLoss: 149.788284\n",
            "Train Epoch: 21 [80/1400 (6%)]\tLoss: 158.002472\n",
            "Train Epoch: 21 [120/1400 (9%)]\tLoss: 153.887238\n",
            "Train Epoch: 21 [160/1400 (11%)]\tLoss: 136.045212\n",
            "Train Epoch: 21 [200/1400 (14%)]\tLoss: 126.969940\n",
            "Train Epoch: 21 [240/1400 (17%)]\tLoss: 136.484543\n",
            "Train Epoch: 21 [280/1400 (20%)]\tLoss: 148.029175\n",
            "Train Epoch: 21 [320/1400 (23%)]\tLoss: 156.388214\n",
            "Train Epoch: 21 [360/1400 (26%)]\tLoss: 143.646973\n",
            "Train Epoch: 21 [400/1400 (29%)]\tLoss: 148.318970\n",
            "Train Epoch: 21 [440/1400 (31%)]\tLoss: 159.862000\n",
            "Train Epoch: 21 [480/1400 (34%)]\tLoss: 143.477966\n",
            "Train Epoch: 21 [520/1400 (37%)]\tLoss: 135.361847\n",
            "Train Epoch: 21 [560/1400 (40%)]\tLoss: 163.771561\n",
            "Train Epoch: 21 [600/1400 (43%)]\tLoss: 168.457138\n",
            "Train Epoch: 21 [640/1400 (46%)]\tLoss: 150.298401\n",
            "Train Epoch: 21 [680/1400 (49%)]\tLoss: 157.359604\n",
            "Train Epoch: 21 [720/1400 (51%)]\tLoss: 164.683807\n",
            "Train Epoch: 21 [760/1400 (54%)]\tLoss: 148.715927\n",
            "Train Epoch: 21 [800/1400 (57%)]\tLoss: 125.233070\n",
            "Train Epoch: 21 [840/1400 (60%)]\tLoss: 152.616562\n",
            "Train Epoch: 21 [880/1400 (63%)]\tLoss: 127.000648\n",
            "Train Epoch: 21 [920/1400 (66%)]\tLoss: 145.708755\n",
            "Train Epoch: 21 [960/1400 (69%)]\tLoss: 137.010269\n",
            "Train Epoch: 21 [1000/1400 (71%)]\tLoss: 156.880417\n",
            "Train Epoch: 21 [1040/1400 (74%)]\tLoss: 140.048660\n",
            "Train Epoch: 21 [1080/1400 (77%)]\tLoss: 136.224884\n",
            "Train Epoch: 21 [1120/1400 (80%)]\tLoss: 139.978424\n",
            "Train Epoch: 21 [1160/1400 (83%)]\tLoss: 135.334900\n",
            "Train Epoch: 21 [1200/1400 (86%)]\tLoss: 152.031570\n",
            "Train Epoch: 21 [1240/1400 (89%)]\tLoss: 110.008896\n",
            "Train Epoch: 21 [1280/1400 (91%)]\tLoss: 155.406967\n",
            "Train Epoch: 21 [1320/1400 (94%)]\tLoss: 123.062187\n",
            "Train Epoch: 21 [1360/1400 (97%)]\tLoss: 135.758835\n",
            "====> Epoch: 21 Average loss: 144.2222\n",
            "====> Test set loss: 147.3792\n",
            "Train Epoch: 22 [0/1400 (0%)]\tLoss: 157.492004\n",
            "Train Epoch: 22 [40/1400 (3%)]\tLoss: 132.115829\n",
            "Train Epoch: 22 [80/1400 (6%)]\tLoss: 151.943436\n",
            "Train Epoch: 22 [120/1400 (9%)]\tLoss: 143.094345\n",
            "Train Epoch: 22 [160/1400 (11%)]\tLoss: 127.040054\n",
            "Train Epoch: 22 [200/1400 (14%)]\tLoss: 139.583344\n",
            "Train Epoch: 22 [240/1400 (17%)]\tLoss: 148.324692\n",
            "Train Epoch: 22 [280/1400 (20%)]\tLoss: 131.620224\n",
            "Train Epoch: 22 [320/1400 (23%)]\tLoss: 145.896255\n",
            "Train Epoch: 22 [360/1400 (26%)]\tLoss: 134.073532\n",
            "Train Epoch: 22 [400/1400 (29%)]\tLoss: 139.055466\n",
            "Train Epoch: 22 [440/1400 (31%)]\tLoss: 153.813461\n",
            "Train Epoch: 22 [480/1400 (34%)]\tLoss: 155.162582\n",
            "Train Epoch: 22 [520/1400 (37%)]\tLoss: 151.645157\n",
            "Train Epoch: 22 [560/1400 (40%)]\tLoss: 130.487030\n",
            "Train Epoch: 22 [600/1400 (43%)]\tLoss: 144.822205\n",
            "Train Epoch: 22 [640/1400 (46%)]\tLoss: 137.377319\n",
            "Train Epoch: 22 [680/1400 (49%)]\tLoss: 133.859375\n",
            "Train Epoch: 22 [720/1400 (51%)]\tLoss: 146.945740\n",
            "Train Epoch: 22 [760/1400 (54%)]\tLoss: 149.972626\n",
            "Train Epoch: 22 [800/1400 (57%)]\tLoss: 147.299423\n",
            "Train Epoch: 22 [840/1400 (60%)]\tLoss: 141.255280\n",
            "Train Epoch: 22 [880/1400 (63%)]\tLoss: 139.445557\n",
            "Train Epoch: 22 [920/1400 (66%)]\tLoss: 136.694580\n",
            "Train Epoch: 22 [960/1400 (69%)]\tLoss: 132.499710\n",
            "Train Epoch: 22 [1000/1400 (71%)]\tLoss: 149.540436\n",
            "Train Epoch: 22 [1040/1400 (74%)]\tLoss: 171.917587\n",
            "Train Epoch: 22 [1080/1400 (77%)]\tLoss: 141.744186\n",
            "Train Epoch: 22 [1120/1400 (80%)]\tLoss: 159.634705\n",
            "Train Epoch: 22 [1160/1400 (83%)]\tLoss: 151.904297\n",
            "Train Epoch: 22 [1200/1400 (86%)]\tLoss: 161.090576\n",
            "Train Epoch: 22 [1240/1400 (89%)]\tLoss: 153.366440\n",
            "Train Epoch: 22 [1280/1400 (91%)]\tLoss: 166.180237\n",
            "Train Epoch: 22 [1320/1400 (94%)]\tLoss: 115.764046\n",
            "Train Epoch: 22 [1360/1400 (97%)]\tLoss: 162.299698\n",
            "====> Epoch: 22 Average loss: 143.7682\n",
            "====> Test set loss: 146.8313\n",
            "Train Epoch: 23 [0/1400 (0%)]\tLoss: 153.119385\n",
            "Train Epoch: 23 [40/1400 (3%)]\tLoss: 145.420547\n",
            "Train Epoch: 23 [80/1400 (6%)]\tLoss: 154.654526\n",
            "Train Epoch: 23 [120/1400 (9%)]\tLoss: 151.829498\n",
            "Train Epoch: 23 [160/1400 (11%)]\tLoss: 171.413925\n",
            "Train Epoch: 23 [200/1400 (14%)]\tLoss: 169.064880\n",
            "Train Epoch: 23 [240/1400 (17%)]\tLoss: 132.567017\n",
            "Train Epoch: 23 [280/1400 (20%)]\tLoss: 130.868820\n",
            "Train Epoch: 23 [320/1400 (23%)]\tLoss: 161.839157\n",
            "Train Epoch: 23 [360/1400 (26%)]\tLoss: 141.318710\n",
            "Train Epoch: 23 [400/1400 (29%)]\tLoss: 151.972305\n",
            "Train Epoch: 23 [440/1400 (31%)]\tLoss: 165.747131\n",
            "Train Epoch: 23 [480/1400 (34%)]\tLoss: 141.058792\n",
            "Train Epoch: 23 [520/1400 (37%)]\tLoss: 136.150070\n",
            "Train Epoch: 23 [560/1400 (40%)]\tLoss: 139.093338\n",
            "Train Epoch: 23 [600/1400 (43%)]\tLoss: 137.472473\n",
            "Train Epoch: 23 [640/1400 (46%)]\tLoss: 167.285812\n",
            "Train Epoch: 23 [680/1400 (49%)]\tLoss: 137.108551\n",
            "Train Epoch: 23 [720/1400 (51%)]\tLoss: 135.371460\n",
            "Train Epoch: 23 [760/1400 (54%)]\tLoss: 137.324677\n",
            "Train Epoch: 23 [800/1400 (57%)]\tLoss: 152.768356\n",
            "Train Epoch: 23 [840/1400 (60%)]\tLoss: 147.161331\n",
            "Train Epoch: 23 [880/1400 (63%)]\tLoss: 137.803146\n",
            "Train Epoch: 23 [920/1400 (66%)]\tLoss: 144.196014\n",
            "Train Epoch: 23 [960/1400 (69%)]\tLoss: 155.216187\n",
            "Train Epoch: 23 [1000/1400 (71%)]\tLoss: 151.579498\n",
            "Train Epoch: 23 [1040/1400 (74%)]\tLoss: 147.774643\n",
            "Train Epoch: 23 [1080/1400 (77%)]\tLoss: 130.652542\n",
            "Train Epoch: 23 [1120/1400 (80%)]\tLoss: 137.343430\n",
            "Train Epoch: 23 [1160/1400 (83%)]\tLoss: 134.321106\n",
            "Train Epoch: 23 [1200/1400 (86%)]\tLoss: 141.791000\n",
            "Train Epoch: 23 [1240/1400 (89%)]\tLoss: 150.186874\n",
            "Train Epoch: 23 [1280/1400 (91%)]\tLoss: 151.108490\n",
            "Train Epoch: 23 [1320/1400 (94%)]\tLoss: 140.840500\n",
            "Train Epoch: 23 [1360/1400 (97%)]\tLoss: 119.536018\n",
            "====> Epoch: 23 Average loss: 143.5300\n",
            "====> Test set loss: 146.3034\n",
            "Train Epoch: 24 [0/1400 (0%)]\tLoss: 142.221100\n",
            "Train Epoch: 24 [40/1400 (3%)]\tLoss: 135.360138\n",
            "Train Epoch: 24 [80/1400 (6%)]\tLoss: 134.721268\n",
            "Train Epoch: 24 [120/1400 (9%)]\tLoss: 148.942520\n",
            "Train Epoch: 24 [160/1400 (11%)]\tLoss: 134.052200\n",
            "Train Epoch: 24 [200/1400 (14%)]\tLoss: 154.822540\n",
            "Train Epoch: 24 [240/1400 (17%)]\tLoss: 135.536331\n",
            "Train Epoch: 24 [280/1400 (20%)]\tLoss: 148.942673\n",
            "Train Epoch: 24 [320/1400 (23%)]\tLoss: 143.761017\n",
            "Train Epoch: 24 [360/1400 (26%)]\tLoss: 142.322830\n",
            "Train Epoch: 24 [400/1400 (29%)]\tLoss: 164.200500\n",
            "Train Epoch: 24 [440/1400 (31%)]\tLoss: 164.683609\n",
            "Train Epoch: 24 [480/1400 (34%)]\tLoss: 162.901260\n",
            "Train Epoch: 24 [520/1400 (37%)]\tLoss: 119.798286\n",
            "Train Epoch: 24 [560/1400 (40%)]\tLoss: 134.753189\n",
            "Train Epoch: 24 [600/1400 (43%)]\tLoss: 150.886444\n",
            "Train Epoch: 24 [640/1400 (46%)]\tLoss: 156.511169\n",
            "Train Epoch: 24 [680/1400 (49%)]\tLoss: 149.739105\n",
            "Train Epoch: 24 [720/1400 (51%)]\tLoss: 147.862366\n",
            "Train Epoch: 24 [760/1400 (54%)]\tLoss: 126.883148\n",
            "Train Epoch: 24 [800/1400 (57%)]\tLoss: 143.191208\n",
            "Train Epoch: 24 [840/1400 (60%)]\tLoss: 135.437180\n",
            "Train Epoch: 24 [880/1400 (63%)]\tLoss: 159.382935\n",
            "Train Epoch: 24 [920/1400 (66%)]\tLoss: 116.259720\n",
            "Train Epoch: 24 [960/1400 (69%)]\tLoss: 131.819504\n",
            "Train Epoch: 24 [1000/1400 (71%)]\tLoss: 140.366333\n",
            "Train Epoch: 24 [1040/1400 (74%)]\tLoss: 157.353287\n",
            "Train Epoch: 24 [1080/1400 (77%)]\tLoss: 162.568314\n",
            "Train Epoch: 24 [1120/1400 (80%)]\tLoss: 152.260422\n",
            "Train Epoch: 24 [1160/1400 (83%)]\tLoss: 147.770813\n",
            "Train Epoch: 24 [1200/1400 (86%)]\tLoss: 125.838577\n",
            "Train Epoch: 24 [1240/1400 (89%)]\tLoss: 132.450821\n",
            "Train Epoch: 24 [1280/1400 (91%)]\tLoss: 126.723053\n",
            "Train Epoch: 24 [1320/1400 (94%)]\tLoss: 133.486206\n",
            "Train Epoch: 24 [1360/1400 (97%)]\tLoss: 145.051254\n",
            "====> Epoch: 24 Average loss: 143.3179\n",
            "====> Test set loss: 146.3565\n",
            "Train Epoch: 25 [0/1400 (0%)]\tLoss: 127.810852\n",
            "Train Epoch: 25 [40/1400 (3%)]\tLoss: 126.437630\n",
            "Train Epoch: 25 [80/1400 (6%)]\tLoss: 148.134064\n",
            "Train Epoch: 25 [120/1400 (9%)]\tLoss: 154.506042\n",
            "Train Epoch: 25 [160/1400 (11%)]\tLoss: 156.707794\n",
            "Train Epoch: 25 [200/1400 (14%)]\tLoss: 165.656494\n",
            "Train Epoch: 25 [240/1400 (17%)]\tLoss: 146.761597\n",
            "Train Epoch: 25 [280/1400 (20%)]\tLoss: 144.816544\n",
            "Train Epoch: 25 [320/1400 (23%)]\tLoss: 138.366898\n",
            "Train Epoch: 25 [360/1400 (26%)]\tLoss: 147.694351\n",
            "Train Epoch: 25 [400/1400 (29%)]\tLoss: 139.551529\n",
            "Train Epoch: 25 [440/1400 (31%)]\tLoss: 152.444122\n",
            "Train Epoch: 25 [480/1400 (34%)]\tLoss: 145.087433\n",
            "Train Epoch: 25 [520/1400 (37%)]\tLoss: 145.925308\n",
            "Train Epoch: 25 [560/1400 (40%)]\tLoss: 156.014038\n",
            "Train Epoch: 25 [600/1400 (43%)]\tLoss: 170.330780\n",
            "Train Epoch: 25 [640/1400 (46%)]\tLoss: 132.136032\n",
            "Train Epoch: 25 [680/1400 (49%)]\tLoss: 150.070312\n",
            "Train Epoch: 25 [720/1400 (51%)]\tLoss: 156.247589\n",
            "Train Epoch: 25 [760/1400 (54%)]\tLoss: 141.574905\n",
            "Train Epoch: 25 [800/1400 (57%)]\tLoss: 139.050385\n",
            "Train Epoch: 25 [840/1400 (60%)]\tLoss: 150.701508\n",
            "Train Epoch: 25 [880/1400 (63%)]\tLoss: 154.866043\n",
            "Train Epoch: 25 [920/1400 (66%)]\tLoss: 149.057175\n",
            "Train Epoch: 25 [960/1400 (69%)]\tLoss: 155.600922\n",
            "Train Epoch: 25 [1000/1400 (71%)]\tLoss: 165.850250\n",
            "Train Epoch: 25 [1040/1400 (74%)]\tLoss: 140.739471\n",
            "Train Epoch: 25 [1080/1400 (77%)]\tLoss: 152.471817\n",
            "Train Epoch: 25 [1120/1400 (80%)]\tLoss: 159.587234\n",
            "Train Epoch: 25 [1160/1400 (83%)]\tLoss: 141.690872\n",
            "Train Epoch: 25 [1200/1400 (86%)]\tLoss: 145.639130\n",
            "Train Epoch: 25 [1240/1400 (89%)]\tLoss: 127.249908\n",
            "Train Epoch: 25 [1280/1400 (91%)]\tLoss: 151.413574\n",
            "Train Epoch: 25 [1320/1400 (94%)]\tLoss: 138.472092\n",
            "Train Epoch: 25 [1360/1400 (97%)]\tLoss: 159.417267\n",
            "====> Epoch: 25 Average loss: 143.1326\n",
            "====> Test set loss: 145.9230\n",
            "Train Epoch: 26 [0/1400 (0%)]\tLoss: 140.070267\n",
            "Train Epoch: 26 [40/1400 (3%)]\tLoss: 136.641083\n",
            "Train Epoch: 26 [80/1400 (6%)]\tLoss: 141.454056\n",
            "Train Epoch: 26 [120/1400 (9%)]\tLoss: 131.751999\n",
            "Train Epoch: 26 [160/1400 (11%)]\tLoss: 161.417038\n",
            "Train Epoch: 26 [200/1400 (14%)]\tLoss: 149.632782\n",
            "Train Epoch: 26 [240/1400 (17%)]\tLoss: 157.925751\n",
            "Train Epoch: 26 [280/1400 (20%)]\tLoss: 150.726898\n",
            "Train Epoch: 26 [320/1400 (23%)]\tLoss: 152.767517\n",
            "Train Epoch: 26 [360/1400 (26%)]\tLoss: 152.414215\n",
            "Train Epoch: 26 [400/1400 (29%)]\tLoss: 161.880997\n",
            "Train Epoch: 26 [440/1400 (31%)]\tLoss: 143.675934\n",
            "Train Epoch: 26 [480/1400 (34%)]\tLoss: 140.438095\n",
            "Train Epoch: 26 [520/1400 (37%)]\tLoss: 123.611877\n",
            "Train Epoch: 26 [560/1400 (40%)]\tLoss: 155.557098\n",
            "Train Epoch: 26 [600/1400 (43%)]\tLoss: 149.322540\n",
            "Train Epoch: 26 [640/1400 (46%)]\tLoss: 135.764801\n",
            "Train Epoch: 26 [680/1400 (49%)]\tLoss: 130.764557\n",
            "Train Epoch: 26 [720/1400 (51%)]\tLoss: 130.686768\n",
            "Train Epoch: 26 [760/1400 (54%)]\tLoss: 135.789017\n",
            "Train Epoch: 26 [800/1400 (57%)]\tLoss: 134.670044\n",
            "Train Epoch: 26 [840/1400 (60%)]\tLoss: 145.704895\n",
            "Train Epoch: 26 [880/1400 (63%)]\tLoss: 125.055336\n",
            "Train Epoch: 26 [920/1400 (66%)]\tLoss: 151.670944\n",
            "Train Epoch: 26 [960/1400 (69%)]\tLoss: 137.505600\n",
            "Train Epoch: 26 [1000/1400 (71%)]\tLoss: 121.446404\n",
            "Train Epoch: 26 [1040/1400 (74%)]\tLoss: 144.433029\n",
            "Train Epoch: 26 [1080/1400 (77%)]\tLoss: 144.728165\n",
            "Train Epoch: 26 [1120/1400 (80%)]\tLoss: 131.245239\n",
            "Train Epoch: 26 [1160/1400 (83%)]\tLoss: 140.645920\n",
            "Train Epoch: 26 [1200/1400 (86%)]\tLoss: 140.576935\n",
            "Train Epoch: 26 [1240/1400 (89%)]\tLoss: 152.663879\n",
            "Train Epoch: 26 [1280/1400 (91%)]\tLoss: 160.587967\n",
            "Train Epoch: 26 [1320/1400 (94%)]\tLoss: 147.028641\n",
            "Train Epoch: 26 [1360/1400 (97%)]\tLoss: 131.468369\n",
            "====> Epoch: 26 Average loss: 142.9640\n",
            "====> Test set loss: 145.9567\n",
            "Train Epoch: 27 [0/1400 (0%)]\tLoss: 135.015823\n",
            "Train Epoch: 27 [40/1400 (3%)]\tLoss: 141.157913\n",
            "Train Epoch: 27 [80/1400 (6%)]\tLoss: 119.465347\n",
            "Train Epoch: 27 [120/1400 (9%)]\tLoss: 149.069946\n",
            "Train Epoch: 27 [160/1400 (11%)]\tLoss: 147.350052\n",
            "Train Epoch: 27 [200/1400 (14%)]\tLoss: 132.256241\n",
            "Train Epoch: 27 [240/1400 (17%)]\tLoss: 143.909271\n",
            "Train Epoch: 27 [280/1400 (20%)]\tLoss: 133.446045\n",
            "Train Epoch: 27 [320/1400 (23%)]\tLoss: 148.968887\n",
            "Train Epoch: 27 [360/1400 (26%)]\tLoss: 153.690399\n",
            "Train Epoch: 27 [400/1400 (29%)]\tLoss: 135.254547\n",
            "Train Epoch: 27 [440/1400 (31%)]\tLoss: 143.515686\n",
            "Train Epoch: 27 [480/1400 (34%)]\tLoss: 142.475067\n",
            "Train Epoch: 27 [520/1400 (37%)]\tLoss: 141.812424\n",
            "Train Epoch: 27 [560/1400 (40%)]\tLoss: 127.319885\n",
            "Train Epoch: 27 [600/1400 (43%)]\tLoss: 120.934357\n",
            "Train Epoch: 27 [640/1400 (46%)]\tLoss: 127.732162\n",
            "Train Epoch: 27 [680/1400 (49%)]\tLoss: 142.864197\n",
            "Train Epoch: 27 [720/1400 (51%)]\tLoss: 156.266586\n",
            "Train Epoch: 27 [760/1400 (54%)]\tLoss: 120.352409\n",
            "Train Epoch: 27 [800/1400 (57%)]\tLoss: 142.929031\n",
            "Train Epoch: 27 [840/1400 (60%)]\tLoss: 151.355209\n",
            "Train Epoch: 27 [880/1400 (63%)]\tLoss: 148.924530\n",
            "Train Epoch: 27 [920/1400 (66%)]\tLoss: 139.491287\n",
            "Train Epoch: 27 [960/1400 (69%)]\tLoss: 133.862228\n",
            "Train Epoch: 27 [1000/1400 (71%)]\tLoss: 143.479248\n",
            "Train Epoch: 27 [1040/1400 (74%)]\tLoss: 155.604492\n",
            "Train Epoch: 27 [1080/1400 (77%)]\tLoss: 126.135559\n",
            "Train Epoch: 27 [1120/1400 (80%)]\tLoss: 157.474152\n",
            "Train Epoch: 27 [1160/1400 (83%)]\tLoss: 129.560059\n",
            "Train Epoch: 27 [1200/1400 (86%)]\tLoss: 135.159286\n",
            "Train Epoch: 27 [1240/1400 (89%)]\tLoss: 159.673828\n",
            "Train Epoch: 27 [1280/1400 (91%)]\tLoss: 121.177078\n",
            "Train Epoch: 27 [1320/1400 (94%)]\tLoss: 149.440521\n",
            "Train Epoch: 27 [1360/1400 (97%)]\tLoss: 157.363403\n",
            "====> Epoch: 27 Average loss: 142.7336\n",
            "====> Test set loss: 145.9940\n",
            "Train Epoch: 28 [0/1400 (0%)]\tLoss: 134.414764\n",
            "Train Epoch: 28 [40/1400 (3%)]\tLoss: 140.782761\n",
            "Train Epoch: 28 [80/1400 (6%)]\tLoss: 150.119675\n",
            "Train Epoch: 28 [120/1400 (9%)]\tLoss: 128.019592\n",
            "Train Epoch: 28 [160/1400 (11%)]\tLoss: 156.254761\n",
            "Train Epoch: 28 [200/1400 (14%)]\tLoss: 129.539062\n",
            "Train Epoch: 28 [240/1400 (17%)]\tLoss: 130.014252\n",
            "Train Epoch: 28 [280/1400 (20%)]\tLoss: 146.757339\n",
            "Train Epoch: 28 [320/1400 (23%)]\tLoss: 147.090012\n",
            "Train Epoch: 28 [360/1400 (26%)]\tLoss: 133.531769\n",
            "Train Epoch: 28 [400/1400 (29%)]\tLoss: 150.776901\n",
            "Train Epoch: 28 [440/1400 (31%)]\tLoss: 113.314026\n",
            "Train Epoch: 28 [480/1400 (34%)]\tLoss: 143.242569\n",
            "Train Epoch: 28 [520/1400 (37%)]\tLoss: 110.096069\n",
            "Train Epoch: 28 [560/1400 (40%)]\tLoss: 136.073730\n",
            "Train Epoch: 28 [600/1400 (43%)]\tLoss: 139.202759\n",
            "Train Epoch: 28 [640/1400 (46%)]\tLoss: 146.910614\n",
            "Train Epoch: 28 [680/1400 (49%)]\tLoss: 140.486282\n",
            "Train Epoch: 28 [720/1400 (51%)]\tLoss: 138.149063\n",
            "Train Epoch: 28 [760/1400 (54%)]\tLoss: 147.135849\n",
            "Train Epoch: 28 [800/1400 (57%)]\tLoss: 154.036560\n",
            "Train Epoch: 28 [840/1400 (60%)]\tLoss: 149.822861\n",
            "Train Epoch: 28 [880/1400 (63%)]\tLoss: 145.911224\n",
            "Train Epoch: 28 [920/1400 (66%)]\tLoss: 157.347763\n",
            "Train Epoch: 28 [960/1400 (69%)]\tLoss: 129.793900\n",
            "Train Epoch: 28 [1000/1400 (71%)]\tLoss: 152.753845\n",
            "Train Epoch: 28 [1040/1400 (74%)]\tLoss: 146.939133\n",
            "Train Epoch: 28 [1080/1400 (77%)]\tLoss: 138.746552\n",
            "Train Epoch: 28 [1120/1400 (80%)]\tLoss: 138.430023\n",
            "Train Epoch: 28 [1160/1400 (83%)]\tLoss: 150.821991\n",
            "Train Epoch: 28 [1200/1400 (86%)]\tLoss: 143.906418\n",
            "Train Epoch: 28 [1240/1400 (89%)]\tLoss: 153.032990\n",
            "Train Epoch: 28 [1280/1400 (91%)]\tLoss: 148.447739\n",
            "Train Epoch: 28 [1320/1400 (94%)]\tLoss: 144.440857\n",
            "Train Epoch: 28 [1360/1400 (97%)]\tLoss: 135.633209\n",
            "====> Epoch: 28 Average loss: 142.8197\n",
            "====> Test set loss: 145.0732\n",
            "Train Epoch: 29 [0/1400 (0%)]\tLoss: 147.693787\n",
            "Train Epoch: 29 [40/1400 (3%)]\tLoss: 155.782822\n",
            "Train Epoch: 29 [80/1400 (6%)]\tLoss: 139.668915\n",
            "Train Epoch: 29 [120/1400 (9%)]\tLoss: 130.786652\n",
            "Train Epoch: 29 [160/1400 (11%)]\tLoss: 118.853096\n",
            "Train Epoch: 29 [200/1400 (14%)]\tLoss: 135.586899\n",
            "Train Epoch: 29 [240/1400 (17%)]\tLoss: 155.732849\n",
            "Train Epoch: 29 [280/1400 (20%)]\tLoss: 153.468903\n",
            "Train Epoch: 29 [320/1400 (23%)]\tLoss: 156.031647\n",
            "Train Epoch: 29 [360/1400 (26%)]\tLoss: 146.680939\n",
            "Train Epoch: 29 [400/1400 (29%)]\tLoss: 152.110870\n",
            "Train Epoch: 29 [440/1400 (31%)]\tLoss: 165.439240\n",
            "Train Epoch: 29 [480/1400 (34%)]\tLoss: 151.174210\n",
            "Train Epoch: 29 [520/1400 (37%)]\tLoss: 145.620468\n",
            "Train Epoch: 29 [560/1400 (40%)]\tLoss: 132.861450\n",
            "Train Epoch: 29 [600/1400 (43%)]\tLoss: 143.425705\n",
            "Train Epoch: 29 [640/1400 (46%)]\tLoss: 147.804901\n",
            "Train Epoch: 29 [680/1400 (49%)]\tLoss: 157.123093\n",
            "Train Epoch: 29 [720/1400 (51%)]\tLoss: 147.352478\n",
            "Train Epoch: 29 [760/1400 (54%)]\tLoss: 161.297394\n",
            "Train Epoch: 29 [800/1400 (57%)]\tLoss: 140.772568\n",
            "Train Epoch: 29 [840/1400 (60%)]\tLoss: 155.280273\n",
            "Train Epoch: 29 [880/1400 (63%)]\tLoss: 141.788040\n",
            "Train Epoch: 29 [920/1400 (66%)]\tLoss: 123.379349\n",
            "Train Epoch: 29 [960/1400 (69%)]\tLoss: 128.122818\n",
            "Train Epoch: 29 [1000/1400 (71%)]\tLoss: 154.384445\n",
            "Train Epoch: 29 [1040/1400 (74%)]\tLoss: 121.407562\n",
            "Train Epoch: 29 [1080/1400 (77%)]\tLoss: 142.877487\n",
            "Train Epoch: 29 [1120/1400 (80%)]\tLoss: 154.123672\n",
            "Train Epoch: 29 [1160/1400 (83%)]\tLoss: 140.767029\n",
            "Train Epoch: 29 [1200/1400 (86%)]\tLoss: 143.302658\n",
            "Train Epoch: 29 [1240/1400 (89%)]\tLoss: 133.818527\n",
            "Train Epoch: 29 [1280/1400 (91%)]\tLoss: 142.149368\n",
            "Train Epoch: 29 [1320/1400 (94%)]\tLoss: 137.288437\n",
            "Train Epoch: 29 [1360/1400 (97%)]\tLoss: 146.982239\n",
            "====> Epoch: 29 Average loss: 142.4923\n",
            "====> Test set loss: 145.3258\n",
            "Train Epoch: 30 [0/1400 (0%)]\tLoss: 127.089149\n",
            "Train Epoch: 30 [40/1400 (3%)]\tLoss: 152.631546\n",
            "Train Epoch: 30 [80/1400 (6%)]\tLoss: 137.863129\n",
            "Train Epoch: 30 [120/1400 (9%)]\tLoss: 143.448059\n",
            "Train Epoch: 30 [160/1400 (11%)]\tLoss: 155.183212\n",
            "Train Epoch: 30 [200/1400 (14%)]\tLoss: 142.419418\n",
            "Train Epoch: 30 [240/1400 (17%)]\tLoss: 149.423584\n",
            "Train Epoch: 30 [280/1400 (20%)]\tLoss: 142.510864\n",
            "Train Epoch: 30 [320/1400 (23%)]\tLoss: 157.309937\n",
            "Train Epoch: 30 [360/1400 (26%)]\tLoss: 142.729767\n",
            "Train Epoch: 30 [400/1400 (29%)]\tLoss: 160.409973\n",
            "Train Epoch: 30 [440/1400 (31%)]\tLoss: 128.493500\n",
            "Train Epoch: 30 [480/1400 (34%)]\tLoss: 137.147552\n",
            "Train Epoch: 30 [520/1400 (37%)]\tLoss: 143.820892\n",
            "Train Epoch: 30 [560/1400 (40%)]\tLoss: 149.903290\n",
            "Train Epoch: 30 [600/1400 (43%)]\tLoss: 133.108459\n",
            "Train Epoch: 30 [640/1400 (46%)]\tLoss: 148.967361\n",
            "Train Epoch: 30 [680/1400 (49%)]\tLoss: 155.719162\n",
            "Train Epoch: 30 [720/1400 (51%)]\tLoss: 121.858536\n",
            "Train Epoch: 30 [760/1400 (54%)]\tLoss: 142.635620\n",
            "Train Epoch: 30 [800/1400 (57%)]\tLoss: 163.577682\n",
            "Train Epoch: 30 [840/1400 (60%)]\tLoss: 134.210831\n",
            "Train Epoch: 30 [880/1400 (63%)]\tLoss: 130.722763\n",
            "Train Epoch: 30 [920/1400 (66%)]\tLoss: 144.771164\n",
            "Train Epoch: 30 [960/1400 (69%)]\tLoss: 138.750748\n",
            "Train Epoch: 30 [1000/1400 (71%)]\tLoss: 143.366806\n",
            "Train Epoch: 30 [1040/1400 (74%)]\tLoss: 135.241455\n",
            "Train Epoch: 30 [1080/1400 (77%)]\tLoss: 146.046753\n",
            "Train Epoch: 30 [1120/1400 (80%)]\tLoss: 142.499222\n",
            "Train Epoch: 30 [1160/1400 (83%)]\tLoss: 137.305405\n",
            "Train Epoch: 30 [1200/1400 (86%)]\tLoss: 143.652573\n",
            "Train Epoch: 30 [1240/1400 (89%)]\tLoss: 136.205276\n",
            "Train Epoch: 30 [1280/1400 (91%)]\tLoss: 126.490692\n",
            "Train Epoch: 30 [1320/1400 (94%)]\tLoss: 153.047745\n",
            "Train Epoch: 30 [1360/1400 (97%)]\tLoss: 130.271988\n",
            "====> Epoch: 30 Average loss: 142.1181\n",
            "====> Test set loss: 145.5933\n",
            "Train Epoch: 31 [0/1400 (0%)]\tLoss: 155.656647\n",
            "Train Epoch: 31 [40/1400 (3%)]\tLoss: 150.890961\n",
            "Train Epoch: 31 [80/1400 (6%)]\tLoss: 124.760620\n",
            "Train Epoch: 31 [120/1400 (9%)]\tLoss: 171.257538\n",
            "Train Epoch: 31 [160/1400 (11%)]\tLoss: 136.915344\n",
            "Train Epoch: 31 [200/1400 (14%)]\tLoss: 132.340408\n",
            "Train Epoch: 31 [240/1400 (17%)]\tLoss: 140.029480\n",
            "Train Epoch: 31 [280/1400 (20%)]\tLoss: 135.223953\n",
            "Train Epoch: 31 [320/1400 (23%)]\tLoss: 151.935638\n",
            "Train Epoch: 31 [360/1400 (26%)]\tLoss: 146.420425\n",
            "Train Epoch: 31 [400/1400 (29%)]\tLoss: 112.807610\n",
            "Train Epoch: 31 [440/1400 (31%)]\tLoss: 134.865860\n",
            "Train Epoch: 31 [480/1400 (34%)]\tLoss: 146.254959\n",
            "Train Epoch: 31 [520/1400 (37%)]\tLoss: 137.616318\n",
            "Train Epoch: 31 [560/1400 (40%)]\tLoss: 151.540710\n",
            "Train Epoch: 31 [600/1400 (43%)]\tLoss: 126.650948\n",
            "Train Epoch: 31 [640/1400 (46%)]\tLoss: 140.173325\n",
            "Train Epoch: 31 [680/1400 (49%)]\tLoss: 151.132095\n",
            "Train Epoch: 31 [720/1400 (51%)]\tLoss: 142.601395\n",
            "Train Epoch: 31 [760/1400 (54%)]\tLoss: 146.815216\n",
            "Train Epoch: 31 [800/1400 (57%)]\tLoss: 140.055878\n",
            "Train Epoch: 31 [840/1400 (60%)]\tLoss: 143.895844\n",
            "Train Epoch: 31 [880/1400 (63%)]\tLoss: 146.892273\n",
            "Train Epoch: 31 [920/1400 (66%)]\tLoss: 127.745636\n",
            "Train Epoch: 31 [960/1400 (69%)]\tLoss: 121.419258\n",
            "Train Epoch: 31 [1000/1400 (71%)]\tLoss: 163.478180\n",
            "Train Epoch: 31 [1040/1400 (74%)]\tLoss: 154.059464\n",
            "Train Epoch: 31 [1080/1400 (77%)]\tLoss: 147.246460\n",
            "Train Epoch: 31 [1120/1400 (80%)]\tLoss: 141.168396\n",
            "Train Epoch: 31 [1160/1400 (83%)]\tLoss: 156.470474\n",
            "Train Epoch: 31 [1200/1400 (86%)]\tLoss: 139.894257\n",
            "Train Epoch: 31 [1240/1400 (89%)]\tLoss: 144.970032\n",
            "Train Epoch: 31 [1280/1400 (91%)]\tLoss: 145.521194\n",
            "Train Epoch: 31 [1320/1400 (94%)]\tLoss: 130.418457\n",
            "Train Epoch: 31 [1360/1400 (97%)]\tLoss: 147.865982\n",
            "====> Epoch: 31 Average loss: 142.0630\n",
            "====> Test set loss: 146.2606\n",
            "Train Epoch: 32 [0/1400 (0%)]\tLoss: 149.403870\n",
            "Train Epoch: 32 [40/1400 (3%)]\tLoss: 146.606079\n",
            "Train Epoch: 32 [80/1400 (6%)]\tLoss: 149.395813\n",
            "Train Epoch: 32 [120/1400 (9%)]\tLoss: 154.988632\n",
            "Train Epoch: 32 [160/1400 (11%)]\tLoss: 144.886200\n",
            "Train Epoch: 32 [200/1400 (14%)]\tLoss: 141.704437\n",
            "Train Epoch: 32 [240/1400 (17%)]\tLoss: 151.057312\n",
            "Train Epoch: 32 [280/1400 (20%)]\tLoss: 148.957184\n",
            "Train Epoch: 32 [320/1400 (23%)]\tLoss: 155.103149\n",
            "Train Epoch: 32 [360/1400 (26%)]\tLoss: 131.746246\n",
            "Train Epoch: 32 [400/1400 (29%)]\tLoss: 148.690308\n",
            "Train Epoch: 32 [440/1400 (31%)]\tLoss: 132.052689\n",
            "Train Epoch: 32 [480/1400 (34%)]\tLoss: 135.722397\n",
            "Train Epoch: 32 [520/1400 (37%)]\tLoss: 132.660583\n",
            "Train Epoch: 32 [560/1400 (40%)]\tLoss: 145.424042\n",
            "Train Epoch: 32 [600/1400 (43%)]\tLoss: 145.193848\n",
            "Train Epoch: 32 [640/1400 (46%)]\tLoss: 148.478851\n",
            "Train Epoch: 32 [680/1400 (49%)]\tLoss: 129.045609\n",
            "Train Epoch: 32 [720/1400 (51%)]\tLoss: 143.819305\n",
            "Train Epoch: 32 [760/1400 (54%)]\tLoss: 146.519806\n",
            "Train Epoch: 32 [800/1400 (57%)]\tLoss: 138.634644\n",
            "Train Epoch: 32 [840/1400 (60%)]\tLoss: 130.179611\n",
            "Train Epoch: 32 [880/1400 (63%)]\tLoss: 137.475128\n",
            "Train Epoch: 32 [920/1400 (66%)]\tLoss: 152.184204\n",
            "Train Epoch: 32 [960/1400 (69%)]\tLoss: 128.873291\n",
            "Train Epoch: 32 [1000/1400 (71%)]\tLoss: 134.247559\n",
            "Train Epoch: 32 [1040/1400 (74%)]\tLoss: 138.704865\n",
            "Train Epoch: 32 [1080/1400 (77%)]\tLoss: 149.599167\n",
            "Train Epoch: 32 [1120/1400 (80%)]\tLoss: 141.486771\n",
            "Train Epoch: 32 [1160/1400 (83%)]\tLoss: 157.260605\n",
            "Train Epoch: 32 [1200/1400 (86%)]\tLoss: 151.354370\n",
            "Train Epoch: 32 [1240/1400 (89%)]\tLoss: 169.356308\n",
            "Train Epoch: 32 [1280/1400 (91%)]\tLoss: 148.287231\n",
            "Train Epoch: 32 [1320/1400 (94%)]\tLoss: 141.022995\n",
            "Train Epoch: 32 [1360/1400 (97%)]\tLoss: 144.361511\n",
            "====> Epoch: 32 Average loss: 141.8583\n",
            "====> Test set loss: 144.5259\n",
            "Train Epoch: 33 [0/1400 (0%)]\tLoss: 144.018585\n",
            "Train Epoch: 33 [40/1400 (3%)]\tLoss: 129.728821\n",
            "Train Epoch: 33 [80/1400 (6%)]\tLoss: 120.434372\n",
            "Train Epoch: 33 [120/1400 (9%)]\tLoss: 127.637306\n",
            "Train Epoch: 33 [160/1400 (11%)]\tLoss: 140.625336\n",
            "Train Epoch: 33 [200/1400 (14%)]\tLoss: 144.680405\n",
            "Train Epoch: 33 [240/1400 (17%)]\tLoss: 152.365265\n",
            "Train Epoch: 33 [280/1400 (20%)]\tLoss: 139.826126\n",
            "Train Epoch: 33 [320/1400 (23%)]\tLoss: 155.158295\n",
            "Train Epoch: 33 [360/1400 (26%)]\tLoss: 137.391953\n",
            "Train Epoch: 33 [400/1400 (29%)]\tLoss: 169.172592\n",
            "Train Epoch: 33 [440/1400 (31%)]\tLoss: 140.287140\n",
            "Train Epoch: 33 [480/1400 (34%)]\tLoss: 141.156372\n",
            "Train Epoch: 33 [520/1400 (37%)]\tLoss: 148.760803\n",
            "Train Epoch: 33 [560/1400 (40%)]\tLoss: 160.748474\n",
            "Train Epoch: 33 [600/1400 (43%)]\tLoss: 168.613297\n",
            "Train Epoch: 33 [640/1400 (46%)]\tLoss: 132.744904\n",
            "Train Epoch: 33 [680/1400 (49%)]\tLoss: 149.626007\n",
            "Train Epoch: 33 [720/1400 (51%)]\tLoss: 132.867065\n",
            "Train Epoch: 33 [760/1400 (54%)]\tLoss: 143.151001\n",
            "Train Epoch: 33 [800/1400 (57%)]\tLoss: 147.435532\n",
            "Train Epoch: 33 [840/1400 (60%)]\tLoss: 156.747833\n",
            "Train Epoch: 33 [880/1400 (63%)]\tLoss: 143.917664\n",
            "Train Epoch: 33 [920/1400 (66%)]\tLoss: 144.328293\n",
            "Train Epoch: 33 [960/1400 (69%)]\tLoss: 130.968155\n",
            "Train Epoch: 33 [1000/1400 (71%)]\tLoss: 140.441818\n",
            "Train Epoch: 33 [1040/1400 (74%)]\tLoss: 138.207870\n",
            "Train Epoch: 33 [1080/1400 (77%)]\tLoss: 130.482773\n",
            "Train Epoch: 33 [1120/1400 (80%)]\tLoss: 148.283524\n",
            "Train Epoch: 33 [1160/1400 (83%)]\tLoss: 135.143387\n",
            "Train Epoch: 33 [1200/1400 (86%)]\tLoss: 132.972946\n",
            "Train Epoch: 33 [1240/1400 (89%)]\tLoss: 154.248062\n",
            "Train Epoch: 33 [1280/1400 (91%)]\tLoss: 143.951035\n",
            "Train Epoch: 33 [1320/1400 (94%)]\tLoss: 132.482056\n",
            "Train Epoch: 33 [1360/1400 (97%)]\tLoss: 155.024643\n",
            "====> Epoch: 33 Average loss: 141.8235\n",
            "====> Test set loss: 144.7908\n",
            "Train Epoch: 34 [0/1400 (0%)]\tLoss: 130.484970\n",
            "Train Epoch: 34 [40/1400 (3%)]\tLoss: 133.079712\n",
            "Train Epoch: 34 [80/1400 (6%)]\tLoss: 129.389069\n",
            "Train Epoch: 34 [120/1400 (9%)]\tLoss: 153.204163\n",
            "Train Epoch: 34 [160/1400 (11%)]\tLoss: 133.864502\n",
            "Train Epoch: 34 [200/1400 (14%)]\tLoss: 151.344284\n",
            "Train Epoch: 34 [240/1400 (17%)]\tLoss: 146.324951\n",
            "Train Epoch: 34 [280/1400 (20%)]\tLoss: 143.955307\n",
            "Train Epoch: 34 [320/1400 (23%)]\tLoss: 131.431152\n",
            "Train Epoch: 34 [360/1400 (26%)]\tLoss: 141.233551\n",
            "Train Epoch: 34 [400/1400 (29%)]\tLoss: 147.519119\n",
            "Train Epoch: 34 [440/1400 (31%)]\tLoss: 124.343750\n",
            "Train Epoch: 34 [480/1400 (34%)]\tLoss: 141.679520\n",
            "Train Epoch: 34 [520/1400 (37%)]\tLoss: 143.811081\n",
            "Train Epoch: 34 [560/1400 (40%)]\tLoss: 111.431290\n",
            "Train Epoch: 34 [600/1400 (43%)]\tLoss: 145.126953\n",
            "Train Epoch: 34 [640/1400 (46%)]\tLoss: 149.700043\n",
            "Train Epoch: 34 [680/1400 (49%)]\tLoss: 142.537430\n",
            "Train Epoch: 34 [720/1400 (51%)]\tLoss: 132.134750\n",
            "Train Epoch: 34 [760/1400 (54%)]\tLoss: 153.461014\n",
            "Train Epoch: 34 [800/1400 (57%)]\tLoss: 151.911362\n",
            "Train Epoch: 34 [840/1400 (60%)]\tLoss: 118.816711\n",
            "Train Epoch: 34 [880/1400 (63%)]\tLoss: 152.565308\n",
            "Train Epoch: 34 [920/1400 (66%)]\tLoss: 132.404449\n",
            "Train Epoch: 34 [960/1400 (69%)]\tLoss: 122.895996\n",
            "Train Epoch: 34 [1000/1400 (71%)]\tLoss: 140.029037\n",
            "Train Epoch: 34 [1040/1400 (74%)]\tLoss: 130.235474\n",
            "Train Epoch: 34 [1080/1400 (77%)]\tLoss: 140.961899\n",
            "Train Epoch: 34 [1120/1400 (80%)]\tLoss: 135.723297\n",
            "Train Epoch: 34 [1160/1400 (83%)]\tLoss: 132.545013\n",
            "Train Epoch: 34 [1200/1400 (86%)]\tLoss: 116.333015\n",
            "Train Epoch: 34 [1240/1400 (89%)]\tLoss: 143.258957\n",
            "Train Epoch: 34 [1280/1400 (91%)]\tLoss: 136.244049\n",
            "Train Epoch: 34 [1320/1400 (94%)]\tLoss: 129.552536\n",
            "Train Epoch: 34 [1360/1400 (97%)]\tLoss: 133.461868\n",
            "====> Epoch: 34 Average loss: 141.5457\n",
            "====> Test set loss: 144.4353\n",
            "Train Epoch: 35 [0/1400 (0%)]\tLoss: 145.220520\n",
            "Train Epoch: 35 [40/1400 (3%)]\tLoss: 138.556335\n",
            "Train Epoch: 35 [80/1400 (6%)]\tLoss: 144.719559\n",
            "Train Epoch: 35 [120/1400 (9%)]\tLoss: 151.149063\n",
            "Train Epoch: 35 [160/1400 (11%)]\tLoss: 123.661331\n",
            "Train Epoch: 35 [200/1400 (14%)]\tLoss: 181.329498\n",
            "Train Epoch: 35 [240/1400 (17%)]\tLoss: 137.685364\n",
            "Train Epoch: 35 [280/1400 (20%)]\tLoss: 143.091400\n",
            "Train Epoch: 35 [320/1400 (23%)]\tLoss: 150.783127\n",
            "Train Epoch: 35 [360/1400 (26%)]\tLoss: 144.367706\n",
            "Train Epoch: 35 [400/1400 (29%)]\tLoss: 139.561356\n",
            "Train Epoch: 35 [440/1400 (31%)]\tLoss: 138.464813\n",
            "Train Epoch: 35 [480/1400 (34%)]\tLoss: 137.459244\n",
            "Train Epoch: 35 [520/1400 (37%)]\tLoss: 156.789078\n",
            "Train Epoch: 35 [560/1400 (40%)]\tLoss: 154.681625\n",
            "Train Epoch: 35 [600/1400 (43%)]\tLoss: 151.310059\n",
            "Train Epoch: 35 [640/1400 (46%)]\tLoss: 139.671448\n",
            "Train Epoch: 35 [680/1400 (49%)]\tLoss: 131.025314\n",
            "Train Epoch: 35 [720/1400 (51%)]\tLoss: 134.815735\n",
            "Train Epoch: 35 [760/1400 (54%)]\tLoss: 151.553177\n",
            "Train Epoch: 35 [800/1400 (57%)]\tLoss: 139.470322\n",
            "Train Epoch: 35 [840/1400 (60%)]\tLoss: 136.487808\n",
            "Train Epoch: 35 [880/1400 (63%)]\tLoss: 147.236328\n",
            "Train Epoch: 35 [920/1400 (66%)]\tLoss: 139.091904\n",
            "Train Epoch: 35 [960/1400 (69%)]\tLoss: 149.269608\n",
            "Train Epoch: 35 [1000/1400 (71%)]\tLoss: 111.515549\n",
            "Train Epoch: 35 [1040/1400 (74%)]\tLoss: 126.920074\n",
            "Train Epoch: 35 [1080/1400 (77%)]\tLoss: 144.961044\n",
            "Train Epoch: 35 [1120/1400 (80%)]\tLoss: 153.143982\n",
            "Train Epoch: 35 [1160/1400 (83%)]\tLoss: 153.331879\n",
            "Train Epoch: 35 [1200/1400 (86%)]\tLoss: 138.657181\n",
            "Train Epoch: 35 [1240/1400 (89%)]\tLoss: 152.095505\n",
            "Train Epoch: 35 [1280/1400 (91%)]\tLoss: 133.241547\n",
            "Train Epoch: 35 [1320/1400 (94%)]\tLoss: 158.811569\n",
            "Train Epoch: 35 [1360/1400 (97%)]\tLoss: 144.560989\n",
            "====> Epoch: 35 Average loss: 141.7008\n",
            "====> Test set loss: 144.5484\n",
            "Train Epoch: 36 [0/1400 (0%)]\tLoss: 128.600113\n",
            "Train Epoch: 36 [40/1400 (3%)]\tLoss: 155.374603\n",
            "Train Epoch: 36 [80/1400 (6%)]\tLoss: 173.203308\n",
            "Train Epoch: 36 [120/1400 (9%)]\tLoss: 143.121155\n",
            "Train Epoch: 36 [160/1400 (11%)]\tLoss: 154.861679\n",
            "Train Epoch: 36 [200/1400 (14%)]\tLoss: 160.329834\n",
            "Train Epoch: 36 [240/1400 (17%)]\tLoss: 144.484573\n",
            "Train Epoch: 36 [280/1400 (20%)]\tLoss: 148.723831\n",
            "Train Epoch: 36 [320/1400 (23%)]\tLoss: 145.849472\n",
            "Train Epoch: 36 [360/1400 (26%)]\tLoss: 124.366959\n",
            "Train Epoch: 36 [400/1400 (29%)]\tLoss: 148.742981\n",
            "Train Epoch: 36 [440/1400 (31%)]\tLoss: 140.645889\n",
            "Train Epoch: 36 [480/1400 (34%)]\tLoss: 136.950607\n",
            "Train Epoch: 36 [520/1400 (37%)]\tLoss: 144.100571\n",
            "Train Epoch: 36 [560/1400 (40%)]\tLoss: 120.184944\n",
            "Train Epoch: 36 [600/1400 (43%)]\tLoss: 152.868713\n",
            "Train Epoch: 36 [640/1400 (46%)]\tLoss: 131.419464\n",
            "Train Epoch: 36 [680/1400 (49%)]\tLoss: 153.443130\n",
            "Train Epoch: 36 [720/1400 (51%)]\tLoss: 145.896896\n",
            "Train Epoch: 36 [760/1400 (54%)]\tLoss: 160.904373\n",
            "Train Epoch: 36 [800/1400 (57%)]\tLoss: 136.345505\n",
            "Train Epoch: 36 [840/1400 (60%)]\tLoss: 126.043152\n",
            "Train Epoch: 36 [880/1400 (63%)]\tLoss: 138.156830\n",
            "Train Epoch: 36 [920/1400 (66%)]\tLoss: 153.609650\n",
            "Train Epoch: 36 [960/1400 (69%)]\tLoss: 144.765182\n",
            "Train Epoch: 36 [1000/1400 (71%)]\tLoss: 142.711945\n",
            "Train Epoch: 36 [1040/1400 (74%)]\tLoss: 129.202576\n",
            "Train Epoch: 36 [1080/1400 (77%)]\tLoss: 137.784683\n",
            "Train Epoch: 36 [1120/1400 (80%)]\tLoss: 140.273804\n",
            "Train Epoch: 36 [1160/1400 (83%)]\tLoss: 132.831757\n",
            "Train Epoch: 36 [1200/1400 (86%)]\tLoss: 129.048431\n",
            "Train Epoch: 36 [1240/1400 (89%)]\tLoss: 139.958115\n",
            "Train Epoch: 36 [1280/1400 (91%)]\tLoss: 143.416351\n",
            "Train Epoch: 36 [1320/1400 (94%)]\tLoss: 164.759247\n",
            "Train Epoch: 36 [1360/1400 (97%)]\tLoss: 138.431076\n",
            "====> Epoch: 36 Average loss: 141.5043\n",
            "====> Test set loss: 144.5378\n",
            "Train Epoch: 37 [0/1400 (0%)]\tLoss: 150.004196\n",
            "Train Epoch: 37 [40/1400 (3%)]\tLoss: 140.165558\n",
            "Train Epoch: 37 [80/1400 (6%)]\tLoss: 155.575821\n",
            "Train Epoch: 37 [120/1400 (9%)]\tLoss: 157.761276\n",
            "Train Epoch: 37 [160/1400 (11%)]\tLoss: 138.213333\n",
            "Train Epoch: 37 [200/1400 (14%)]\tLoss: 133.686600\n",
            "Train Epoch: 37 [240/1400 (17%)]\tLoss: 139.467636\n",
            "Train Epoch: 37 [280/1400 (20%)]\tLoss: 130.297287\n",
            "Train Epoch: 37 [320/1400 (23%)]\tLoss: 124.580002\n",
            "Train Epoch: 37 [360/1400 (26%)]\tLoss: 159.575943\n",
            "Train Epoch: 37 [400/1400 (29%)]\tLoss: 122.565712\n",
            "Train Epoch: 37 [440/1400 (31%)]\tLoss: 139.147720\n",
            "Train Epoch: 37 [480/1400 (34%)]\tLoss: 146.236633\n",
            "Train Epoch: 37 [520/1400 (37%)]\tLoss: 134.830444\n",
            "Train Epoch: 37 [560/1400 (40%)]\tLoss: 153.660156\n",
            "Train Epoch: 37 [600/1400 (43%)]\tLoss: 152.833588\n",
            "Train Epoch: 37 [640/1400 (46%)]\tLoss: 149.417297\n",
            "Train Epoch: 37 [680/1400 (49%)]\tLoss: 140.891495\n",
            "Train Epoch: 37 [720/1400 (51%)]\tLoss: 146.445099\n",
            "Train Epoch: 37 [760/1400 (54%)]\tLoss: 155.592499\n",
            "Train Epoch: 37 [800/1400 (57%)]\tLoss: 148.323212\n",
            "Train Epoch: 37 [840/1400 (60%)]\tLoss: 153.951462\n",
            "Train Epoch: 37 [880/1400 (63%)]\tLoss: 126.490646\n",
            "Train Epoch: 37 [920/1400 (66%)]\tLoss: 126.729599\n",
            "Train Epoch: 37 [960/1400 (69%)]\tLoss: 145.202850\n",
            "Train Epoch: 37 [1000/1400 (71%)]\tLoss: 155.324356\n",
            "Train Epoch: 37 [1040/1400 (74%)]\tLoss: 148.084045\n",
            "Train Epoch: 37 [1080/1400 (77%)]\tLoss: 128.221863\n",
            "Train Epoch: 37 [1120/1400 (80%)]\tLoss: 135.879013\n",
            "Train Epoch: 37 [1160/1400 (83%)]\tLoss: 131.968445\n",
            "Train Epoch: 37 [1200/1400 (86%)]\tLoss: 166.209412\n",
            "Train Epoch: 37 [1240/1400 (89%)]\tLoss: 135.282730\n",
            "Train Epoch: 37 [1280/1400 (91%)]\tLoss: 145.497070\n",
            "Train Epoch: 37 [1320/1400 (94%)]\tLoss: 142.162048\n",
            "Train Epoch: 37 [1360/1400 (97%)]\tLoss: 152.130219\n",
            "====> Epoch: 37 Average loss: 141.2202\n",
            "====> Test set loss: 144.3263\n",
            "Train Epoch: 38 [0/1400 (0%)]\tLoss: 134.324783\n",
            "Train Epoch: 38 [40/1400 (3%)]\tLoss: 158.405838\n",
            "Train Epoch: 38 [80/1400 (6%)]\tLoss: 138.677399\n",
            "Train Epoch: 38 [120/1400 (9%)]\tLoss: 146.215271\n",
            "Train Epoch: 38 [160/1400 (11%)]\tLoss: 149.665527\n",
            "Train Epoch: 38 [200/1400 (14%)]\tLoss: 140.934418\n",
            "Train Epoch: 38 [240/1400 (17%)]\tLoss: 152.057648\n",
            "Train Epoch: 38 [280/1400 (20%)]\tLoss: 141.472565\n",
            "Train Epoch: 38 [320/1400 (23%)]\tLoss: 145.723740\n",
            "Train Epoch: 38 [360/1400 (26%)]\tLoss: 123.859558\n",
            "Train Epoch: 38 [400/1400 (29%)]\tLoss: 138.183731\n",
            "Train Epoch: 38 [440/1400 (31%)]\tLoss: 117.937820\n",
            "Train Epoch: 38 [480/1400 (34%)]\tLoss: 151.097000\n",
            "Train Epoch: 38 [520/1400 (37%)]\tLoss: 150.924362\n",
            "Train Epoch: 38 [560/1400 (40%)]\tLoss: 135.291367\n",
            "Train Epoch: 38 [600/1400 (43%)]\tLoss: 145.898636\n",
            "Train Epoch: 38 [640/1400 (46%)]\tLoss: 142.517319\n",
            "Train Epoch: 38 [680/1400 (49%)]\tLoss: 158.033127\n",
            "Train Epoch: 38 [720/1400 (51%)]\tLoss: 138.090805\n",
            "Train Epoch: 38 [760/1400 (54%)]\tLoss: 133.990921\n",
            "Train Epoch: 38 [800/1400 (57%)]\tLoss: 154.839172\n",
            "Train Epoch: 38 [840/1400 (60%)]\tLoss: 120.077957\n",
            "Train Epoch: 38 [880/1400 (63%)]\tLoss: 136.415359\n",
            "Train Epoch: 38 [920/1400 (66%)]\tLoss: 131.932907\n",
            "Train Epoch: 38 [960/1400 (69%)]\tLoss: 153.015274\n",
            "Train Epoch: 38 [1000/1400 (71%)]\tLoss: 159.731766\n",
            "Train Epoch: 38 [1040/1400 (74%)]\tLoss: 145.413605\n",
            "Train Epoch: 38 [1080/1400 (77%)]\tLoss: 138.348358\n",
            "Train Epoch: 38 [1120/1400 (80%)]\tLoss: 141.058395\n",
            "Train Epoch: 38 [1160/1400 (83%)]\tLoss: 132.677979\n",
            "Train Epoch: 38 [1200/1400 (86%)]\tLoss: 122.019928\n",
            "Train Epoch: 38 [1240/1400 (89%)]\tLoss: 146.212006\n",
            "Train Epoch: 38 [1280/1400 (91%)]\tLoss: 137.187073\n",
            "Train Epoch: 38 [1320/1400 (94%)]\tLoss: 129.990845\n",
            "Train Epoch: 38 [1360/1400 (97%)]\tLoss: 152.054291\n",
            "====> Epoch: 38 Average loss: 141.0954\n",
            "====> Test set loss: 144.0166\n",
            "Train Epoch: 39 [0/1400 (0%)]\tLoss: 122.747414\n",
            "Train Epoch: 39 [40/1400 (3%)]\tLoss: 138.924866\n",
            "Train Epoch: 39 [80/1400 (6%)]\tLoss: 138.937241\n",
            "Train Epoch: 39 [120/1400 (9%)]\tLoss: 143.261307\n",
            "Train Epoch: 39 [160/1400 (11%)]\tLoss: 148.007584\n",
            "Train Epoch: 39 [200/1400 (14%)]\tLoss: 152.871124\n",
            "Train Epoch: 39 [240/1400 (17%)]\tLoss: 136.698181\n",
            "Train Epoch: 39 [280/1400 (20%)]\tLoss: 143.860641\n",
            "Train Epoch: 39 [320/1400 (23%)]\tLoss: 137.473495\n",
            "Train Epoch: 39 [360/1400 (26%)]\tLoss: 170.221848\n",
            "Train Epoch: 39 [400/1400 (29%)]\tLoss: 143.294083\n",
            "Train Epoch: 39 [440/1400 (31%)]\tLoss: 130.319534\n",
            "Train Epoch: 39 [480/1400 (34%)]\tLoss: 154.469421\n",
            "Train Epoch: 39 [520/1400 (37%)]\tLoss: 160.520050\n",
            "Train Epoch: 39 [560/1400 (40%)]\tLoss: 162.996796\n",
            "Train Epoch: 39 [600/1400 (43%)]\tLoss: 138.511475\n",
            "Train Epoch: 39 [640/1400 (46%)]\tLoss: 137.800049\n",
            "Train Epoch: 39 [680/1400 (49%)]\tLoss: 154.613159\n",
            "Train Epoch: 39 [720/1400 (51%)]\tLoss: 155.939728\n",
            "Train Epoch: 39 [760/1400 (54%)]\tLoss: 127.558823\n",
            "Train Epoch: 39 [800/1400 (57%)]\tLoss: 141.433853\n",
            "Train Epoch: 39 [840/1400 (60%)]\tLoss: 120.692673\n",
            "Train Epoch: 39 [880/1400 (63%)]\tLoss: 132.610825\n",
            "Train Epoch: 39 [920/1400 (66%)]\tLoss: 150.856247\n",
            "Train Epoch: 39 [960/1400 (69%)]\tLoss: 142.884537\n",
            "Train Epoch: 39 [1000/1400 (71%)]\tLoss: 135.434479\n",
            "Train Epoch: 39 [1040/1400 (74%)]\tLoss: 159.103928\n",
            "Train Epoch: 39 [1080/1400 (77%)]\tLoss: 126.672768\n",
            "Train Epoch: 39 [1120/1400 (80%)]\tLoss: 139.190140\n",
            "Train Epoch: 39 [1160/1400 (83%)]\tLoss: 143.923325\n",
            "Train Epoch: 39 [1200/1400 (86%)]\tLoss: 129.163971\n",
            "Train Epoch: 39 [1240/1400 (89%)]\tLoss: 148.108597\n",
            "Train Epoch: 39 [1280/1400 (91%)]\tLoss: 133.496246\n",
            "Train Epoch: 39 [1320/1400 (94%)]\tLoss: 147.278137\n",
            "Train Epoch: 39 [1360/1400 (97%)]\tLoss: 138.362015\n",
            "====> Epoch: 39 Average loss: 141.1451\n",
            "====> Test set loss: 144.3636\n",
            "Train Epoch: 40 [0/1400 (0%)]\tLoss: 153.528992\n",
            "Train Epoch: 40 [40/1400 (3%)]\tLoss: 134.757095\n",
            "Train Epoch: 40 [80/1400 (6%)]\tLoss: 130.629456\n",
            "Train Epoch: 40 [120/1400 (9%)]\tLoss: 146.152710\n",
            "Train Epoch: 40 [160/1400 (11%)]\tLoss: 142.528122\n",
            "Train Epoch: 40 [200/1400 (14%)]\tLoss: 159.004807\n",
            "Train Epoch: 40 [240/1400 (17%)]\tLoss: 119.558250\n",
            "Train Epoch: 40 [280/1400 (20%)]\tLoss: 138.211395\n",
            "Train Epoch: 40 [320/1400 (23%)]\tLoss: 133.264633\n",
            "Train Epoch: 40 [360/1400 (26%)]\tLoss: 132.385101\n",
            "Train Epoch: 40 [400/1400 (29%)]\tLoss: 119.251144\n",
            "Train Epoch: 40 [440/1400 (31%)]\tLoss: 133.658463\n",
            "Train Epoch: 40 [480/1400 (34%)]\tLoss: 133.560425\n",
            "Train Epoch: 40 [520/1400 (37%)]\tLoss: 159.262695\n",
            "Train Epoch: 40 [560/1400 (40%)]\tLoss: 147.822311\n",
            "Train Epoch: 40 [600/1400 (43%)]\tLoss: 159.168991\n",
            "Train Epoch: 40 [640/1400 (46%)]\tLoss: 131.923340\n",
            "Train Epoch: 40 [680/1400 (49%)]\tLoss: 144.164398\n",
            "Train Epoch: 40 [720/1400 (51%)]\tLoss: 155.803131\n",
            "Train Epoch: 40 [760/1400 (54%)]\tLoss: 147.560501\n",
            "Train Epoch: 40 [800/1400 (57%)]\tLoss: 148.720367\n",
            "Train Epoch: 40 [840/1400 (60%)]\tLoss: 137.629318\n",
            "Train Epoch: 40 [880/1400 (63%)]\tLoss: 141.847366\n",
            "Train Epoch: 40 [920/1400 (66%)]\tLoss: 113.743469\n",
            "Train Epoch: 40 [960/1400 (69%)]\tLoss: 138.420013\n",
            "Train Epoch: 40 [1000/1400 (71%)]\tLoss: 143.249313\n",
            "Train Epoch: 40 [1040/1400 (74%)]\tLoss: 148.398697\n",
            "Train Epoch: 40 [1080/1400 (77%)]\tLoss: 138.354156\n",
            "Train Epoch: 40 [1120/1400 (80%)]\tLoss: 142.775085\n",
            "Train Epoch: 40 [1160/1400 (83%)]\tLoss: 139.077316\n",
            "Train Epoch: 40 [1200/1400 (86%)]\tLoss: 126.305092\n",
            "Train Epoch: 40 [1240/1400 (89%)]\tLoss: 142.219269\n",
            "Train Epoch: 40 [1280/1400 (91%)]\tLoss: 129.951828\n",
            "Train Epoch: 40 [1320/1400 (94%)]\tLoss: 160.223541\n",
            "Train Epoch: 40 [1360/1400 (97%)]\tLoss: 109.506523\n",
            "====> Epoch: 40 Average loss: 140.9641\n",
            "====> Test set loss: 143.9338\n",
            "Train Epoch: 41 [0/1400 (0%)]\tLoss: 150.852188\n",
            "Train Epoch: 41 [40/1400 (3%)]\tLoss: 130.648529\n",
            "Train Epoch: 41 [80/1400 (6%)]\tLoss: 140.882996\n",
            "Train Epoch: 41 [120/1400 (9%)]\tLoss: 156.509537\n",
            "Train Epoch: 41 [160/1400 (11%)]\tLoss: 157.210098\n",
            "Train Epoch: 41 [200/1400 (14%)]\tLoss: 135.656448\n",
            "Train Epoch: 41 [240/1400 (17%)]\tLoss: 137.288559\n",
            "Train Epoch: 41 [280/1400 (20%)]\tLoss: 157.568451\n",
            "Train Epoch: 41 [320/1400 (23%)]\tLoss: 126.266594\n",
            "Train Epoch: 41 [360/1400 (26%)]\tLoss: 141.990616\n",
            "Train Epoch: 41 [400/1400 (29%)]\tLoss: 140.368576\n",
            "Train Epoch: 41 [440/1400 (31%)]\tLoss: 125.622391\n",
            "Train Epoch: 41 [480/1400 (34%)]\tLoss: 161.725098\n",
            "Train Epoch: 41 [520/1400 (37%)]\tLoss: 153.147125\n",
            "Train Epoch: 41 [560/1400 (40%)]\tLoss: 131.187958\n",
            "Train Epoch: 41 [600/1400 (43%)]\tLoss: 151.196838\n",
            "Train Epoch: 41 [640/1400 (46%)]\tLoss: 124.482079\n",
            "Train Epoch: 41 [680/1400 (49%)]\tLoss: 116.227905\n",
            "Train Epoch: 41 [720/1400 (51%)]\tLoss: 132.766907\n",
            "Train Epoch: 41 [760/1400 (54%)]\tLoss: 128.661880\n",
            "Train Epoch: 41 [800/1400 (57%)]\tLoss: 139.151917\n",
            "Train Epoch: 41 [840/1400 (60%)]\tLoss: 144.278214\n",
            "Train Epoch: 41 [880/1400 (63%)]\tLoss: 140.360397\n",
            "Train Epoch: 41 [920/1400 (66%)]\tLoss: 154.736954\n",
            "Train Epoch: 41 [960/1400 (69%)]\tLoss: 143.294281\n",
            "Train Epoch: 41 [1000/1400 (71%)]\tLoss: 143.855347\n",
            "Train Epoch: 41 [1040/1400 (74%)]\tLoss: 150.785370\n",
            "Train Epoch: 41 [1080/1400 (77%)]\tLoss: 153.065735\n",
            "Train Epoch: 41 [1120/1400 (80%)]\tLoss: 130.903046\n",
            "Train Epoch: 41 [1160/1400 (83%)]\tLoss: 149.351425\n",
            "Train Epoch: 41 [1200/1400 (86%)]\tLoss: 124.257339\n",
            "Train Epoch: 41 [1240/1400 (89%)]\tLoss: 133.471329\n",
            "Train Epoch: 41 [1280/1400 (91%)]\tLoss: 148.763397\n",
            "Train Epoch: 41 [1320/1400 (94%)]\tLoss: 122.611992\n",
            "Train Epoch: 41 [1360/1400 (97%)]\tLoss: 122.622253\n",
            "====> Epoch: 41 Average loss: 140.8821\n",
            "====> Test set loss: 143.8328\n",
            "Train Epoch: 42 [0/1400 (0%)]\tLoss: 125.213379\n",
            "Train Epoch: 42 [40/1400 (3%)]\tLoss: 142.083313\n",
            "Train Epoch: 42 [80/1400 (6%)]\tLoss: 128.696915\n",
            "Train Epoch: 42 [120/1400 (9%)]\tLoss: 124.530449\n",
            "Train Epoch: 42 [160/1400 (11%)]\tLoss: 138.403015\n",
            "Train Epoch: 42 [200/1400 (14%)]\tLoss: 157.980453\n",
            "Train Epoch: 42 [240/1400 (17%)]\tLoss: 136.920120\n",
            "Train Epoch: 42 [280/1400 (20%)]\tLoss: 167.402786\n",
            "Train Epoch: 42 [320/1400 (23%)]\tLoss: 140.274445\n",
            "Train Epoch: 42 [360/1400 (26%)]\tLoss: 149.826035\n",
            "Train Epoch: 42 [400/1400 (29%)]\tLoss: 139.797501\n",
            "Train Epoch: 42 [440/1400 (31%)]\tLoss: 157.815933\n",
            "Train Epoch: 42 [480/1400 (34%)]\tLoss: 172.435455\n",
            "Train Epoch: 42 [520/1400 (37%)]\tLoss: 141.910278\n",
            "Train Epoch: 42 [560/1400 (40%)]\tLoss: 146.522949\n",
            "Train Epoch: 42 [600/1400 (43%)]\tLoss: 151.086899\n",
            "Train Epoch: 42 [640/1400 (46%)]\tLoss: 146.395905\n",
            "Train Epoch: 42 [680/1400 (49%)]\tLoss: 146.501465\n",
            "Train Epoch: 42 [720/1400 (51%)]\tLoss: 136.163483\n",
            "Train Epoch: 42 [760/1400 (54%)]\tLoss: 149.942001\n",
            "Train Epoch: 42 [800/1400 (57%)]\tLoss: 137.605713\n",
            "Train Epoch: 42 [840/1400 (60%)]\tLoss: 147.533966\n",
            "Train Epoch: 42 [880/1400 (63%)]\tLoss: 141.698730\n",
            "Train Epoch: 42 [920/1400 (66%)]\tLoss: 140.460449\n",
            "Train Epoch: 42 [960/1400 (69%)]\tLoss: 141.937057\n",
            "Train Epoch: 42 [1000/1400 (71%)]\tLoss: 137.869843\n",
            "Train Epoch: 42 [1040/1400 (74%)]\tLoss: 134.690353\n",
            "Train Epoch: 42 [1080/1400 (77%)]\tLoss: 116.198196\n",
            "Train Epoch: 42 [1120/1400 (80%)]\tLoss: 146.015518\n",
            "Train Epoch: 42 [1160/1400 (83%)]\tLoss: 127.996269\n",
            "Train Epoch: 42 [1200/1400 (86%)]\tLoss: 156.290222\n",
            "Train Epoch: 42 [1240/1400 (89%)]\tLoss: 148.222977\n",
            "Train Epoch: 42 [1280/1400 (91%)]\tLoss: 137.112686\n",
            "Train Epoch: 42 [1320/1400 (94%)]\tLoss: 151.030930\n",
            "Train Epoch: 42 [1360/1400 (97%)]\tLoss: 132.847092\n",
            "====> Epoch: 42 Average loss: 140.7293\n",
            "====> Test set loss: 143.9672\n",
            "Train Epoch: 43 [0/1400 (0%)]\tLoss: 126.872505\n",
            "Train Epoch: 43 [40/1400 (3%)]\tLoss: 149.991119\n",
            "Train Epoch: 43 [80/1400 (6%)]\tLoss: 138.762512\n",
            "Train Epoch: 43 [120/1400 (9%)]\tLoss: 126.007629\n",
            "Train Epoch: 43 [160/1400 (11%)]\tLoss: 141.995468\n",
            "Train Epoch: 43 [200/1400 (14%)]\tLoss: 153.900482\n",
            "Train Epoch: 43 [240/1400 (17%)]\tLoss: 114.343735\n",
            "Train Epoch: 43 [280/1400 (20%)]\tLoss: 149.026047\n",
            "Train Epoch: 43 [320/1400 (23%)]\tLoss: 141.801743\n",
            "Train Epoch: 43 [360/1400 (26%)]\tLoss: 128.476456\n",
            "Train Epoch: 43 [400/1400 (29%)]\tLoss: 138.272308\n",
            "Train Epoch: 43 [440/1400 (31%)]\tLoss: 141.275177\n",
            "Train Epoch: 43 [480/1400 (34%)]\tLoss: 138.241669\n",
            "Train Epoch: 43 [520/1400 (37%)]\tLoss: 161.780823\n",
            "Train Epoch: 43 [560/1400 (40%)]\tLoss: 125.873039\n",
            "Train Epoch: 43 [600/1400 (43%)]\tLoss: 149.565598\n",
            "Train Epoch: 43 [640/1400 (46%)]\tLoss: 141.843155\n",
            "Train Epoch: 43 [680/1400 (49%)]\tLoss: 131.501541\n",
            "Train Epoch: 43 [720/1400 (51%)]\tLoss: 155.857117\n",
            "Train Epoch: 43 [760/1400 (54%)]\tLoss: 152.863922\n",
            "Train Epoch: 43 [800/1400 (57%)]\tLoss: 138.736099\n",
            "Train Epoch: 43 [840/1400 (60%)]\tLoss: 141.180405\n",
            "Train Epoch: 43 [880/1400 (63%)]\tLoss: 135.186905\n",
            "Train Epoch: 43 [920/1400 (66%)]\tLoss: 154.926651\n",
            "Train Epoch: 43 [960/1400 (69%)]\tLoss: 161.992752\n",
            "Train Epoch: 43 [1000/1400 (71%)]\tLoss: 154.064972\n",
            "Train Epoch: 43 [1040/1400 (74%)]\tLoss: 136.177063\n",
            "Train Epoch: 43 [1080/1400 (77%)]\tLoss: 147.012299\n",
            "Train Epoch: 43 [1120/1400 (80%)]\tLoss: 135.742203\n",
            "Train Epoch: 43 [1160/1400 (83%)]\tLoss: 125.265038\n",
            "Train Epoch: 43 [1200/1400 (86%)]\tLoss: 148.883163\n",
            "Train Epoch: 43 [1240/1400 (89%)]\tLoss: 146.942444\n",
            "Train Epoch: 43 [1280/1400 (91%)]\tLoss: 136.438446\n",
            "Train Epoch: 43 [1320/1400 (94%)]\tLoss: 134.830933\n",
            "Train Epoch: 43 [1360/1400 (97%)]\tLoss: 157.635712\n",
            "====> Epoch: 43 Average loss: 140.5168\n",
            "====> Test set loss: 143.2945\n",
            "Train Epoch: 44 [0/1400 (0%)]\tLoss: 126.766838\n",
            "Train Epoch: 44 [40/1400 (3%)]\tLoss: 152.141251\n",
            "Train Epoch: 44 [80/1400 (6%)]\tLoss: 132.301575\n",
            "Train Epoch: 44 [120/1400 (9%)]\tLoss: 152.960800\n",
            "Train Epoch: 44 [160/1400 (11%)]\tLoss: 115.935066\n",
            "Train Epoch: 44 [200/1400 (14%)]\tLoss: 138.361053\n",
            "Train Epoch: 44 [240/1400 (17%)]\tLoss: 132.884705\n",
            "Train Epoch: 44 [280/1400 (20%)]\tLoss: 147.661484\n",
            "Train Epoch: 44 [320/1400 (23%)]\tLoss: 130.529572\n",
            "Train Epoch: 44 [360/1400 (26%)]\tLoss: 155.235138\n",
            "Train Epoch: 44 [400/1400 (29%)]\tLoss: 119.711533\n",
            "Train Epoch: 44 [440/1400 (31%)]\tLoss: 146.728333\n",
            "Train Epoch: 44 [480/1400 (34%)]\tLoss: 142.204651\n",
            "Train Epoch: 44 [520/1400 (37%)]\tLoss: 130.892151\n",
            "Train Epoch: 44 [560/1400 (40%)]\tLoss: 122.982338\n",
            "Train Epoch: 44 [600/1400 (43%)]\tLoss: 150.488495\n",
            "Train Epoch: 44 [640/1400 (46%)]\tLoss: 175.234543\n",
            "Train Epoch: 44 [680/1400 (49%)]\tLoss: 157.287018\n",
            "Train Epoch: 44 [720/1400 (51%)]\tLoss: 157.709045\n",
            "Train Epoch: 44 [760/1400 (54%)]\tLoss: 139.563339\n",
            "Train Epoch: 44 [800/1400 (57%)]\tLoss: 145.280106\n",
            "Train Epoch: 44 [840/1400 (60%)]\tLoss: 143.846664\n",
            "Train Epoch: 44 [880/1400 (63%)]\tLoss: 145.445618\n",
            "Train Epoch: 44 [920/1400 (66%)]\tLoss: 119.775841\n",
            "Train Epoch: 44 [960/1400 (69%)]\tLoss: 146.493393\n",
            "Train Epoch: 44 [1000/1400 (71%)]\tLoss: 145.147339\n",
            "Train Epoch: 44 [1040/1400 (74%)]\tLoss: 138.516464\n",
            "Train Epoch: 44 [1080/1400 (77%)]\tLoss: 156.712921\n",
            "Train Epoch: 44 [1120/1400 (80%)]\tLoss: 130.522461\n",
            "Train Epoch: 44 [1160/1400 (83%)]\tLoss: 130.644043\n",
            "Train Epoch: 44 [1200/1400 (86%)]\tLoss: 110.036095\n",
            "Train Epoch: 44 [1240/1400 (89%)]\tLoss: 139.881104\n",
            "Train Epoch: 44 [1280/1400 (91%)]\tLoss: 148.332504\n",
            "Train Epoch: 44 [1320/1400 (94%)]\tLoss: 139.042984\n",
            "Train Epoch: 44 [1360/1400 (97%)]\tLoss: 127.742004\n",
            "====> Epoch: 44 Average loss: 140.4849\n",
            "====> Test set loss: 143.2150\n",
            "Train Epoch: 45 [0/1400 (0%)]\tLoss: 131.011536\n",
            "Train Epoch: 45 [40/1400 (3%)]\tLoss: 150.881714\n",
            "Train Epoch: 45 [80/1400 (6%)]\tLoss: 155.582809\n",
            "Train Epoch: 45 [120/1400 (9%)]\tLoss: 138.687775\n",
            "Train Epoch: 45 [160/1400 (11%)]\tLoss: 124.818008\n",
            "Train Epoch: 45 [200/1400 (14%)]\tLoss: 131.533157\n",
            "Train Epoch: 45 [240/1400 (17%)]\tLoss: 134.595093\n",
            "Train Epoch: 45 [280/1400 (20%)]\tLoss: 149.347488\n",
            "Train Epoch: 45 [320/1400 (23%)]\tLoss: 139.740097\n",
            "Train Epoch: 45 [360/1400 (26%)]\tLoss: 154.203583\n",
            "Train Epoch: 45 [400/1400 (29%)]\tLoss: 158.616425\n",
            "Train Epoch: 45 [440/1400 (31%)]\tLoss: 167.011887\n",
            "Train Epoch: 45 [480/1400 (34%)]\tLoss: 147.311752\n",
            "Train Epoch: 45 [520/1400 (37%)]\tLoss: 140.528671\n",
            "Train Epoch: 45 [560/1400 (40%)]\tLoss: 128.216339\n",
            "Train Epoch: 45 [600/1400 (43%)]\tLoss: 141.653839\n",
            "Train Epoch: 45 [640/1400 (46%)]\tLoss: 118.497505\n",
            "Train Epoch: 45 [680/1400 (49%)]\tLoss: 163.214447\n",
            "Train Epoch: 45 [720/1400 (51%)]\tLoss: 144.820221\n",
            "Train Epoch: 45 [760/1400 (54%)]\tLoss: 143.573303\n",
            "Train Epoch: 45 [800/1400 (57%)]\tLoss: 155.300308\n",
            "Train Epoch: 45 [840/1400 (60%)]\tLoss: 139.736496\n",
            "Train Epoch: 45 [880/1400 (63%)]\tLoss: 129.743576\n",
            "Train Epoch: 45 [920/1400 (66%)]\tLoss: 138.787613\n",
            "Train Epoch: 45 [960/1400 (69%)]\tLoss: 155.374496\n",
            "Train Epoch: 45 [1000/1400 (71%)]\tLoss: 153.952133\n",
            "Train Epoch: 45 [1040/1400 (74%)]\tLoss: 124.860405\n",
            "Train Epoch: 45 [1080/1400 (77%)]\tLoss: 145.388962\n",
            "Train Epoch: 45 [1120/1400 (80%)]\tLoss: 162.545517\n",
            "Train Epoch: 45 [1160/1400 (83%)]\tLoss: 136.143082\n",
            "Train Epoch: 45 [1200/1400 (86%)]\tLoss: 138.272125\n",
            "Train Epoch: 45 [1240/1400 (89%)]\tLoss: 134.073563\n",
            "Train Epoch: 45 [1280/1400 (91%)]\tLoss: 137.251099\n",
            "Train Epoch: 45 [1320/1400 (94%)]\tLoss: 147.978546\n",
            "Train Epoch: 45 [1360/1400 (97%)]\tLoss: 144.109085\n",
            "====> Epoch: 45 Average loss: 140.3721\n",
            "====> Test set loss: 143.6682\n",
            "Train Epoch: 46 [0/1400 (0%)]\tLoss: 153.831726\n",
            "Train Epoch: 46 [40/1400 (3%)]\tLoss: 135.350586\n",
            "Train Epoch: 46 [80/1400 (6%)]\tLoss: 131.719681\n",
            "Train Epoch: 46 [120/1400 (9%)]\tLoss: 153.166443\n",
            "Train Epoch: 46 [160/1400 (11%)]\tLoss: 138.228973\n",
            "Train Epoch: 46 [200/1400 (14%)]\tLoss: 150.610046\n",
            "Train Epoch: 46 [240/1400 (17%)]\tLoss: 142.994965\n",
            "Train Epoch: 46 [280/1400 (20%)]\tLoss: 147.154282\n",
            "Train Epoch: 46 [320/1400 (23%)]\tLoss: 137.773788\n",
            "Train Epoch: 46 [360/1400 (26%)]\tLoss: 135.017471\n",
            "Train Epoch: 46 [400/1400 (29%)]\tLoss: 139.836060\n",
            "Train Epoch: 46 [440/1400 (31%)]\tLoss: 138.967438\n",
            "Train Epoch: 46 [480/1400 (34%)]\tLoss: 144.961578\n",
            "Train Epoch: 46 [520/1400 (37%)]\tLoss: 148.028030\n",
            "Train Epoch: 46 [560/1400 (40%)]\tLoss: 162.394928\n",
            "Train Epoch: 46 [600/1400 (43%)]\tLoss: 136.949478\n",
            "Train Epoch: 46 [640/1400 (46%)]\tLoss: 147.086014\n",
            "Train Epoch: 46 [680/1400 (49%)]\tLoss: 145.376678\n",
            "Train Epoch: 46 [720/1400 (51%)]\tLoss: 149.598099\n",
            "Train Epoch: 46 [760/1400 (54%)]\tLoss: 133.740784\n",
            "Train Epoch: 46 [800/1400 (57%)]\tLoss: 128.691391\n",
            "Train Epoch: 46 [840/1400 (60%)]\tLoss: 130.650925\n",
            "Train Epoch: 46 [880/1400 (63%)]\tLoss: 113.973099\n",
            "Train Epoch: 46 [920/1400 (66%)]\tLoss: 134.426010\n",
            "Train Epoch: 46 [960/1400 (69%)]\tLoss: 122.044533\n",
            "Train Epoch: 46 [1000/1400 (71%)]\tLoss: 144.982376\n",
            "Train Epoch: 46 [1040/1400 (74%)]\tLoss: 141.884033\n",
            "Train Epoch: 46 [1080/1400 (77%)]\tLoss: 146.198334\n",
            "Train Epoch: 46 [1120/1400 (80%)]\tLoss: 153.883926\n",
            "Train Epoch: 46 [1160/1400 (83%)]\tLoss: 159.466293\n",
            "Train Epoch: 46 [1200/1400 (86%)]\tLoss: 143.535065\n",
            "Train Epoch: 46 [1240/1400 (89%)]\tLoss: 127.161064\n",
            "Train Epoch: 46 [1280/1400 (91%)]\tLoss: 140.742294\n",
            "Train Epoch: 46 [1320/1400 (94%)]\tLoss: 137.145050\n",
            "Train Epoch: 46 [1360/1400 (97%)]\tLoss: 131.468597\n",
            "====> Epoch: 46 Average loss: 140.2181\n",
            "====> Test set loss: 143.4699\n",
            "Train Epoch: 47 [0/1400 (0%)]\tLoss: 147.611572\n",
            "Train Epoch: 47 [40/1400 (3%)]\tLoss: 137.525162\n",
            "Train Epoch: 47 [80/1400 (6%)]\tLoss: 137.026291\n",
            "Train Epoch: 47 [120/1400 (9%)]\tLoss: 137.703629\n",
            "Train Epoch: 47 [160/1400 (11%)]\tLoss: 144.481140\n",
            "Train Epoch: 47 [200/1400 (14%)]\tLoss: 160.351791\n",
            "Train Epoch: 47 [240/1400 (17%)]\tLoss: 122.173149\n",
            "Train Epoch: 47 [280/1400 (20%)]\tLoss: 145.020599\n",
            "Train Epoch: 47 [320/1400 (23%)]\tLoss: 151.900879\n",
            "Train Epoch: 47 [360/1400 (26%)]\tLoss: 145.501633\n",
            "Train Epoch: 47 [400/1400 (29%)]\tLoss: 165.209366\n",
            "Train Epoch: 47 [440/1400 (31%)]\tLoss: 143.890457\n",
            "Train Epoch: 47 [480/1400 (34%)]\tLoss: 136.672577\n",
            "Train Epoch: 47 [520/1400 (37%)]\tLoss: 125.257683\n",
            "Train Epoch: 47 [560/1400 (40%)]\tLoss: 157.611984\n",
            "Train Epoch: 47 [600/1400 (43%)]\tLoss: 138.392197\n",
            "Train Epoch: 47 [640/1400 (46%)]\tLoss: 128.062500\n",
            "Train Epoch: 47 [680/1400 (49%)]\tLoss: 147.842773\n",
            "Train Epoch: 47 [720/1400 (51%)]\tLoss: 146.958176\n",
            "Train Epoch: 47 [760/1400 (54%)]\tLoss: 149.135468\n",
            "Train Epoch: 47 [800/1400 (57%)]\tLoss: 143.992340\n",
            "Train Epoch: 47 [840/1400 (60%)]\tLoss: 164.755951\n",
            "Train Epoch: 47 [880/1400 (63%)]\tLoss: 153.107605\n",
            "Train Epoch: 47 [920/1400 (66%)]\tLoss: 128.984604\n",
            "Train Epoch: 47 [960/1400 (69%)]\tLoss: 144.114914\n",
            "Train Epoch: 47 [1000/1400 (71%)]\tLoss: 149.279190\n",
            "Train Epoch: 47 [1040/1400 (74%)]\tLoss: 144.006989\n",
            "Train Epoch: 47 [1080/1400 (77%)]\tLoss: 134.979202\n",
            "Train Epoch: 47 [1120/1400 (80%)]\tLoss: 150.968613\n",
            "Train Epoch: 47 [1160/1400 (83%)]\tLoss: 125.208183\n",
            "Train Epoch: 47 [1200/1400 (86%)]\tLoss: 130.785400\n",
            "Train Epoch: 47 [1240/1400 (89%)]\tLoss: 143.211472\n",
            "Train Epoch: 47 [1280/1400 (91%)]\tLoss: 155.396942\n",
            "Train Epoch: 47 [1320/1400 (94%)]\tLoss: 142.411667\n",
            "Train Epoch: 47 [1360/1400 (97%)]\tLoss: 136.591995\n",
            "====> Epoch: 47 Average loss: 140.2138\n",
            "====> Test set loss: 143.2764\n",
            "Train Epoch: 48 [0/1400 (0%)]\tLoss: 147.800888\n",
            "Train Epoch: 48 [40/1400 (3%)]\tLoss: 136.108704\n",
            "Train Epoch: 48 [80/1400 (6%)]\tLoss: 152.981705\n",
            "Train Epoch: 48 [120/1400 (9%)]\tLoss: 135.021744\n",
            "Train Epoch: 48 [160/1400 (11%)]\tLoss: 148.620102\n",
            "Train Epoch: 48 [200/1400 (14%)]\tLoss: 97.652557\n",
            "Train Epoch: 48 [240/1400 (17%)]\tLoss: 145.334671\n",
            "Train Epoch: 48 [280/1400 (20%)]\tLoss: 138.798340\n",
            "Train Epoch: 48 [320/1400 (23%)]\tLoss: 145.108398\n",
            "Train Epoch: 48 [360/1400 (26%)]\tLoss: 147.096817\n",
            "Train Epoch: 48 [400/1400 (29%)]\tLoss: 151.119064\n",
            "Train Epoch: 48 [440/1400 (31%)]\tLoss: 146.309814\n",
            "Train Epoch: 48 [480/1400 (34%)]\tLoss: 145.644562\n",
            "Train Epoch: 48 [520/1400 (37%)]\tLoss: 146.574127\n",
            "Train Epoch: 48 [560/1400 (40%)]\tLoss: 160.620132\n",
            "Train Epoch: 48 [600/1400 (43%)]\tLoss: 133.870422\n",
            "Train Epoch: 48 [640/1400 (46%)]\tLoss: 123.078735\n",
            "Train Epoch: 48 [680/1400 (49%)]\tLoss: 153.869781\n",
            "Train Epoch: 48 [720/1400 (51%)]\tLoss: 132.340271\n",
            "Train Epoch: 48 [760/1400 (54%)]\tLoss: 143.490433\n",
            "Train Epoch: 48 [800/1400 (57%)]\tLoss: 127.329643\n",
            "Train Epoch: 48 [840/1400 (60%)]\tLoss: 146.428818\n",
            "Train Epoch: 48 [880/1400 (63%)]\tLoss: 116.135498\n",
            "Train Epoch: 48 [920/1400 (66%)]\tLoss: 149.344681\n",
            "Train Epoch: 48 [960/1400 (69%)]\tLoss: 156.637466\n",
            "Train Epoch: 48 [1000/1400 (71%)]\tLoss: 139.346588\n",
            "Train Epoch: 48 [1040/1400 (74%)]\tLoss: 162.765198\n",
            "Train Epoch: 48 [1080/1400 (77%)]\tLoss: 152.970169\n",
            "Train Epoch: 48 [1120/1400 (80%)]\tLoss: 161.631348\n",
            "Train Epoch: 48 [1160/1400 (83%)]\tLoss: 160.246231\n",
            "Train Epoch: 48 [1200/1400 (86%)]\tLoss: 138.095123\n",
            "Train Epoch: 48 [1240/1400 (89%)]\tLoss: 152.867554\n",
            "Train Epoch: 48 [1280/1400 (91%)]\tLoss: 135.240677\n",
            "Train Epoch: 48 [1320/1400 (94%)]\tLoss: 130.122421\n",
            "Train Epoch: 48 [1360/1400 (97%)]\tLoss: 111.056419\n",
            "====> Epoch: 48 Average loss: 140.0977\n",
            "====> Test set loss: 143.4903\n",
            "Train Epoch: 49 [0/1400 (0%)]\tLoss: 130.311768\n",
            "Train Epoch: 49 [40/1400 (3%)]\tLoss: 137.070007\n",
            "Train Epoch: 49 [80/1400 (6%)]\tLoss: 115.523697\n",
            "Train Epoch: 49 [120/1400 (9%)]\tLoss: 149.237030\n",
            "Train Epoch: 49 [160/1400 (11%)]\tLoss: 139.062317\n",
            "Train Epoch: 49 [200/1400 (14%)]\tLoss: 111.097710\n",
            "Train Epoch: 49 [240/1400 (17%)]\tLoss: 136.973663\n",
            "Train Epoch: 49 [280/1400 (20%)]\tLoss: 130.676926\n",
            "Train Epoch: 49 [320/1400 (23%)]\tLoss: 143.448608\n",
            "Train Epoch: 49 [360/1400 (26%)]\tLoss: 134.089981\n",
            "Train Epoch: 49 [400/1400 (29%)]\tLoss: 166.404739\n",
            "Train Epoch: 49 [440/1400 (31%)]\tLoss: 152.718201\n",
            "Train Epoch: 49 [480/1400 (34%)]\tLoss: 118.962410\n",
            "Train Epoch: 49 [520/1400 (37%)]\tLoss: 157.059708\n",
            "Train Epoch: 49 [560/1400 (40%)]\tLoss: 145.769379\n",
            "Train Epoch: 49 [600/1400 (43%)]\tLoss: 125.437737\n",
            "Train Epoch: 49 [640/1400 (46%)]\tLoss: 138.240402\n",
            "Train Epoch: 49 [680/1400 (49%)]\tLoss: 146.746246\n",
            "Train Epoch: 49 [720/1400 (51%)]\tLoss: 159.609573\n",
            "Train Epoch: 49 [760/1400 (54%)]\tLoss: 134.522385\n",
            "Train Epoch: 49 [800/1400 (57%)]\tLoss: 124.259834\n",
            "Train Epoch: 49 [840/1400 (60%)]\tLoss: 137.154343\n",
            "Train Epoch: 49 [880/1400 (63%)]\tLoss: 122.501228\n",
            "Train Epoch: 49 [920/1400 (66%)]\tLoss: 149.917953\n",
            "Train Epoch: 49 [960/1400 (69%)]\tLoss: 131.868179\n",
            "Train Epoch: 49 [1000/1400 (71%)]\tLoss: 159.232101\n",
            "Train Epoch: 49 [1040/1400 (74%)]\tLoss: 151.955276\n",
            "Train Epoch: 49 [1080/1400 (77%)]\tLoss: 158.890594\n",
            "Train Epoch: 49 [1120/1400 (80%)]\tLoss: 135.555649\n",
            "Train Epoch: 49 [1160/1400 (83%)]\tLoss: 130.576172\n",
            "Train Epoch: 49 [1200/1400 (86%)]\tLoss: 118.397789\n",
            "Train Epoch: 49 [1240/1400 (89%)]\tLoss: 143.212173\n",
            "Train Epoch: 49 [1280/1400 (91%)]\tLoss: 162.500107\n",
            "Train Epoch: 49 [1320/1400 (94%)]\tLoss: 133.382980\n",
            "Train Epoch: 49 [1360/1400 (97%)]\tLoss: 160.972534\n",
            "====> Epoch: 49 Average loss: 140.0369\n",
            "====> Test set loss: 143.0790\n",
            "Train Epoch: 50 [0/1400 (0%)]\tLoss: 168.661453\n",
            "Train Epoch: 50 [40/1400 (3%)]\tLoss: 149.218765\n",
            "Train Epoch: 50 [80/1400 (6%)]\tLoss: 156.273422\n",
            "Train Epoch: 50 [120/1400 (9%)]\tLoss: 120.267242\n",
            "Train Epoch: 50 [160/1400 (11%)]\tLoss: 147.263428\n",
            "Train Epoch: 50 [200/1400 (14%)]\tLoss: 129.023346\n",
            "Train Epoch: 50 [240/1400 (17%)]\tLoss: 129.380264\n",
            "Train Epoch: 50 [280/1400 (20%)]\tLoss: 122.400185\n",
            "Train Epoch: 50 [320/1400 (23%)]\tLoss: 150.318848\n",
            "Train Epoch: 50 [360/1400 (26%)]\tLoss: 140.117264\n",
            "Train Epoch: 50 [400/1400 (29%)]\tLoss: 142.108246\n",
            "Train Epoch: 50 [440/1400 (31%)]\tLoss: 130.107162\n",
            "Train Epoch: 50 [480/1400 (34%)]\tLoss: 144.069946\n",
            "Train Epoch: 50 [520/1400 (37%)]\tLoss: 131.039658\n",
            "Train Epoch: 50 [560/1400 (40%)]\tLoss: 127.958534\n",
            "Train Epoch: 50 [600/1400 (43%)]\tLoss: 120.675797\n",
            "Train Epoch: 50 [640/1400 (46%)]\tLoss: 135.873871\n",
            "Train Epoch: 50 [680/1400 (49%)]\tLoss: 154.071503\n",
            "Train Epoch: 50 [720/1400 (51%)]\tLoss: 128.803329\n",
            "Train Epoch: 50 [760/1400 (54%)]\tLoss: 132.215820\n",
            "Train Epoch: 50 [800/1400 (57%)]\tLoss: 158.421768\n",
            "Train Epoch: 50 [840/1400 (60%)]\tLoss: 134.644318\n",
            "Train Epoch: 50 [880/1400 (63%)]\tLoss: 131.915756\n",
            "Train Epoch: 50 [920/1400 (66%)]\tLoss: 140.445862\n",
            "Train Epoch: 50 [960/1400 (69%)]\tLoss: 120.819443\n",
            "Train Epoch: 50 [1000/1400 (71%)]\tLoss: 160.922684\n",
            "Train Epoch: 50 [1040/1400 (74%)]\tLoss: 135.513321\n",
            "Train Epoch: 50 [1080/1400 (77%)]\tLoss: 130.283249\n",
            "Train Epoch: 50 [1120/1400 (80%)]\tLoss: 128.948471\n",
            "Train Epoch: 50 [1160/1400 (83%)]\tLoss: 137.370407\n",
            "Train Epoch: 50 [1200/1400 (86%)]\tLoss: 134.722702\n",
            "Train Epoch: 50 [1240/1400 (89%)]\tLoss: 140.367142\n",
            "Train Epoch: 50 [1280/1400 (91%)]\tLoss: 130.510208\n",
            "Train Epoch: 50 [1320/1400 (94%)]\tLoss: 140.401917\n",
            "Train Epoch: 50 [1360/1400 (97%)]\tLoss: 124.106804\n",
            "====> Epoch: 50 Average loss: 139.8168\n",
            "====> Test set loss: 143.0047\n",
            "Train Epoch: 51 [0/1400 (0%)]\tLoss: 116.161362\n",
            "Train Epoch: 51 [40/1400 (3%)]\tLoss: 174.402283\n",
            "Train Epoch: 51 [80/1400 (6%)]\tLoss: 164.527420\n",
            "Train Epoch: 51 [120/1400 (9%)]\tLoss: 129.096268\n",
            "Train Epoch: 51 [160/1400 (11%)]\tLoss: 145.734665\n",
            "Train Epoch: 51 [200/1400 (14%)]\tLoss: 156.285217\n",
            "Train Epoch: 51 [240/1400 (17%)]\tLoss: 119.794144\n",
            "Train Epoch: 51 [280/1400 (20%)]\tLoss: 131.535217\n",
            "Train Epoch: 51 [320/1400 (23%)]\tLoss: 149.693176\n",
            "Train Epoch: 51 [360/1400 (26%)]\tLoss: 146.379761\n",
            "Train Epoch: 51 [400/1400 (29%)]\tLoss: 163.156006\n",
            "Train Epoch: 51 [440/1400 (31%)]\tLoss: 130.505569\n",
            "Train Epoch: 51 [480/1400 (34%)]\tLoss: 126.267906\n",
            "Train Epoch: 51 [520/1400 (37%)]\tLoss: 129.455597\n",
            "Train Epoch: 51 [560/1400 (40%)]\tLoss: 126.857658\n",
            "Train Epoch: 51 [600/1400 (43%)]\tLoss: 130.648483\n",
            "Train Epoch: 51 [640/1400 (46%)]\tLoss: 139.706406\n",
            "Train Epoch: 51 [680/1400 (49%)]\tLoss: 125.940094\n",
            "Train Epoch: 51 [720/1400 (51%)]\tLoss: 121.302193\n",
            "Train Epoch: 51 [760/1400 (54%)]\tLoss: 164.415939\n",
            "Train Epoch: 51 [800/1400 (57%)]\tLoss: 131.837357\n",
            "Train Epoch: 51 [840/1400 (60%)]\tLoss: 137.045303\n",
            "Train Epoch: 51 [880/1400 (63%)]\tLoss: 144.970825\n",
            "Train Epoch: 51 [920/1400 (66%)]\tLoss: 132.844131\n",
            "Train Epoch: 51 [960/1400 (69%)]\tLoss: 156.990433\n",
            "Train Epoch: 51 [1000/1400 (71%)]\tLoss: 126.920052\n",
            "Train Epoch: 51 [1040/1400 (74%)]\tLoss: 129.935944\n",
            "Train Epoch: 51 [1080/1400 (77%)]\tLoss: 126.604561\n",
            "Train Epoch: 51 [1120/1400 (80%)]\tLoss: 141.617966\n",
            "Train Epoch: 51 [1160/1400 (83%)]\tLoss: 157.711670\n",
            "Train Epoch: 51 [1200/1400 (86%)]\tLoss: 125.056549\n",
            "Train Epoch: 51 [1240/1400 (89%)]\tLoss: 124.858345\n",
            "Train Epoch: 51 [1280/1400 (91%)]\tLoss: 140.147034\n",
            "Train Epoch: 51 [1320/1400 (94%)]\tLoss: 140.009979\n",
            "Train Epoch: 51 [1360/1400 (97%)]\tLoss: 128.437973\n",
            "====> Epoch: 51 Average loss: 139.7616\n",
            "====> Test set loss: 142.8277\n",
            "Train Epoch: 52 [0/1400 (0%)]\tLoss: 127.469589\n",
            "Train Epoch: 52 [40/1400 (3%)]\tLoss: 145.367203\n",
            "Train Epoch: 52 [80/1400 (6%)]\tLoss: 142.428207\n",
            "Train Epoch: 52 [120/1400 (9%)]\tLoss: 138.292175\n",
            "Train Epoch: 52 [160/1400 (11%)]\tLoss: 145.907471\n",
            "Train Epoch: 52 [200/1400 (14%)]\tLoss: 149.922043\n",
            "Train Epoch: 52 [240/1400 (17%)]\tLoss: 148.178589\n",
            "Train Epoch: 52 [280/1400 (20%)]\tLoss: 148.514389\n",
            "Train Epoch: 52 [320/1400 (23%)]\tLoss: 157.042313\n",
            "Train Epoch: 52 [360/1400 (26%)]\tLoss: 126.980118\n",
            "Train Epoch: 52 [400/1400 (29%)]\tLoss: 145.872269\n",
            "Train Epoch: 52 [440/1400 (31%)]\tLoss: 118.667152\n",
            "Train Epoch: 52 [480/1400 (34%)]\tLoss: 148.878891\n",
            "Train Epoch: 52 [520/1400 (37%)]\tLoss: 143.375397\n",
            "Train Epoch: 52 [560/1400 (40%)]\tLoss: 130.493744\n",
            "Train Epoch: 52 [600/1400 (43%)]\tLoss: 143.107956\n",
            "Train Epoch: 52 [640/1400 (46%)]\tLoss: 149.348877\n",
            "Train Epoch: 52 [680/1400 (49%)]\tLoss: 135.694260\n",
            "Train Epoch: 52 [720/1400 (51%)]\tLoss: 112.948357\n",
            "Train Epoch: 52 [760/1400 (54%)]\tLoss: 153.309723\n",
            "Train Epoch: 52 [800/1400 (57%)]\tLoss: 141.500305\n",
            "Train Epoch: 52 [840/1400 (60%)]\tLoss: 134.326279\n",
            "Train Epoch: 52 [880/1400 (63%)]\tLoss: 118.233139\n",
            "Train Epoch: 52 [920/1400 (66%)]\tLoss: 134.438339\n",
            "Train Epoch: 52 [960/1400 (69%)]\tLoss: 121.957390\n",
            "Train Epoch: 52 [1000/1400 (71%)]\tLoss: 120.048996\n",
            "Train Epoch: 52 [1040/1400 (74%)]\tLoss: 145.312912\n",
            "Train Epoch: 52 [1080/1400 (77%)]\tLoss: 128.527145\n",
            "Train Epoch: 52 [1120/1400 (80%)]\tLoss: 143.833771\n",
            "Train Epoch: 52 [1160/1400 (83%)]\tLoss: 132.335495\n",
            "Train Epoch: 52 [1200/1400 (86%)]\tLoss: 131.084610\n",
            "Train Epoch: 52 [1240/1400 (89%)]\tLoss: 157.758606\n",
            "Train Epoch: 52 [1280/1400 (91%)]\tLoss: 135.117188\n",
            "Train Epoch: 52 [1320/1400 (94%)]\tLoss: 136.537445\n",
            "Train Epoch: 52 [1360/1400 (97%)]\tLoss: 149.840408\n",
            "====> Epoch: 52 Average loss: 139.6493\n",
            "====> Test set loss: 143.0302\n",
            "Train Epoch: 53 [0/1400 (0%)]\tLoss: 133.205856\n",
            "Train Epoch: 53 [40/1400 (3%)]\tLoss: 141.157745\n",
            "Train Epoch: 53 [80/1400 (6%)]\tLoss: 154.912338\n",
            "Train Epoch: 53 [120/1400 (9%)]\tLoss: 151.887177\n",
            "Train Epoch: 53 [160/1400 (11%)]\tLoss: 148.282333\n",
            "Train Epoch: 53 [200/1400 (14%)]\tLoss: 118.207794\n",
            "Train Epoch: 53 [240/1400 (17%)]\tLoss: 155.579254\n",
            "Train Epoch: 53 [280/1400 (20%)]\tLoss: 125.963318\n",
            "Train Epoch: 53 [320/1400 (23%)]\tLoss: 149.374176\n",
            "Train Epoch: 53 [360/1400 (26%)]\tLoss: 119.746231\n",
            "Train Epoch: 53 [400/1400 (29%)]\tLoss: 128.310349\n",
            "Train Epoch: 53 [440/1400 (31%)]\tLoss: 144.662247\n",
            "Train Epoch: 53 [480/1400 (34%)]\tLoss: 152.731903\n",
            "Train Epoch: 53 [520/1400 (37%)]\tLoss: 112.421738\n",
            "Train Epoch: 53 [560/1400 (40%)]\tLoss: 137.888336\n",
            "Train Epoch: 53 [600/1400 (43%)]\tLoss: 147.579086\n",
            "Train Epoch: 53 [640/1400 (46%)]\tLoss: 127.884857\n",
            "Train Epoch: 53 [680/1400 (49%)]\tLoss: 175.235077\n",
            "Train Epoch: 53 [720/1400 (51%)]\tLoss: 151.177338\n",
            "Train Epoch: 53 [760/1400 (54%)]\tLoss: 140.670044\n",
            "Train Epoch: 53 [800/1400 (57%)]\tLoss: 127.454239\n",
            "Train Epoch: 53 [840/1400 (60%)]\tLoss: 136.688705\n",
            "Train Epoch: 53 [880/1400 (63%)]\tLoss: 160.515991\n",
            "Train Epoch: 53 [920/1400 (66%)]\tLoss: 143.557190\n",
            "Train Epoch: 53 [960/1400 (69%)]\tLoss: 131.360153\n",
            "Train Epoch: 53 [1000/1400 (71%)]\tLoss: 132.819366\n",
            "Train Epoch: 53 [1040/1400 (74%)]\tLoss: 139.870972\n",
            "Train Epoch: 53 [1080/1400 (77%)]\tLoss: 143.499603\n",
            "Train Epoch: 53 [1120/1400 (80%)]\tLoss: 123.174850\n",
            "Train Epoch: 53 [1160/1400 (83%)]\tLoss: 156.086563\n",
            "Train Epoch: 53 [1200/1400 (86%)]\tLoss: 138.582626\n",
            "Train Epoch: 53 [1240/1400 (89%)]\tLoss: 147.037048\n",
            "Train Epoch: 53 [1280/1400 (91%)]\tLoss: 159.197006\n",
            "Train Epoch: 53 [1320/1400 (94%)]\tLoss: 147.331223\n",
            "Train Epoch: 53 [1360/1400 (97%)]\tLoss: 140.226181\n",
            "====> Epoch: 53 Average loss: 139.6624\n",
            "====> Test set loss: 142.8413\n",
            "Train Epoch: 54 [0/1400 (0%)]\tLoss: 161.090790\n",
            "Train Epoch: 54 [40/1400 (3%)]\tLoss: 171.972382\n",
            "Train Epoch: 54 [80/1400 (6%)]\tLoss: 124.051773\n",
            "Train Epoch: 54 [120/1400 (9%)]\tLoss: 143.545670\n",
            "Train Epoch: 54 [160/1400 (11%)]\tLoss: 122.119125\n",
            "Train Epoch: 54 [200/1400 (14%)]\tLoss: 119.984123\n",
            "Train Epoch: 54 [240/1400 (17%)]\tLoss: 129.310104\n",
            "Train Epoch: 54 [280/1400 (20%)]\tLoss: 136.459045\n",
            "Train Epoch: 54 [320/1400 (23%)]\tLoss: 124.039093\n",
            "Train Epoch: 54 [360/1400 (26%)]\tLoss: 121.721825\n",
            "Train Epoch: 54 [400/1400 (29%)]\tLoss: 133.092606\n",
            "Train Epoch: 54 [440/1400 (31%)]\tLoss: 140.355347\n",
            "Train Epoch: 54 [480/1400 (34%)]\tLoss: 149.725082\n",
            "Train Epoch: 54 [520/1400 (37%)]\tLoss: 136.746445\n",
            "Train Epoch: 54 [560/1400 (40%)]\tLoss: 140.977371\n",
            "Train Epoch: 54 [600/1400 (43%)]\tLoss: 149.092529\n",
            "Train Epoch: 54 [640/1400 (46%)]\tLoss: 157.989090\n",
            "Train Epoch: 54 [680/1400 (49%)]\tLoss: 131.137238\n",
            "Train Epoch: 54 [720/1400 (51%)]\tLoss: 136.810623\n",
            "Train Epoch: 54 [760/1400 (54%)]\tLoss: 151.371216\n",
            "Train Epoch: 54 [800/1400 (57%)]\tLoss: 138.927872\n",
            "Train Epoch: 54 [840/1400 (60%)]\tLoss: 134.728210\n",
            "Train Epoch: 54 [880/1400 (63%)]\tLoss: 152.432877\n",
            "Train Epoch: 54 [920/1400 (66%)]\tLoss: 123.104332\n",
            "Train Epoch: 54 [960/1400 (69%)]\tLoss: 142.298599\n",
            "Train Epoch: 54 [1000/1400 (71%)]\tLoss: 131.709656\n",
            "Train Epoch: 54 [1040/1400 (74%)]\tLoss: 128.003677\n",
            "Train Epoch: 54 [1080/1400 (77%)]\tLoss: 136.725021\n",
            "Train Epoch: 54 [1120/1400 (80%)]\tLoss: 138.573730\n",
            "Train Epoch: 54 [1160/1400 (83%)]\tLoss: 141.765579\n",
            "Train Epoch: 54 [1200/1400 (86%)]\tLoss: 129.120651\n",
            "Train Epoch: 54 [1240/1400 (89%)]\tLoss: 146.277100\n",
            "Train Epoch: 54 [1280/1400 (91%)]\tLoss: 137.425476\n",
            "Train Epoch: 54 [1320/1400 (94%)]\tLoss: 130.315430\n",
            "Train Epoch: 54 [1360/1400 (97%)]\tLoss: 132.171585\n",
            "====> Epoch: 54 Average loss: 139.5033\n",
            "====> Test set loss: 142.5022\n",
            "Train Epoch: 55 [0/1400 (0%)]\tLoss: 134.760391\n",
            "Train Epoch: 55 [40/1400 (3%)]\tLoss: 129.992645\n",
            "Train Epoch: 55 [80/1400 (6%)]\tLoss: 152.451416\n",
            "Train Epoch: 55 [120/1400 (9%)]\tLoss: 129.845581\n",
            "Train Epoch: 55 [160/1400 (11%)]\tLoss: 137.513535\n",
            "Train Epoch: 55 [200/1400 (14%)]\tLoss: 146.893387\n",
            "Train Epoch: 55 [240/1400 (17%)]\tLoss: 135.268402\n",
            "Train Epoch: 55 [280/1400 (20%)]\tLoss: 133.862213\n",
            "Train Epoch: 55 [320/1400 (23%)]\tLoss: 143.486298\n",
            "Train Epoch: 55 [360/1400 (26%)]\tLoss: 137.740005\n",
            "Train Epoch: 55 [400/1400 (29%)]\tLoss: 138.645187\n",
            "Train Epoch: 55 [440/1400 (31%)]\tLoss: 151.482803\n",
            "Train Epoch: 55 [480/1400 (34%)]\tLoss: 149.315475\n",
            "Train Epoch: 55 [520/1400 (37%)]\tLoss: 106.875259\n",
            "Train Epoch: 55 [560/1400 (40%)]\tLoss: 136.662979\n",
            "Train Epoch: 55 [600/1400 (43%)]\tLoss: 145.808319\n",
            "Train Epoch: 55 [640/1400 (46%)]\tLoss: 153.989410\n",
            "Train Epoch: 55 [680/1400 (49%)]\tLoss: 131.993973\n",
            "Train Epoch: 55 [720/1400 (51%)]\tLoss: 133.867844\n",
            "Train Epoch: 55 [760/1400 (54%)]\tLoss: 142.477600\n",
            "Train Epoch: 55 [800/1400 (57%)]\tLoss: 153.719879\n",
            "Train Epoch: 55 [840/1400 (60%)]\tLoss: 125.486542\n",
            "Train Epoch: 55 [880/1400 (63%)]\tLoss: 129.562836\n",
            "Train Epoch: 55 [920/1400 (66%)]\tLoss: 128.196167\n",
            "Train Epoch: 55 [960/1400 (69%)]\tLoss: 127.925110\n",
            "Train Epoch: 55 [1000/1400 (71%)]\tLoss: 131.333542\n",
            "Train Epoch: 55 [1040/1400 (74%)]\tLoss: 135.084900\n",
            "Train Epoch: 55 [1080/1400 (77%)]\tLoss: 132.816437\n",
            "Train Epoch: 55 [1120/1400 (80%)]\tLoss: 148.032028\n",
            "Train Epoch: 55 [1160/1400 (83%)]\tLoss: 147.687759\n",
            "Train Epoch: 55 [1200/1400 (86%)]\tLoss: 135.493622\n",
            "Train Epoch: 55 [1240/1400 (89%)]\tLoss: 128.968552\n",
            "Train Epoch: 55 [1280/1400 (91%)]\tLoss: 146.102570\n",
            "Train Epoch: 55 [1320/1400 (94%)]\tLoss: 150.596725\n",
            "Train Epoch: 55 [1360/1400 (97%)]\tLoss: 138.347015\n",
            "====> Epoch: 55 Average loss: 139.3954\n",
            "====> Test set loss: 142.8599\n",
            "Train Epoch: 56 [0/1400 (0%)]\tLoss: 146.449982\n",
            "Train Epoch: 56 [40/1400 (3%)]\tLoss: 124.918922\n",
            "Train Epoch: 56 [80/1400 (6%)]\tLoss: 129.769775\n",
            "Train Epoch: 56 [120/1400 (9%)]\tLoss: 141.783386\n",
            "Train Epoch: 56 [160/1400 (11%)]\tLoss: 132.591003\n",
            "Train Epoch: 56 [200/1400 (14%)]\tLoss: 126.131836\n",
            "Train Epoch: 56 [240/1400 (17%)]\tLoss: 122.829285\n",
            "Train Epoch: 56 [280/1400 (20%)]\tLoss: 142.166275\n",
            "Train Epoch: 56 [320/1400 (23%)]\tLoss: 119.920418\n",
            "Train Epoch: 56 [360/1400 (26%)]\tLoss: 139.453049\n",
            "Train Epoch: 56 [400/1400 (29%)]\tLoss: 140.500839\n",
            "Train Epoch: 56 [440/1400 (31%)]\tLoss: 131.388809\n",
            "Train Epoch: 56 [480/1400 (34%)]\tLoss: 131.692703\n",
            "Train Epoch: 56 [520/1400 (37%)]\tLoss: 131.397919\n",
            "Train Epoch: 56 [560/1400 (40%)]\tLoss: 132.105896\n",
            "Train Epoch: 56 [600/1400 (43%)]\tLoss: 149.397324\n",
            "Train Epoch: 56 [640/1400 (46%)]\tLoss: 108.889008\n",
            "Train Epoch: 56 [680/1400 (49%)]\tLoss: 145.717957\n",
            "Train Epoch: 56 [720/1400 (51%)]\tLoss: 144.191177\n",
            "Train Epoch: 56 [760/1400 (54%)]\tLoss: 144.204697\n",
            "Train Epoch: 56 [800/1400 (57%)]\tLoss: 140.162903\n",
            "Train Epoch: 56 [840/1400 (60%)]\tLoss: 142.666351\n",
            "Train Epoch: 56 [880/1400 (63%)]\tLoss: 132.928833\n",
            "Train Epoch: 56 [920/1400 (66%)]\tLoss: 137.988968\n",
            "Train Epoch: 56 [960/1400 (69%)]\tLoss: 128.273254\n",
            "Train Epoch: 56 [1000/1400 (71%)]\tLoss: 142.830063\n",
            "Train Epoch: 56 [1040/1400 (74%)]\tLoss: 116.538979\n",
            "Train Epoch: 56 [1080/1400 (77%)]\tLoss: 127.373528\n",
            "Train Epoch: 56 [1120/1400 (80%)]\tLoss: 139.830475\n",
            "Train Epoch: 56 [1160/1400 (83%)]\tLoss: 142.430023\n",
            "Train Epoch: 56 [1200/1400 (86%)]\tLoss: 147.000031\n",
            "Train Epoch: 56 [1240/1400 (89%)]\tLoss: 130.325180\n",
            "Train Epoch: 56 [1280/1400 (91%)]\tLoss: 137.918991\n",
            "Train Epoch: 56 [1320/1400 (94%)]\tLoss: 138.153030\n",
            "Train Epoch: 56 [1360/1400 (97%)]\tLoss: 137.072876\n",
            "====> Epoch: 56 Average loss: 139.3912\n",
            "====> Test set loss: 142.5716\n",
            "Train Epoch: 57 [0/1400 (0%)]\tLoss: 136.378815\n",
            "Train Epoch: 57 [40/1400 (3%)]\tLoss: 129.436966\n",
            "Train Epoch: 57 [80/1400 (6%)]\tLoss: 123.163788\n",
            "Train Epoch: 57 [120/1400 (9%)]\tLoss: 136.543533\n",
            "Train Epoch: 57 [160/1400 (11%)]\tLoss: 132.900681\n",
            "Train Epoch: 57 [200/1400 (14%)]\tLoss: 144.597672\n",
            "Train Epoch: 57 [240/1400 (17%)]\tLoss: 100.627121\n",
            "Train Epoch: 57 [280/1400 (20%)]\tLoss: 163.653793\n",
            "Train Epoch: 57 [320/1400 (23%)]\tLoss: 143.712738\n",
            "Train Epoch: 57 [360/1400 (26%)]\tLoss: 135.384048\n",
            "Train Epoch: 57 [400/1400 (29%)]\tLoss: 152.539230\n",
            "Train Epoch: 57 [440/1400 (31%)]\tLoss: 141.365982\n",
            "Train Epoch: 57 [480/1400 (34%)]\tLoss: 146.894363\n",
            "Train Epoch: 57 [520/1400 (37%)]\tLoss: 146.663712\n",
            "Train Epoch: 57 [560/1400 (40%)]\tLoss: 118.797287\n",
            "Train Epoch: 57 [600/1400 (43%)]\tLoss: 136.982361\n",
            "Train Epoch: 57 [640/1400 (46%)]\tLoss: 129.385010\n",
            "Train Epoch: 57 [680/1400 (49%)]\tLoss: 134.400940\n",
            "Train Epoch: 57 [720/1400 (51%)]\tLoss: 132.890945\n",
            "Train Epoch: 57 [760/1400 (54%)]\tLoss: 130.102478\n",
            "Train Epoch: 57 [800/1400 (57%)]\tLoss: 161.226151\n",
            "Train Epoch: 57 [840/1400 (60%)]\tLoss: 151.143982\n",
            "Train Epoch: 57 [880/1400 (63%)]\tLoss: 133.176956\n",
            "Train Epoch: 57 [920/1400 (66%)]\tLoss: 138.145676\n",
            "Train Epoch: 57 [960/1400 (69%)]\tLoss: 150.747833\n",
            "Train Epoch: 57 [1000/1400 (71%)]\tLoss: 153.450623\n",
            "Train Epoch: 57 [1040/1400 (74%)]\tLoss: 145.991974\n",
            "Train Epoch: 57 [1080/1400 (77%)]\tLoss: 140.177460\n",
            "Train Epoch: 57 [1120/1400 (80%)]\tLoss: 131.423203\n",
            "Train Epoch: 57 [1160/1400 (83%)]\tLoss: 119.513031\n",
            "Train Epoch: 57 [1200/1400 (86%)]\tLoss: 145.355759\n",
            "Train Epoch: 57 [1240/1400 (89%)]\tLoss: 136.643143\n",
            "Train Epoch: 57 [1280/1400 (91%)]\tLoss: 145.199692\n",
            "Train Epoch: 57 [1320/1400 (94%)]\tLoss: 127.187286\n",
            "Train Epoch: 57 [1360/1400 (97%)]\tLoss: 138.004410\n",
            "====> Epoch: 57 Average loss: 139.2937\n",
            "====> Test set loss: 142.6121\n",
            "Train Epoch: 58 [0/1400 (0%)]\tLoss: 126.617287\n",
            "Train Epoch: 58 [40/1400 (3%)]\tLoss: 127.057053\n",
            "Train Epoch: 58 [80/1400 (6%)]\tLoss: 138.918198\n",
            "Train Epoch: 58 [120/1400 (9%)]\tLoss: 131.877472\n",
            "Train Epoch: 58 [160/1400 (11%)]\tLoss: 143.196548\n",
            "Train Epoch: 58 [200/1400 (14%)]\tLoss: 120.990479\n",
            "Train Epoch: 58 [240/1400 (17%)]\tLoss: 137.002502\n",
            "Train Epoch: 58 [280/1400 (20%)]\tLoss: 128.644318\n",
            "Train Epoch: 58 [320/1400 (23%)]\tLoss: 130.024384\n",
            "Train Epoch: 58 [360/1400 (26%)]\tLoss: 151.474487\n",
            "Train Epoch: 58 [400/1400 (29%)]\tLoss: 139.629044\n",
            "Train Epoch: 58 [440/1400 (31%)]\tLoss: 129.373001\n",
            "Train Epoch: 58 [480/1400 (34%)]\tLoss: 163.531097\n",
            "Train Epoch: 58 [520/1400 (37%)]\tLoss: 146.349411\n",
            "Train Epoch: 58 [560/1400 (40%)]\tLoss: 130.652542\n",
            "Train Epoch: 58 [600/1400 (43%)]\tLoss: 124.898026\n",
            "Train Epoch: 58 [640/1400 (46%)]\tLoss: 143.261749\n",
            "Train Epoch: 58 [680/1400 (49%)]\tLoss: 128.495285\n",
            "Train Epoch: 58 [720/1400 (51%)]\tLoss: 132.291840\n",
            "Train Epoch: 58 [760/1400 (54%)]\tLoss: 137.514786\n",
            "Train Epoch: 58 [800/1400 (57%)]\tLoss: 150.798080\n",
            "Train Epoch: 58 [840/1400 (60%)]\tLoss: 148.908432\n",
            "Train Epoch: 58 [880/1400 (63%)]\tLoss: 146.162918\n",
            "Train Epoch: 58 [920/1400 (66%)]\tLoss: 123.602234\n",
            "Train Epoch: 58 [960/1400 (69%)]\tLoss: 135.449615\n",
            "Train Epoch: 58 [1000/1400 (71%)]\tLoss: 131.292068\n",
            "Train Epoch: 58 [1040/1400 (74%)]\tLoss: 135.739273\n",
            "Train Epoch: 58 [1080/1400 (77%)]\tLoss: 150.804565\n",
            "Train Epoch: 58 [1120/1400 (80%)]\tLoss: 144.804565\n",
            "Train Epoch: 58 [1160/1400 (83%)]\tLoss: 128.853516\n",
            "Train Epoch: 58 [1200/1400 (86%)]\tLoss: 160.537048\n",
            "Train Epoch: 58 [1240/1400 (89%)]\tLoss: 134.472717\n",
            "Train Epoch: 58 [1280/1400 (91%)]\tLoss: 151.523148\n",
            "Train Epoch: 58 [1320/1400 (94%)]\tLoss: 141.766815\n",
            "Train Epoch: 58 [1360/1400 (97%)]\tLoss: 151.335434\n",
            "====> Epoch: 58 Average loss: 139.1845\n",
            "====> Test set loss: 142.0826\n",
            "Train Epoch: 59 [0/1400 (0%)]\tLoss: 147.075165\n",
            "Train Epoch: 59 [40/1400 (3%)]\tLoss: 153.659973\n",
            "Train Epoch: 59 [80/1400 (6%)]\tLoss: 123.445839\n",
            "Train Epoch: 59 [120/1400 (9%)]\tLoss: 123.458817\n",
            "Train Epoch: 59 [160/1400 (11%)]\tLoss: 144.886078\n",
            "Train Epoch: 59 [200/1400 (14%)]\tLoss: 115.774399\n",
            "Train Epoch: 59 [240/1400 (17%)]\tLoss: 153.915359\n",
            "Train Epoch: 59 [280/1400 (20%)]\tLoss: 145.819107\n",
            "Train Epoch: 59 [320/1400 (23%)]\tLoss: 139.039200\n",
            "Train Epoch: 59 [360/1400 (26%)]\tLoss: 139.007309\n",
            "Train Epoch: 59 [400/1400 (29%)]\tLoss: 139.160995\n",
            "Train Epoch: 59 [440/1400 (31%)]\tLoss: 133.365311\n",
            "Train Epoch: 59 [480/1400 (34%)]\tLoss: 140.763947\n",
            "Train Epoch: 59 [520/1400 (37%)]\tLoss: 141.087860\n",
            "Train Epoch: 59 [560/1400 (40%)]\tLoss: 138.893799\n",
            "Train Epoch: 59 [600/1400 (43%)]\tLoss: 146.109146\n",
            "Train Epoch: 59 [640/1400 (46%)]\tLoss: 128.832870\n",
            "Train Epoch: 59 [680/1400 (49%)]\tLoss: 118.603355\n",
            "Train Epoch: 59 [720/1400 (51%)]\tLoss: 137.603561\n",
            "Train Epoch: 59 [760/1400 (54%)]\tLoss: 131.246780\n",
            "Train Epoch: 59 [800/1400 (57%)]\tLoss: 153.048553\n",
            "Train Epoch: 59 [840/1400 (60%)]\tLoss: 140.872955\n",
            "Train Epoch: 59 [880/1400 (63%)]\tLoss: 166.394836\n",
            "Train Epoch: 59 [920/1400 (66%)]\tLoss: 146.039124\n",
            "Train Epoch: 59 [960/1400 (69%)]\tLoss: 143.659637\n",
            "Train Epoch: 59 [1000/1400 (71%)]\tLoss: 115.922333\n",
            "Train Epoch: 59 [1040/1400 (74%)]\tLoss: 148.313950\n",
            "Train Epoch: 59 [1080/1400 (77%)]\tLoss: 119.922340\n",
            "Train Epoch: 59 [1120/1400 (80%)]\tLoss: 152.244629\n",
            "Train Epoch: 59 [1160/1400 (83%)]\tLoss: 125.096191\n",
            "Train Epoch: 59 [1200/1400 (86%)]\tLoss: 143.361542\n",
            "Train Epoch: 59 [1240/1400 (89%)]\tLoss: 155.075317\n",
            "Train Epoch: 59 [1280/1400 (91%)]\tLoss: 141.790802\n",
            "Train Epoch: 59 [1320/1400 (94%)]\tLoss: 151.067291\n",
            "Train Epoch: 59 [1360/1400 (97%)]\tLoss: 141.038452\n",
            "====> Epoch: 59 Average loss: 139.0233\n",
            "====> Test set loss: 142.6944\n",
            "Train Epoch: 60 [0/1400 (0%)]\tLoss: 129.375366\n",
            "Train Epoch: 60 [40/1400 (3%)]\tLoss: 168.270874\n",
            "Train Epoch: 60 [80/1400 (6%)]\tLoss: 134.286469\n",
            "Train Epoch: 60 [120/1400 (9%)]\tLoss: 151.961761\n",
            "Train Epoch: 60 [160/1400 (11%)]\tLoss: 152.349304\n",
            "Train Epoch: 60 [200/1400 (14%)]\tLoss: 125.424477\n",
            "Train Epoch: 60 [240/1400 (17%)]\tLoss: 107.770119\n",
            "Train Epoch: 60 [280/1400 (20%)]\tLoss: 118.373878\n",
            "Train Epoch: 60 [320/1400 (23%)]\tLoss: 138.252853\n",
            "Train Epoch: 60 [360/1400 (26%)]\tLoss: 131.243927\n",
            "Train Epoch: 60 [400/1400 (29%)]\tLoss: 132.604416\n",
            "Train Epoch: 60 [440/1400 (31%)]\tLoss: 146.343430\n",
            "Train Epoch: 60 [480/1400 (34%)]\tLoss: 122.259270\n",
            "Train Epoch: 60 [520/1400 (37%)]\tLoss: 142.577011\n",
            "Train Epoch: 60 [560/1400 (40%)]\tLoss: 138.093582\n",
            "Train Epoch: 60 [600/1400 (43%)]\tLoss: 170.471252\n",
            "Train Epoch: 60 [640/1400 (46%)]\tLoss: 151.545731\n",
            "Train Epoch: 60 [680/1400 (49%)]\tLoss: 153.219849\n",
            "Train Epoch: 60 [720/1400 (51%)]\tLoss: 136.128052\n",
            "Train Epoch: 60 [760/1400 (54%)]\tLoss: 150.455551\n",
            "Train Epoch: 60 [800/1400 (57%)]\tLoss: 149.069443\n",
            "Train Epoch: 60 [840/1400 (60%)]\tLoss: 146.976913\n",
            "Train Epoch: 60 [880/1400 (63%)]\tLoss: 124.580551\n",
            "Train Epoch: 60 [920/1400 (66%)]\tLoss: 146.262024\n",
            "Train Epoch: 60 [960/1400 (69%)]\tLoss: 150.930130\n",
            "Train Epoch: 60 [1000/1400 (71%)]\tLoss: 137.602066\n",
            "Train Epoch: 60 [1040/1400 (74%)]\tLoss: 132.007431\n",
            "Train Epoch: 60 [1080/1400 (77%)]\tLoss: 129.025223\n",
            "Train Epoch: 60 [1120/1400 (80%)]\tLoss: 142.142365\n",
            "Train Epoch: 60 [1160/1400 (83%)]\tLoss: 149.113419\n",
            "Train Epoch: 60 [1200/1400 (86%)]\tLoss: 116.284103\n",
            "Train Epoch: 60 [1240/1400 (89%)]\tLoss: 146.726654\n",
            "Train Epoch: 60 [1280/1400 (91%)]\tLoss: 118.247467\n",
            "Train Epoch: 60 [1320/1400 (94%)]\tLoss: 148.262100\n",
            "Train Epoch: 60 [1360/1400 (97%)]\tLoss: 140.725983\n",
            "====> Epoch: 60 Average loss: 139.1185\n",
            "====> Test set loss: 142.3399\n",
            "Train Epoch: 61 [0/1400 (0%)]\tLoss: 135.003845\n",
            "Train Epoch: 61 [40/1400 (3%)]\tLoss: 129.762161\n",
            "Train Epoch: 61 [80/1400 (6%)]\tLoss: 136.112137\n",
            "Train Epoch: 61 [120/1400 (9%)]\tLoss: 134.648010\n",
            "Train Epoch: 61 [160/1400 (11%)]\tLoss: 127.771271\n",
            "Train Epoch: 61 [200/1400 (14%)]\tLoss: 150.169098\n",
            "Train Epoch: 61 [240/1400 (17%)]\tLoss: 129.667068\n",
            "Train Epoch: 61 [280/1400 (20%)]\tLoss: 140.607681\n",
            "Train Epoch: 61 [320/1400 (23%)]\tLoss: 143.418671\n",
            "Train Epoch: 61 [360/1400 (26%)]\tLoss: 138.176041\n",
            "Train Epoch: 61 [400/1400 (29%)]\tLoss: 141.830124\n",
            "Train Epoch: 61 [440/1400 (31%)]\tLoss: 145.588776\n",
            "Train Epoch: 61 [480/1400 (34%)]\tLoss: 131.066132\n",
            "Train Epoch: 61 [520/1400 (37%)]\tLoss: 127.940056\n",
            "Train Epoch: 61 [560/1400 (40%)]\tLoss: 132.097153\n",
            "Train Epoch: 61 [600/1400 (43%)]\tLoss: 134.927765\n",
            "Train Epoch: 61 [640/1400 (46%)]\tLoss: 126.665741\n",
            "Train Epoch: 61 [680/1400 (49%)]\tLoss: 139.776535\n",
            "Train Epoch: 61 [720/1400 (51%)]\tLoss: 139.335007\n",
            "Train Epoch: 61 [760/1400 (54%)]\tLoss: 153.078812\n",
            "Train Epoch: 61 [800/1400 (57%)]\tLoss: 150.340881\n",
            "Train Epoch: 61 [840/1400 (60%)]\tLoss: 153.868546\n",
            "Train Epoch: 61 [880/1400 (63%)]\tLoss: 143.251282\n",
            "Train Epoch: 61 [920/1400 (66%)]\tLoss: 148.170364\n",
            "Train Epoch: 61 [960/1400 (69%)]\tLoss: 136.562546\n",
            "Train Epoch: 61 [1000/1400 (71%)]\tLoss: 137.790344\n",
            "Train Epoch: 61 [1040/1400 (74%)]\tLoss: 126.994209\n",
            "Train Epoch: 61 [1080/1400 (77%)]\tLoss: 129.031677\n",
            "Train Epoch: 61 [1120/1400 (80%)]\tLoss: 140.515137\n",
            "Train Epoch: 61 [1160/1400 (83%)]\tLoss: 164.175476\n",
            "Train Epoch: 61 [1200/1400 (86%)]\tLoss: 113.335556\n",
            "Train Epoch: 61 [1240/1400 (89%)]\tLoss: 159.382492\n",
            "Train Epoch: 61 [1280/1400 (91%)]\tLoss: 153.472076\n",
            "Train Epoch: 61 [1320/1400 (94%)]\tLoss: 145.056931\n",
            "Train Epoch: 61 [1360/1400 (97%)]\tLoss: 150.323669\n",
            "====> Epoch: 61 Average loss: 138.8931\n",
            "====> Test set loss: 142.0910\n",
            "Train Epoch: 62 [0/1400 (0%)]\tLoss: 139.928360\n",
            "Train Epoch: 62 [40/1400 (3%)]\tLoss: 130.163818\n",
            "Train Epoch: 62 [80/1400 (6%)]\tLoss: 137.483948\n",
            "Train Epoch: 62 [120/1400 (9%)]\tLoss: 125.193138\n",
            "Train Epoch: 62 [160/1400 (11%)]\tLoss: 140.413879\n",
            "Train Epoch: 62 [200/1400 (14%)]\tLoss: 125.748932\n",
            "Train Epoch: 62 [240/1400 (17%)]\tLoss: 123.001740\n",
            "Train Epoch: 62 [280/1400 (20%)]\tLoss: 126.637169\n",
            "Train Epoch: 62 [320/1400 (23%)]\tLoss: 127.486038\n",
            "Train Epoch: 62 [360/1400 (26%)]\tLoss: 131.067719\n",
            "Train Epoch: 62 [400/1400 (29%)]\tLoss: 139.627869\n",
            "Train Epoch: 62 [440/1400 (31%)]\tLoss: 131.552963\n",
            "Train Epoch: 62 [480/1400 (34%)]\tLoss: 171.425171\n",
            "Train Epoch: 62 [520/1400 (37%)]\tLoss: 138.244980\n",
            "Train Epoch: 62 [560/1400 (40%)]\tLoss: 141.612457\n",
            "Train Epoch: 62 [600/1400 (43%)]\tLoss: 144.015671\n",
            "Train Epoch: 62 [640/1400 (46%)]\tLoss: 162.358841\n",
            "Train Epoch: 62 [680/1400 (49%)]\tLoss: 158.561249\n",
            "Train Epoch: 62 [720/1400 (51%)]\tLoss: 112.052582\n",
            "Train Epoch: 62 [760/1400 (54%)]\tLoss: 114.725250\n",
            "Train Epoch: 62 [800/1400 (57%)]\tLoss: 138.090714\n",
            "Train Epoch: 62 [840/1400 (60%)]\tLoss: 136.142075\n",
            "Train Epoch: 62 [880/1400 (63%)]\tLoss: 145.390472\n",
            "Train Epoch: 62 [920/1400 (66%)]\tLoss: 135.819077\n",
            "Train Epoch: 62 [960/1400 (69%)]\tLoss: 145.581375\n",
            "Train Epoch: 62 [1000/1400 (71%)]\tLoss: 120.322319\n",
            "Train Epoch: 62 [1040/1400 (74%)]\tLoss: 139.357483\n",
            "Train Epoch: 62 [1080/1400 (77%)]\tLoss: 126.146378\n",
            "Train Epoch: 62 [1120/1400 (80%)]\tLoss: 125.347755\n",
            "Train Epoch: 62 [1160/1400 (83%)]\tLoss: 154.982391\n",
            "Train Epoch: 62 [1200/1400 (86%)]\tLoss: 155.228729\n",
            "Train Epoch: 62 [1240/1400 (89%)]\tLoss: 162.001541\n",
            "Train Epoch: 62 [1280/1400 (91%)]\tLoss: 151.167709\n",
            "Train Epoch: 62 [1320/1400 (94%)]\tLoss: 136.466614\n",
            "Train Epoch: 62 [1360/1400 (97%)]\tLoss: 139.415192\n",
            "====> Epoch: 62 Average loss: 138.8450\n",
            "====> Test set loss: 142.3006\n",
            "Train Epoch: 63 [0/1400 (0%)]\tLoss: 146.113922\n",
            "Train Epoch: 63 [40/1400 (3%)]\tLoss: 151.318314\n",
            "Train Epoch: 63 [80/1400 (6%)]\tLoss: 137.302780\n",
            "Train Epoch: 63 [120/1400 (9%)]\tLoss: 169.478394\n",
            "Train Epoch: 63 [160/1400 (11%)]\tLoss: 133.756241\n",
            "Train Epoch: 63 [200/1400 (14%)]\tLoss: 139.468750\n",
            "Train Epoch: 63 [240/1400 (17%)]\tLoss: 153.952515\n",
            "Train Epoch: 63 [280/1400 (20%)]\tLoss: 121.406891\n",
            "Train Epoch: 63 [320/1400 (23%)]\tLoss: 136.463333\n",
            "Train Epoch: 63 [360/1400 (26%)]\tLoss: 144.955353\n",
            "Train Epoch: 63 [400/1400 (29%)]\tLoss: 122.197289\n",
            "Train Epoch: 63 [440/1400 (31%)]\tLoss: 138.247589\n",
            "Train Epoch: 63 [480/1400 (34%)]\tLoss: 122.488419\n",
            "Train Epoch: 63 [520/1400 (37%)]\tLoss: 148.898483\n",
            "Train Epoch: 63 [560/1400 (40%)]\tLoss: 178.639236\n",
            "Train Epoch: 63 [600/1400 (43%)]\tLoss: 144.094269\n",
            "Train Epoch: 63 [640/1400 (46%)]\tLoss: 132.345612\n",
            "Train Epoch: 63 [680/1400 (49%)]\tLoss: 132.173431\n",
            "Train Epoch: 63 [720/1400 (51%)]\tLoss: 151.815109\n",
            "Train Epoch: 63 [760/1400 (54%)]\tLoss: 139.635620\n",
            "Train Epoch: 63 [800/1400 (57%)]\tLoss: 141.199677\n",
            "Train Epoch: 63 [840/1400 (60%)]\tLoss: 143.000107\n",
            "Train Epoch: 63 [880/1400 (63%)]\tLoss: 132.878342\n",
            "Train Epoch: 63 [920/1400 (66%)]\tLoss: 156.811386\n",
            "Train Epoch: 63 [960/1400 (69%)]\tLoss: 109.890121\n",
            "Train Epoch: 63 [1000/1400 (71%)]\tLoss: 121.015129\n",
            "Train Epoch: 63 [1040/1400 (74%)]\tLoss: 124.842628\n",
            "Train Epoch: 63 [1080/1400 (77%)]\tLoss: 144.365753\n",
            "Train Epoch: 63 [1120/1400 (80%)]\tLoss: 143.007843\n",
            "Train Epoch: 63 [1160/1400 (83%)]\tLoss: 129.995514\n",
            "Train Epoch: 63 [1200/1400 (86%)]\tLoss: 152.579193\n",
            "Train Epoch: 63 [1240/1400 (89%)]\tLoss: 139.556427\n",
            "Train Epoch: 63 [1280/1400 (91%)]\tLoss: 148.834824\n",
            "Train Epoch: 63 [1320/1400 (94%)]\tLoss: 148.995972\n",
            "Train Epoch: 63 [1360/1400 (97%)]\tLoss: 164.161850\n",
            "====> Epoch: 63 Average loss: 138.8262\n",
            "====> Test set loss: 142.0861\n",
            "Train Epoch: 64 [0/1400 (0%)]\tLoss: 150.568863\n",
            "Train Epoch: 64 [40/1400 (3%)]\tLoss: 131.259857\n",
            "Train Epoch: 64 [80/1400 (6%)]\tLoss: 146.000504\n",
            "Train Epoch: 64 [120/1400 (9%)]\tLoss: 157.759460\n",
            "Train Epoch: 64 [160/1400 (11%)]\tLoss: 144.810257\n",
            "Train Epoch: 64 [200/1400 (14%)]\tLoss: 141.730988\n",
            "Train Epoch: 64 [240/1400 (17%)]\tLoss: 150.646179\n",
            "Train Epoch: 64 [280/1400 (20%)]\tLoss: 137.829529\n",
            "Train Epoch: 64 [320/1400 (23%)]\tLoss: 132.626953\n",
            "Train Epoch: 64 [360/1400 (26%)]\tLoss: 148.827637\n",
            "Train Epoch: 64 [400/1400 (29%)]\tLoss: 137.639847\n",
            "Train Epoch: 64 [440/1400 (31%)]\tLoss: 143.885788\n",
            "Train Epoch: 64 [480/1400 (34%)]\tLoss: 136.249146\n",
            "Train Epoch: 64 [520/1400 (37%)]\tLoss: 151.771637\n",
            "Train Epoch: 64 [560/1400 (40%)]\tLoss: 133.847137\n",
            "Train Epoch: 64 [600/1400 (43%)]\tLoss: 132.641724\n",
            "Train Epoch: 64 [640/1400 (46%)]\tLoss: 124.099487\n",
            "Train Epoch: 64 [680/1400 (49%)]\tLoss: 134.937180\n",
            "Train Epoch: 64 [720/1400 (51%)]\tLoss: 123.417526\n",
            "Train Epoch: 64 [760/1400 (54%)]\tLoss: 143.622726\n",
            "Train Epoch: 64 [800/1400 (57%)]\tLoss: 138.040588\n",
            "Train Epoch: 64 [840/1400 (60%)]\tLoss: 151.289627\n",
            "Train Epoch: 64 [880/1400 (63%)]\tLoss: 142.238388\n",
            "Train Epoch: 64 [920/1400 (66%)]\tLoss: 135.231827\n",
            "Train Epoch: 64 [960/1400 (69%)]\tLoss: 131.687561\n",
            "Train Epoch: 64 [1000/1400 (71%)]\tLoss: 137.794174\n",
            "Train Epoch: 64 [1040/1400 (74%)]\tLoss: 134.273407\n",
            "Train Epoch: 64 [1080/1400 (77%)]\tLoss: 143.489349\n",
            "Train Epoch: 64 [1120/1400 (80%)]\tLoss: 135.558807\n",
            "Train Epoch: 64 [1160/1400 (83%)]\tLoss: 136.994751\n",
            "Train Epoch: 64 [1200/1400 (86%)]\tLoss: 152.783875\n",
            "Train Epoch: 64 [1240/1400 (89%)]\tLoss: 130.399307\n",
            "Train Epoch: 64 [1280/1400 (91%)]\tLoss: 136.982803\n",
            "Train Epoch: 64 [1320/1400 (94%)]\tLoss: 139.332367\n",
            "Train Epoch: 64 [1360/1400 (97%)]\tLoss: 152.113525\n",
            "====> Epoch: 64 Average loss: 138.7000\n",
            "====> Test set loss: 142.0000\n",
            "Train Epoch: 65 [0/1400 (0%)]\tLoss: 141.784225\n",
            "Train Epoch: 65 [40/1400 (3%)]\tLoss: 123.981796\n",
            "Train Epoch: 65 [80/1400 (6%)]\tLoss: 140.088562\n",
            "Train Epoch: 65 [120/1400 (9%)]\tLoss: 125.724876\n",
            "Train Epoch: 65 [160/1400 (11%)]\tLoss: 127.870987\n",
            "Train Epoch: 65 [200/1400 (14%)]\tLoss: 131.444016\n",
            "Train Epoch: 65 [240/1400 (17%)]\tLoss: 134.623245\n",
            "Train Epoch: 65 [280/1400 (20%)]\tLoss: 147.173737\n",
            "Train Epoch: 65 [320/1400 (23%)]\tLoss: 128.336960\n",
            "Train Epoch: 65 [360/1400 (26%)]\tLoss: 145.184296\n",
            "Train Epoch: 65 [400/1400 (29%)]\tLoss: 146.297897\n",
            "Train Epoch: 65 [440/1400 (31%)]\tLoss: 158.024551\n",
            "Train Epoch: 65 [480/1400 (34%)]\tLoss: 134.684784\n",
            "Train Epoch: 65 [520/1400 (37%)]\tLoss: 137.047226\n",
            "Train Epoch: 65 [560/1400 (40%)]\tLoss: 149.352509\n",
            "Train Epoch: 65 [600/1400 (43%)]\tLoss: 156.262344\n",
            "Train Epoch: 65 [640/1400 (46%)]\tLoss: 153.849121\n",
            "Train Epoch: 65 [680/1400 (49%)]\tLoss: 138.171478\n",
            "Train Epoch: 65 [720/1400 (51%)]\tLoss: 136.293167\n",
            "Train Epoch: 65 [760/1400 (54%)]\tLoss: 142.014587\n",
            "Train Epoch: 65 [800/1400 (57%)]\tLoss: 137.533279\n",
            "Train Epoch: 65 [840/1400 (60%)]\tLoss: 152.223740\n",
            "Train Epoch: 65 [880/1400 (63%)]\tLoss: 127.761177\n",
            "Train Epoch: 65 [920/1400 (66%)]\tLoss: 135.745453\n",
            "Train Epoch: 65 [960/1400 (69%)]\tLoss: 138.835510\n",
            "Train Epoch: 65 [1000/1400 (71%)]\tLoss: 136.218933\n",
            "Train Epoch: 65 [1040/1400 (74%)]\tLoss: 139.760971\n",
            "Train Epoch: 65 [1080/1400 (77%)]\tLoss: 154.725006\n",
            "Train Epoch: 65 [1120/1400 (80%)]\tLoss: 145.304047\n",
            "Train Epoch: 65 [1160/1400 (83%)]\tLoss: 141.334564\n",
            "Train Epoch: 65 [1200/1400 (86%)]\tLoss: 127.068039\n",
            "Train Epoch: 65 [1240/1400 (89%)]\tLoss: 129.440140\n",
            "Train Epoch: 65 [1280/1400 (91%)]\tLoss: 119.172775\n",
            "Train Epoch: 65 [1320/1400 (94%)]\tLoss: 141.491333\n",
            "Train Epoch: 65 [1360/1400 (97%)]\tLoss: 157.050797\n",
            "====> Epoch: 65 Average loss: 138.6964\n",
            "====> Test set loss: 141.7834\n",
            "Train Epoch: 66 [0/1400 (0%)]\tLoss: 149.510559\n",
            "Train Epoch: 66 [40/1400 (3%)]\tLoss: 118.046967\n",
            "Train Epoch: 66 [80/1400 (6%)]\tLoss: 138.375427\n",
            "Train Epoch: 66 [120/1400 (9%)]\tLoss: 118.821953\n",
            "Train Epoch: 66 [160/1400 (11%)]\tLoss: 159.305466\n",
            "Train Epoch: 66 [200/1400 (14%)]\tLoss: 139.031387\n",
            "Train Epoch: 66 [240/1400 (17%)]\tLoss: 148.336243\n",
            "Train Epoch: 66 [280/1400 (20%)]\tLoss: 133.433380\n",
            "Train Epoch: 66 [320/1400 (23%)]\tLoss: 163.349365\n",
            "Train Epoch: 66 [360/1400 (26%)]\tLoss: 151.612122\n",
            "Train Epoch: 66 [400/1400 (29%)]\tLoss: 135.736588\n",
            "Train Epoch: 66 [440/1400 (31%)]\tLoss: 156.735931\n",
            "Train Epoch: 66 [480/1400 (34%)]\tLoss: 146.370117\n",
            "Train Epoch: 66 [520/1400 (37%)]\tLoss: 122.253433\n",
            "Train Epoch: 66 [560/1400 (40%)]\tLoss: 142.576614\n",
            "Train Epoch: 66 [600/1400 (43%)]\tLoss: 139.996353\n",
            "Train Epoch: 66 [640/1400 (46%)]\tLoss: 132.807388\n",
            "Train Epoch: 66 [680/1400 (49%)]\tLoss: 124.290466\n",
            "Train Epoch: 66 [720/1400 (51%)]\tLoss: 134.810547\n",
            "Train Epoch: 66 [760/1400 (54%)]\tLoss: 143.975677\n",
            "Train Epoch: 66 [800/1400 (57%)]\tLoss: 139.002472\n",
            "Train Epoch: 66 [840/1400 (60%)]\tLoss: 138.171509\n",
            "Train Epoch: 66 [880/1400 (63%)]\tLoss: 139.353760\n",
            "Train Epoch: 66 [920/1400 (66%)]\tLoss: 128.763824\n",
            "Train Epoch: 66 [960/1400 (69%)]\tLoss: 141.622849\n",
            "Train Epoch: 66 [1000/1400 (71%)]\tLoss: 149.621689\n",
            "Train Epoch: 66 [1040/1400 (74%)]\tLoss: 137.661987\n",
            "Train Epoch: 66 [1080/1400 (77%)]\tLoss: 147.762772\n",
            "Train Epoch: 66 [1120/1400 (80%)]\tLoss: 140.907501\n",
            "Train Epoch: 66 [1160/1400 (83%)]\tLoss: 132.174225\n",
            "Train Epoch: 66 [1200/1400 (86%)]\tLoss: 152.207977\n",
            "Train Epoch: 66 [1240/1400 (89%)]\tLoss: 132.458893\n",
            "Train Epoch: 66 [1280/1400 (91%)]\tLoss: 138.467346\n",
            "Train Epoch: 66 [1320/1400 (94%)]\tLoss: 127.085976\n",
            "Train Epoch: 66 [1360/1400 (97%)]\tLoss: 160.342377\n",
            "====> Epoch: 66 Average loss: 138.6442\n",
            "====> Test set loss: 141.8420\n",
            "Train Epoch: 67 [0/1400 (0%)]\tLoss: 140.530319\n",
            "Train Epoch: 67 [40/1400 (3%)]\tLoss: 117.581573\n",
            "Train Epoch: 67 [80/1400 (6%)]\tLoss: 158.240417\n",
            "Train Epoch: 67 [120/1400 (9%)]\tLoss: 121.066551\n",
            "Train Epoch: 67 [160/1400 (11%)]\tLoss: 163.148758\n",
            "Train Epoch: 67 [200/1400 (14%)]\tLoss: 132.424545\n",
            "Train Epoch: 67 [240/1400 (17%)]\tLoss: 147.141174\n",
            "Train Epoch: 67 [280/1400 (20%)]\tLoss: 132.093414\n",
            "Train Epoch: 67 [320/1400 (23%)]\tLoss: 136.053513\n",
            "Train Epoch: 67 [360/1400 (26%)]\tLoss: 146.314560\n",
            "Train Epoch: 67 [400/1400 (29%)]\tLoss: 136.292786\n",
            "Train Epoch: 67 [440/1400 (31%)]\tLoss: 124.250526\n",
            "Train Epoch: 67 [480/1400 (34%)]\tLoss: 143.790482\n",
            "Train Epoch: 67 [520/1400 (37%)]\tLoss: 139.982803\n",
            "Train Epoch: 67 [560/1400 (40%)]\tLoss: 129.102997\n",
            "Train Epoch: 67 [600/1400 (43%)]\tLoss: 125.807930\n",
            "Train Epoch: 67 [640/1400 (46%)]\tLoss: 157.064545\n",
            "Train Epoch: 67 [680/1400 (49%)]\tLoss: 135.968307\n",
            "Train Epoch: 67 [720/1400 (51%)]\tLoss: 136.091705\n",
            "Train Epoch: 67 [760/1400 (54%)]\tLoss: 139.872391\n",
            "Train Epoch: 67 [800/1400 (57%)]\tLoss: 154.953445\n",
            "Train Epoch: 67 [840/1400 (60%)]\tLoss: 141.940552\n",
            "Train Epoch: 67 [880/1400 (63%)]\tLoss: 124.313354\n",
            "Train Epoch: 67 [920/1400 (66%)]\tLoss: 153.227188\n",
            "Train Epoch: 67 [960/1400 (69%)]\tLoss: 131.108032\n",
            "Train Epoch: 67 [1000/1400 (71%)]\tLoss: 144.473053\n",
            "Train Epoch: 67 [1040/1400 (74%)]\tLoss: 134.155991\n",
            "Train Epoch: 67 [1080/1400 (77%)]\tLoss: 151.747330\n",
            "Train Epoch: 67 [1120/1400 (80%)]\tLoss: 135.069962\n",
            "Train Epoch: 67 [1160/1400 (83%)]\tLoss: 132.955353\n",
            "Train Epoch: 67 [1200/1400 (86%)]\tLoss: 144.884598\n",
            "Train Epoch: 67 [1240/1400 (89%)]\tLoss: 146.764191\n",
            "Train Epoch: 67 [1280/1400 (91%)]\tLoss: 151.614838\n",
            "Train Epoch: 67 [1320/1400 (94%)]\tLoss: 134.647446\n",
            "Train Epoch: 67 [1360/1400 (97%)]\tLoss: 148.533997\n",
            "====> Epoch: 67 Average loss: 138.5305\n",
            "====> Test set loss: 142.0113\n",
            "Train Epoch: 68 [0/1400 (0%)]\tLoss: 158.841156\n",
            "Train Epoch: 68 [40/1400 (3%)]\tLoss: 149.732101\n",
            "Train Epoch: 68 [80/1400 (6%)]\tLoss: 133.168930\n",
            "Train Epoch: 68 [120/1400 (9%)]\tLoss: 129.648972\n",
            "Train Epoch: 68 [160/1400 (11%)]\tLoss: 141.833359\n",
            "Train Epoch: 68 [200/1400 (14%)]\tLoss: 138.417999\n",
            "Train Epoch: 68 [240/1400 (17%)]\tLoss: 144.159409\n",
            "Train Epoch: 68 [280/1400 (20%)]\tLoss: 128.920868\n",
            "Train Epoch: 68 [320/1400 (23%)]\tLoss: 154.087173\n",
            "Train Epoch: 68 [360/1400 (26%)]\tLoss: 144.225815\n",
            "Train Epoch: 68 [400/1400 (29%)]\tLoss: 148.859909\n",
            "Train Epoch: 68 [440/1400 (31%)]\tLoss: 131.768326\n",
            "Train Epoch: 68 [480/1400 (34%)]\tLoss: 121.131332\n",
            "Train Epoch: 68 [520/1400 (37%)]\tLoss: 145.178116\n",
            "Train Epoch: 68 [560/1400 (40%)]\tLoss: 109.797981\n",
            "Train Epoch: 68 [600/1400 (43%)]\tLoss: 139.207397\n",
            "Train Epoch: 68 [640/1400 (46%)]\tLoss: 147.435547\n",
            "Train Epoch: 68 [680/1400 (49%)]\tLoss: 133.624023\n",
            "Train Epoch: 68 [720/1400 (51%)]\tLoss: 122.176071\n",
            "Train Epoch: 68 [760/1400 (54%)]\tLoss: 146.766800\n",
            "Train Epoch: 68 [800/1400 (57%)]\tLoss: 141.696487\n",
            "Train Epoch: 68 [840/1400 (60%)]\tLoss: 128.924545\n",
            "Train Epoch: 68 [880/1400 (63%)]\tLoss: 118.201683\n",
            "Train Epoch: 68 [920/1400 (66%)]\tLoss: 151.469284\n",
            "Train Epoch: 68 [960/1400 (69%)]\tLoss: 135.538742\n",
            "Train Epoch: 68 [1000/1400 (71%)]\tLoss: 146.877762\n",
            "Train Epoch: 68 [1040/1400 (74%)]\tLoss: 131.659607\n",
            "Train Epoch: 68 [1080/1400 (77%)]\tLoss: 136.613907\n",
            "Train Epoch: 68 [1120/1400 (80%)]\tLoss: 132.348312\n",
            "Train Epoch: 68 [1160/1400 (83%)]\tLoss: 158.578537\n",
            "Train Epoch: 68 [1200/1400 (86%)]\tLoss: 127.652740\n",
            "Train Epoch: 68 [1240/1400 (89%)]\tLoss: 129.116760\n",
            "Train Epoch: 68 [1280/1400 (91%)]\tLoss: 138.568588\n",
            "Train Epoch: 68 [1320/1400 (94%)]\tLoss: 142.708908\n",
            "Train Epoch: 68 [1360/1400 (97%)]\tLoss: 139.719833\n",
            "====> Epoch: 68 Average loss: 138.5429\n",
            "====> Test set loss: 142.1238\n",
            "Train Epoch: 69 [0/1400 (0%)]\tLoss: 142.408981\n",
            "Train Epoch: 69 [40/1400 (3%)]\tLoss: 133.660660\n",
            "Train Epoch: 69 [80/1400 (6%)]\tLoss: 139.212463\n",
            "Train Epoch: 69 [120/1400 (9%)]\tLoss: 136.646210\n",
            "Train Epoch: 69 [160/1400 (11%)]\tLoss: 147.194092\n",
            "Train Epoch: 69 [200/1400 (14%)]\tLoss: 146.575165\n",
            "Train Epoch: 69 [240/1400 (17%)]\tLoss: 139.100510\n",
            "Train Epoch: 69 [280/1400 (20%)]\tLoss: 129.589294\n",
            "Train Epoch: 69 [320/1400 (23%)]\tLoss: 141.227921\n",
            "Train Epoch: 69 [360/1400 (26%)]\tLoss: 141.509369\n",
            "Train Epoch: 69 [400/1400 (29%)]\tLoss: 132.755768\n",
            "Train Epoch: 69 [440/1400 (31%)]\tLoss: 110.820251\n",
            "Train Epoch: 69 [480/1400 (34%)]\tLoss: 118.229591\n",
            "Train Epoch: 69 [520/1400 (37%)]\tLoss: 142.505966\n",
            "Train Epoch: 69 [560/1400 (40%)]\tLoss: 131.158386\n",
            "Train Epoch: 69 [600/1400 (43%)]\tLoss: 136.237534\n",
            "Train Epoch: 69 [640/1400 (46%)]\tLoss: 155.752106\n",
            "Train Epoch: 69 [680/1400 (49%)]\tLoss: 143.541733\n",
            "Train Epoch: 69 [720/1400 (51%)]\tLoss: 134.718323\n",
            "Train Epoch: 69 [760/1400 (54%)]\tLoss: 144.711563\n",
            "Train Epoch: 69 [800/1400 (57%)]\tLoss: 137.088562\n",
            "Train Epoch: 69 [840/1400 (60%)]\tLoss: 136.463501\n",
            "Train Epoch: 69 [880/1400 (63%)]\tLoss: 142.608902\n",
            "Train Epoch: 69 [920/1400 (66%)]\tLoss: 152.773422\n",
            "Train Epoch: 69 [960/1400 (69%)]\tLoss: 136.494431\n",
            "Train Epoch: 69 [1000/1400 (71%)]\tLoss: 130.635529\n",
            "Train Epoch: 69 [1040/1400 (74%)]\tLoss: 164.198486\n",
            "Train Epoch: 69 [1080/1400 (77%)]\tLoss: 131.551498\n",
            "Train Epoch: 69 [1120/1400 (80%)]\tLoss: 132.177979\n",
            "Train Epoch: 69 [1160/1400 (83%)]\tLoss: 137.012482\n",
            "Train Epoch: 69 [1200/1400 (86%)]\tLoss: 130.798386\n",
            "Train Epoch: 69 [1240/1400 (89%)]\tLoss: 137.418747\n",
            "Train Epoch: 69 [1280/1400 (91%)]\tLoss: 136.518951\n",
            "Train Epoch: 69 [1320/1400 (94%)]\tLoss: 135.710770\n",
            "Train Epoch: 69 [1360/1400 (97%)]\tLoss: 134.218491\n",
            "====> Epoch: 69 Average loss: 138.3540\n",
            "====> Test set loss: 141.9563\n",
            "Train Epoch: 70 [0/1400 (0%)]\tLoss: 129.449158\n",
            "Train Epoch: 70 [40/1400 (3%)]\tLoss: 136.327744\n",
            "Train Epoch: 70 [80/1400 (6%)]\tLoss: 140.500092\n",
            "Train Epoch: 70 [120/1400 (9%)]\tLoss: 127.338318\n",
            "Train Epoch: 70 [160/1400 (11%)]\tLoss: 149.318634\n",
            "Train Epoch: 70 [200/1400 (14%)]\tLoss: 148.844467\n",
            "Train Epoch: 70 [240/1400 (17%)]\tLoss: 140.556427\n",
            "Train Epoch: 70 [280/1400 (20%)]\tLoss: 141.278519\n",
            "Train Epoch: 70 [320/1400 (23%)]\tLoss: 136.301880\n",
            "Train Epoch: 70 [360/1400 (26%)]\tLoss: 136.383728\n",
            "Train Epoch: 70 [400/1400 (29%)]\tLoss: 126.643677\n",
            "Train Epoch: 70 [440/1400 (31%)]\tLoss: 130.352921\n",
            "Train Epoch: 70 [480/1400 (34%)]\tLoss: 144.521759\n",
            "Train Epoch: 70 [520/1400 (37%)]\tLoss: 144.618576\n",
            "Train Epoch: 70 [560/1400 (40%)]\tLoss: 143.379517\n",
            "Train Epoch: 70 [600/1400 (43%)]\tLoss: 131.145142\n",
            "Train Epoch: 70 [640/1400 (46%)]\tLoss: 135.685135\n",
            "Train Epoch: 70 [680/1400 (49%)]\tLoss: 140.562851\n",
            "Train Epoch: 70 [720/1400 (51%)]\tLoss: 121.582581\n",
            "Train Epoch: 70 [760/1400 (54%)]\tLoss: 130.472244\n",
            "Train Epoch: 70 [800/1400 (57%)]\tLoss: 118.601997\n",
            "Train Epoch: 70 [840/1400 (60%)]\tLoss: 130.071014\n",
            "Train Epoch: 70 [880/1400 (63%)]\tLoss: 157.855072\n",
            "Train Epoch: 70 [920/1400 (66%)]\tLoss: 122.575050\n",
            "Train Epoch: 70 [960/1400 (69%)]\tLoss: 127.994408\n",
            "Train Epoch: 70 [1000/1400 (71%)]\tLoss: 145.486176\n",
            "Train Epoch: 70 [1040/1400 (74%)]\tLoss: 162.719086\n",
            "Train Epoch: 70 [1080/1400 (77%)]\tLoss: 132.575455\n",
            "Train Epoch: 70 [1120/1400 (80%)]\tLoss: 129.146378\n",
            "Train Epoch: 70 [1160/1400 (83%)]\tLoss: 143.850021\n",
            "Train Epoch: 70 [1200/1400 (86%)]\tLoss: 123.512161\n",
            "Train Epoch: 70 [1240/1400 (89%)]\tLoss: 140.286530\n",
            "Train Epoch: 70 [1280/1400 (91%)]\tLoss: 128.605881\n",
            "Train Epoch: 70 [1320/1400 (94%)]\tLoss: 148.345886\n",
            "Train Epoch: 70 [1360/1400 (97%)]\tLoss: 152.845795\n",
            "====> Epoch: 70 Average loss: 138.3823\n",
            "====> Test set loss: 141.7500\n",
            "Train Epoch: 71 [0/1400 (0%)]\tLoss: 122.375275\n",
            "Train Epoch: 71 [40/1400 (3%)]\tLoss: 149.597885\n",
            "Train Epoch: 71 [80/1400 (6%)]\tLoss: 119.366547\n",
            "Train Epoch: 71 [120/1400 (9%)]\tLoss: 127.410927\n",
            "Train Epoch: 71 [160/1400 (11%)]\tLoss: 126.101295\n",
            "Train Epoch: 71 [200/1400 (14%)]\tLoss: 146.009460\n",
            "Train Epoch: 71 [240/1400 (17%)]\tLoss: 151.064178\n",
            "Train Epoch: 71 [280/1400 (20%)]\tLoss: 123.599586\n",
            "Train Epoch: 71 [320/1400 (23%)]\tLoss: 139.237564\n",
            "Train Epoch: 71 [360/1400 (26%)]\tLoss: 124.786659\n",
            "Train Epoch: 71 [400/1400 (29%)]\tLoss: 132.189789\n",
            "Train Epoch: 71 [440/1400 (31%)]\tLoss: 135.402237\n",
            "Train Epoch: 71 [480/1400 (34%)]\tLoss: 126.111496\n",
            "Train Epoch: 71 [520/1400 (37%)]\tLoss: 161.463318\n",
            "Train Epoch: 71 [560/1400 (40%)]\tLoss: 144.185532\n",
            "Train Epoch: 71 [600/1400 (43%)]\tLoss: 143.189529\n",
            "Train Epoch: 71 [640/1400 (46%)]\tLoss: 145.853867\n",
            "Train Epoch: 71 [680/1400 (49%)]\tLoss: 148.988480\n",
            "Train Epoch: 71 [720/1400 (51%)]\tLoss: 158.937088\n",
            "Train Epoch: 71 [760/1400 (54%)]\tLoss: 126.052139\n",
            "Train Epoch: 71 [800/1400 (57%)]\tLoss: 138.984665\n",
            "Train Epoch: 71 [840/1400 (60%)]\tLoss: 151.166916\n",
            "Train Epoch: 71 [880/1400 (63%)]\tLoss: 125.294418\n",
            "Train Epoch: 71 [920/1400 (66%)]\tLoss: 147.669296\n",
            "Train Epoch: 71 [960/1400 (69%)]\tLoss: 128.781479\n",
            "Train Epoch: 71 [1000/1400 (71%)]\tLoss: 148.099625\n",
            "Train Epoch: 71 [1040/1400 (74%)]\tLoss: 119.006737\n",
            "Train Epoch: 71 [1080/1400 (77%)]\tLoss: 144.998535\n",
            "Train Epoch: 71 [1120/1400 (80%)]\tLoss: 119.203186\n",
            "Train Epoch: 71 [1160/1400 (83%)]\tLoss: 135.718246\n",
            "Train Epoch: 71 [1200/1400 (86%)]\tLoss: 156.288879\n",
            "Train Epoch: 71 [1240/1400 (89%)]\tLoss: 146.707932\n",
            "Train Epoch: 71 [1280/1400 (91%)]\tLoss: 134.054779\n",
            "Train Epoch: 71 [1320/1400 (94%)]\tLoss: 121.045624\n",
            "Train Epoch: 71 [1360/1400 (97%)]\tLoss: 132.107925\n",
            "====> Epoch: 71 Average loss: 138.3367\n",
            "====> Test set loss: 141.6538\n",
            "Train Epoch: 72 [0/1400 (0%)]\tLoss: 146.668915\n",
            "Train Epoch: 72 [40/1400 (3%)]\tLoss: 134.559433\n",
            "Train Epoch: 72 [80/1400 (6%)]\tLoss: 128.935226\n",
            "Train Epoch: 72 [120/1400 (9%)]\tLoss: 154.641296\n",
            "Train Epoch: 72 [160/1400 (11%)]\tLoss: 134.747742\n",
            "Train Epoch: 72 [200/1400 (14%)]\tLoss: 129.583588\n",
            "Train Epoch: 72 [240/1400 (17%)]\tLoss: 131.934097\n",
            "Train Epoch: 72 [280/1400 (20%)]\tLoss: 132.756012\n",
            "Train Epoch: 72 [320/1400 (23%)]\tLoss: 138.831985\n",
            "Train Epoch: 72 [360/1400 (26%)]\tLoss: 160.248764\n",
            "Train Epoch: 72 [400/1400 (29%)]\tLoss: 152.025024\n",
            "Train Epoch: 72 [440/1400 (31%)]\tLoss: 145.449066\n",
            "Train Epoch: 72 [480/1400 (34%)]\tLoss: 158.505615\n",
            "Train Epoch: 72 [520/1400 (37%)]\tLoss: 116.424484\n",
            "Train Epoch: 72 [560/1400 (40%)]\tLoss: 132.682358\n",
            "Train Epoch: 72 [600/1400 (43%)]\tLoss: 156.175949\n",
            "Train Epoch: 72 [640/1400 (46%)]\tLoss: 139.552658\n",
            "Train Epoch: 72 [680/1400 (49%)]\tLoss: 125.406395\n",
            "Train Epoch: 72 [720/1400 (51%)]\tLoss: 135.554581\n",
            "Train Epoch: 72 [760/1400 (54%)]\tLoss: 158.400040\n",
            "Train Epoch: 72 [800/1400 (57%)]\tLoss: 150.238800\n",
            "Train Epoch: 72 [840/1400 (60%)]\tLoss: 135.997925\n",
            "Train Epoch: 72 [880/1400 (63%)]\tLoss: 143.569244\n",
            "Train Epoch: 72 [920/1400 (66%)]\tLoss: 140.069244\n",
            "Train Epoch: 72 [960/1400 (69%)]\tLoss: 123.688560\n",
            "Train Epoch: 72 [1000/1400 (71%)]\tLoss: 160.766113\n",
            "Train Epoch: 72 [1040/1400 (74%)]\tLoss: 132.130066\n",
            "Train Epoch: 72 [1080/1400 (77%)]\tLoss: 123.305328\n",
            "Train Epoch: 72 [1120/1400 (80%)]\tLoss: 135.837936\n",
            "Train Epoch: 72 [1160/1400 (83%)]\tLoss: 117.935226\n",
            "Train Epoch: 72 [1200/1400 (86%)]\tLoss: 119.687370\n",
            "Train Epoch: 72 [1240/1400 (89%)]\tLoss: 124.981003\n",
            "Train Epoch: 72 [1280/1400 (91%)]\tLoss: 156.561722\n",
            "Train Epoch: 72 [1320/1400 (94%)]\tLoss: 140.892853\n",
            "Train Epoch: 72 [1360/1400 (97%)]\tLoss: 119.599380\n",
            "====> Epoch: 72 Average loss: 138.2846\n",
            "====> Test set loss: 141.5867\n",
            "Train Epoch: 73 [0/1400 (0%)]\tLoss: 158.962112\n",
            "Train Epoch: 73 [40/1400 (3%)]\tLoss: 136.136597\n",
            "Train Epoch: 73 [80/1400 (6%)]\tLoss: 140.514801\n",
            "Train Epoch: 73 [120/1400 (9%)]\tLoss: 143.262604\n",
            "Train Epoch: 73 [160/1400 (11%)]\tLoss: 112.482216\n",
            "Train Epoch: 73 [200/1400 (14%)]\tLoss: 137.497360\n",
            "Train Epoch: 73 [240/1400 (17%)]\tLoss: 138.348328\n",
            "Train Epoch: 73 [280/1400 (20%)]\tLoss: 161.832031\n",
            "Train Epoch: 73 [320/1400 (23%)]\tLoss: 133.470840\n",
            "Train Epoch: 73 [360/1400 (26%)]\tLoss: 141.875549\n",
            "Train Epoch: 73 [400/1400 (29%)]\tLoss: 157.123703\n",
            "Train Epoch: 73 [440/1400 (31%)]\tLoss: 140.734436\n",
            "Train Epoch: 73 [480/1400 (34%)]\tLoss: 117.861153\n",
            "Train Epoch: 73 [520/1400 (37%)]\tLoss: 153.046555\n",
            "Train Epoch: 73 [560/1400 (40%)]\tLoss: 147.097733\n",
            "Train Epoch: 73 [600/1400 (43%)]\tLoss: 151.853241\n",
            "Train Epoch: 73 [640/1400 (46%)]\tLoss: 151.408859\n",
            "Train Epoch: 73 [680/1400 (49%)]\tLoss: 119.559875\n",
            "Train Epoch: 73 [720/1400 (51%)]\tLoss: 148.389542\n",
            "Train Epoch: 73 [760/1400 (54%)]\tLoss: 134.341827\n",
            "Train Epoch: 73 [800/1400 (57%)]\tLoss: 123.488228\n",
            "Train Epoch: 73 [840/1400 (60%)]\tLoss: 138.313507\n",
            "Train Epoch: 73 [880/1400 (63%)]\tLoss: 127.771873\n",
            "Train Epoch: 73 [920/1400 (66%)]\tLoss: 167.824066\n",
            "Train Epoch: 73 [960/1400 (69%)]\tLoss: 122.145264\n",
            "Train Epoch: 73 [1000/1400 (71%)]\tLoss: 165.075439\n",
            "Train Epoch: 73 [1040/1400 (74%)]\tLoss: 134.079712\n",
            "Train Epoch: 73 [1080/1400 (77%)]\tLoss: 149.432938\n",
            "Train Epoch: 73 [1120/1400 (80%)]\tLoss: 116.407295\n",
            "Train Epoch: 73 [1160/1400 (83%)]\tLoss: 136.456146\n",
            "Train Epoch: 73 [1200/1400 (86%)]\tLoss: 145.671112\n",
            "Train Epoch: 73 [1240/1400 (89%)]\tLoss: 132.342697\n",
            "Train Epoch: 73 [1280/1400 (91%)]\tLoss: 140.956879\n",
            "Train Epoch: 73 [1320/1400 (94%)]\tLoss: 137.381760\n",
            "Train Epoch: 73 [1360/1400 (97%)]\tLoss: 146.992645\n",
            "====> Epoch: 73 Average loss: 138.2568\n",
            "====> Test set loss: 141.5826\n",
            "Train Epoch: 74 [0/1400 (0%)]\tLoss: 143.129227\n",
            "Train Epoch: 74 [40/1400 (3%)]\tLoss: 128.360779\n",
            "Train Epoch: 74 [80/1400 (6%)]\tLoss: 131.565613\n",
            "Train Epoch: 74 [120/1400 (9%)]\tLoss: 128.798538\n",
            "Train Epoch: 74 [160/1400 (11%)]\tLoss: 136.229309\n",
            "Train Epoch: 74 [200/1400 (14%)]\tLoss: 156.585861\n",
            "Train Epoch: 74 [240/1400 (17%)]\tLoss: 120.921783\n",
            "Train Epoch: 74 [280/1400 (20%)]\tLoss: 132.685211\n",
            "Train Epoch: 74 [320/1400 (23%)]\tLoss: 125.137939\n",
            "Train Epoch: 74 [360/1400 (26%)]\tLoss: 147.340851\n",
            "Train Epoch: 74 [400/1400 (29%)]\tLoss: 145.577393\n",
            "Train Epoch: 74 [440/1400 (31%)]\tLoss: 136.889328\n",
            "Train Epoch: 74 [480/1400 (34%)]\tLoss: 130.214325\n",
            "Train Epoch: 74 [520/1400 (37%)]\tLoss: 120.154198\n",
            "Train Epoch: 74 [560/1400 (40%)]\tLoss: 151.742981\n",
            "Train Epoch: 74 [600/1400 (43%)]\tLoss: 138.893616\n",
            "Train Epoch: 74 [640/1400 (46%)]\tLoss: 133.772842\n",
            "Train Epoch: 74 [680/1400 (49%)]\tLoss: 151.176910\n",
            "Train Epoch: 74 [720/1400 (51%)]\tLoss: 137.447006\n",
            "Train Epoch: 74 [760/1400 (54%)]\tLoss: 145.470749\n",
            "Train Epoch: 74 [800/1400 (57%)]\tLoss: 150.909637\n",
            "Train Epoch: 74 [840/1400 (60%)]\tLoss: 128.930588\n",
            "Train Epoch: 74 [880/1400 (63%)]\tLoss: 141.724838\n",
            "Train Epoch: 74 [920/1400 (66%)]\tLoss: 133.774170\n",
            "Train Epoch: 74 [960/1400 (69%)]\tLoss: 149.142975\n",
            "Train Epoch: 74 [1000/1400 (71%)]\tLoss: 125.031288\n",
            "Train Epoch: 74 [1040/1400 (74%)]\tLoss: 146.405701\n",
            "Train Epoch: 74 [1080/1400 (77%)]\tLoss: 139.669586\n",
            "Train Epoch: 74 [1120/1400 (80%)]\tLoss: 124.287323\n",
            "Train Epoch: 74 [1160/1400 (83%)]\tLoss: 143.847214\n",
            "Train Epoch: 74 [1200/1400 (86%)]\tLoss: 134.276810\n",
            "Train Epoch: 74 [1240/1400 (89%)]\tLoss: 139.108231\n",
            "Train Epoch: 74 [1280/1400 (91%)]\tLoss: 133.586548\n",
            "Train Epoch: 74 [1320/1400 (94%)]\tLoss: 133.345276\n",
            "Train Epoch: 74 [1360/1400 (97%)]\tLoss: 151.952454\n",
            "====> Epoch: 74 Average loss: 138.1594\n",
            "====> Test set loss: 141.4173\n",
            "Train Epoch: 75 [0/1400 (0%)]\tLoss: 122.138855\n",
            "Train Epoch: 75 [40/1400 (3%)]\tLoss: 155.212585\n",
            "Train Epoch: 75 [80/1400 (6%)]\tLoss: 128.112610\n",
            "Train Epoch: 75 [120/1400 (9%)]\tLoss: 120.363670\n",
            "Train Epoch: 75 [160/1400 (11%)]\tLoss: 127.244438\n",
            "Train Epoch: 75 [200/1400 (14%)]\tLoss: 134.809189\n",
            "Train Epoch: 75 [240/1400 (17%)]\tLoss: 139.836563\n",
            "Train Epoch: 75 [280/1400 (20%)]\tLoss: 136.110886\n",
            "Train Epoch: 75 [320/1400 (23%)]\tLoss: 134.552750\n",
            "Train Epoch: 75 [360/1400 (26%)]\tLoss: 124.121361\n",
            "Train Epoch: 75 [400/1400 (29%)]\tLoss: 130.977859\n",
            "Train Epoch: 75 [440/1400 (31%)]\tLoss: 150.570862\n",
            "Train Epoch: 75 [480/1400 (34%)]\tLoss: 156.359665\n",
            "Train Epoch: 75 [520/1400 (37%)]\tLoss: 140.536713\n",
            "Train Epoch: 75 [560/1400 (40%)]\tLoss: 126.231003\n",
            "Train Epoch: 75 [600/1400 (43%)]\tLoss: 128.815216\n",
            "Train Epoch: 75 [640/1400 (46%)]\tLoss: 124.758003\n",
            "Train Epoch: 75 [680/1400 (49%)]\tLoss: 137.968842\n",
            "Train Epoch: 75 [720/1400 (51%)]\tLoss: 168.675797\n",
            "Train Epoch: 75 [760/1400 (54%)]\tLoss: 142.227707\n",
            "Train Epoch: 75 [800/1400 (57%)]\tLoss: 142.671295\n",
            "Train Epoch: 75 [840/1400 (60%)]\tLoss: 140.320984\n",
            "Train Epoch: 75 [880/1400 (63%)]\tLoss: 134.027908\n",
            "Train Epoch: 75 [920/1400 (66%)]\tLoss: 136.799225\n",
            "Train Epoch: 75 [960/1400 (69%)]\tLoss: 130.407852\n",
            "Train Epoch: 75 [1000/1400 (71%)]\tLoss: 134.698624\n",
            "Train Epoch: 75 [1040/1400 (74%)]\tLoss: 134.191742\n",
            "Train Epoch: 75 [1080/1400 (77%)]\tLoss: 140.188660\n",
            "Train Epoch: 75 [1120/1400 (80%)]\tLoss: 151.059357\n",
            "Train Epoch: 75 [1160/1400 (83%)]\tLoss: 154.864761\n",
            "Train Epoch: 75 [1200/1400 (86%)]\tLoss: 135.106247\n",
            "Train Epoch: 75 [1240/1400 (89%)]\tLoss: 125.811592\n",
            "Train Epoch: 75 [1280/1400 (91%)]\tLoss: 155.748886\n",
            "Train Epoch: 75 [1320/1400 (94%)]\tLoss: 120.120102\n",
            "Train Epoch: 75 [1360/1400 (97%)]\tLoss: 150.749405\n",
            "====> Epoch: 75 Average loss: 138.0961\n",
            "====> Test set loss: 141.4930\n",
            "Train Epoch: 76 [0/1400 (0%)]\tLoss: 130.807846\n",
            "Train Epoch: 76 [40/1400 (3%)]\tLoss: 116.690933\n",
            "Train Epoch: 76 [80/1400 (6%)]\tLoss: 152.272186\n",
            "Train Epoch: 76 [120/1400 (9%)]\tLoss: 127.707718\n",
            "Train Epoch: 76 [160/1400 (11%)]\tLoss: 148.186646\n",
            "Train Epoch: 76 [200/1400 (14%)]\tLoss: 125.221870\n",
            "Train Epoch: 76 [240/1400 (17%)]\tLoss: 161.490509\n",
            "Train Epoch: 76 [280/1400 (20%)]\tLoss: 129.005203\n",
            "Train Epoch: 76 [320/1400 (23%)]\tLoss: 128.819443\n",
            "Train Epoch: 76 [360/1400 (26%)]\tLoss: 131.160385\n",
            "Train Epoch: 76 [400/1400 (29%)]\tLoss: 151.504379\n",
            "Train Epoch: 76 [440/1400 (31%)]\tLoss: 120.414101\n",
            "Train Epoch: 76 [480/1400 (34%)]\tLoss: 106.770111\n",
            "Train Epoch: 76 [520/1400 (37%)]\tLoss: 127.581924\n",
            "Train Epoch: 76 [560/1400 (40%)]\tLoss: 129.623032\n",
            "Train Epoch: 76 [600/1400 (43%)]\tLoss: 122.552055\n",
            "Train Epoch: 76 [640/1400 (46%)]\tLoss: 137.888672\n",
            "Train Epoch: 76 [680/1400 (49%)]\tLoss: 135.866608\n",
            "Train Epoch: 76 [720/1400 (51%)]\tLoss: 129.480408\n",
            "Train Epoch: 76 [760/1400 (54%)]\tLoss: 140.422607\n",
            "Train Epoch: 76 [800/1400 (57%)]\tLoss: 126.183472\n",
            "Train Epoch: 76 [840/1400 (60%)]\tLoss: 138.385361\n",
            "Train Epoch: 76 [880/1400 (63%)]\tLoss: 111.624542\n",
            "Train Epoch: 76 [920/1400 (66%)]\tLoss: 117.180824\n",
            "Train Epoch: 76 [960/1400 (69%)]\tLoss: 120.861435\n",
            "Train Epoch: 76 [1000/1400 (71%)]\tLoss: 144.949890\n",
            "Train Epoch: 76 [1040/1400 (74%)]\tLoss: 134.848206\n",
            "Train Epoch: 76 [1080/1400 (77%)]\tLoss: 155.322510\n",
            "Train Epoch: 76 [1120/1400 (80%)]\tLoss: 136.633026\n",
            "Train Epoch: 76 [1160/1400 (83%)]\tLoss: 142.552017\n",
            "Train Epoch: 76 [1200/1400 (86%)]\tLoss: 131.133728\n",
            "Train Epoch: 76 [1240/1400 (89%)]\tLoss: 151.267151\n",
            "Train Epoch: 76 [1280/1400 (91%)]\tLoss: 137.599442\n",
            "Train Epoch: 76 [1320/1400 (94%)]\tLoss: 144.975082\n",
            "Train Epoch: 76 [1360/1400 (97%)]\tLoss: 134.070511\n",
            "====> Epoch: 76 Average loss: 138.1336\n",
            "====> Test set loss: 141.4415\n",
            "Train Epoch: 77 [0/1400 (0%)]\tLoss: 110.364746\n",
            "Train Epoch: 77 [40/1400 (3%)]\tLoss: 128.365875\n",
            "Train Epoch: 77 [80/1400 (6%)]\tLoss: 141.844986\n",
            "Train Epoch: 77 [120/1400 (9%)]\tLoss: 145.345474\n",
            "Train Epoch: 77 [160/1400 (11%)]\tLoss: 141.521210\n",
            "Train Epoch: 77 [200/1400 (14%)]\tLoss: 147.958389\n",
            "Train Epoch: 77 [240/1400 (17%)]\tLoss: 158.400742\n",
            "Train Epoch: 77 [280/1400 (20%)]\tLoss: 157.037979\n",
            "Train Epoch: 77 [320/1400 (23%)]\tLoss: 145.055984\n",
            "Train Epoch: 77 [360/1400 (26%)]\tLoss: 141.914322\n",
            "Train Epoch: 77 [400/1400 (29%)]\tLoss: 129.180878\n",
            "Train Epoch: 77 [440/1400 (31%)]\tLoss: 128.938019\n",
            "Train Epoch: 77 [480/1400 (34%)]\tLoss: 135.757416\n",
            "Train Epoch: 77 [520/1400 (37%)]\tLoss: 153.361557\n",
            "Train Epoch: 77 [560/1400 (40%)]\tLoss: 137.448776\n",
            "Train Epoch: 77 [600/1400 (43%)]\tLoss: 131.958817\n",
            "Train Epoch: 77 [640/1400 (46%)]\tLoss: 140.557983\n",
            "Train Epoch: 77 [680/1400 (49%)]\tLoss: 147.748016\n",
            "Train Epoch: 77 [720/1400 (51%)]\tLoss: 158.087296\n",
            "Train Epoch: 77 [760/1400 (54%)]\tLoss: 141.934250\n",
            "Train Epoch: 77 [800/1400 (57%)]\tLoss: 138.732071\n",
            "Train Epoch: 77 [840/1400 (60%)]\tLoss: 142.681900\n",
            "Train Epoch: 77 [880/1400 (63%)]\tLoss: 133.177689\n",
            "Train Epoch: 77 [920/1400 (66%)]\tLoss: 133.231079\n",
            "Train Epoch: 77 [960/1400 (69%)]\tLoss: 140.434692\n",
            "Train Epoch: 77 [1000/1400 (71%)]\tLoss: 158.377090\n",
            "Train Epoch: 77 [1040/1400 (74%)]\tLoss: 122.488274\n",
            "Train Epoch: 77 [1080/1400 (77%)]\tLoss: 133.683060\n",
            "Train Epoch: 77 [1120/1400 (80%)]\tLoss: 127.034264\n",
            "Train Epoch: 77 [1160/1400 (83%)]\tLoss: 134.611877\n",
            "Train Epoch: 77 [1200/1400 (86%)]\tLoss: 168.071320\n",
            "Train Epoch: 77 [1240/1400 (89%)]\tLoss: 144.338425\n",
            "Train Epoch: 77 [1280/1400 (91%)]\tLoss: 125.469025\n",
            "Train Epoch: 77 [1320/1400 (94%)]\tLoss: 136.049835\n",
            "Train Epoch: 77 [1360/1400 (97%)]\tLoss: 159.053192\n",
            "====> Epoch: 77 Average loss: 138.0277\n",
            "====> Test set loss: 141.4990\n",
            "Train Epoch: 78 [0/1400 (0%)]\tLoss: 132.663055\n",
            "Train Epoch: 78 [40/1400 (3%)]\tLoss: 144.794815\n",
            "Train Epoch: 78 [80/1400 (6%)]\tLoss: 142.205185\n",
            "Train Epoch: 78 [120/1400 (9%)]\tLoss: 146.591049\n",
            "Train Epoch: 78 [160/1400 (11%)]\tLoss: 120.889496\n",
            "Train Epoch: 78 [200/1400 (14%)]\tLoss: 155.916275\n",
            "Train Epoch: 78 [240/1400 (17%)]\tLoss: 150.585327\n",
            "Train Epoch: 78 [280/1400 (20%)]\tLoss: 143.302612\n",
            "Train Epoch: 78 [320/1400 (23%)]\tLoss: 146.855942\n",
            "Train Epoch: 78 [360/1400 (26%)]\tLoss: 146.950790\n",
            "Train Epoch: 78 [400/1400 (29%)]\tLoss: 129.026535\n",
            "Train Epoch: 78 [440/1400 (31%)]\tLoss: 141.484802\n",
            "Train Epoch: 78 [480/1400 (34%)]\tLoss: 138.886978\n",
            "Train Epoch: 78 [520/1400 (37%)]\tLoss: 139.307632\n",
            "Train Epoch: 78 [560/1400 (40%)]\tLoss: 123.016525\n",
            "Train Epoch: 78 [600/1400 (43%)]\tLoss: 139.666290\n",
            "Train Epoch: 78 [640/1400 (46%)]\tLoss: 123.647026\n",
            "Train Epoch: 78 [680/1400 (49%)]\tLoss: 133.890579\n",
            "Train Epoch: 78 [720/1400 (51%)]\tLoss: 145.897369\n",
            "Train Epoch: 78 [760/1400 (54%)]\tLoss: 133.809753\n",
            "Train Epoch: 78 [800/1400 (57%)]\tLoss: 122.185043\n",
            "Train Epoch: 78 [840/1400 (60%)]\tLoss: 122.197014\n",
            "Train Epoch: 78 [880/1400 (63%)]\tLoss: 113.999405\n",
            "Train Epoch: 78 [920/1400 (66%)]\tLoss: 129.965897\n",
            "Train Epoch: 78 [960/1400 (69%)]\tLoss: 135.538193\n",
            "Train Epoch: 78 [1000/1400 (71%)]\tLoss: 152.114075\n",
            "Train Epoch: 78 [1040/1400 (74%)]\tLoss: 130.316162\n",
            "Train Epoch: 78 [1080/1400 (77%)]\tLoss: 129.365707\n",
            "Train Epoch: 78 [1120/1400 (80%)]\tLoss: 127.096092\n",
            "Train Epoch: 78 [1160/1400 (83%)]\tLoss: 136.622330\n",
            "Train Epoch: 78 [1200/1400 (86%)]\tLoss: 138.226974\n",
            "Train Epoch: 78 [1240/1400 (89%)]\tLoss: 124.354980\n",
            "Train Epoch: 78 [1280/1400 (91%)]\tLoss: 135.091278\n",
            "Train Epoch: 78 [1320/1400 (94%)]\tLoss: 144.392502\n",
            "Train Epoch: 78 [1360/1400 (97%)]\tLoss: 123.734894\n",
            "====> Epoch: 78 Average loss: 138.0226\n",
            "====> Test set loss: 141.3736\n",
            "Train Epoch: 79 [0/1400 (0%)]\tLoss: 127.965996\n",
            "Train Epoch: 79 [40/1400 (3%)]\tLoss: 124.491531\n",
            "Train Epoch: 79 [80/1400 (6%)]\tLoss: 141.774704\n",
            "Train Epoch: 79 [120/1400 (9%)]\tLoss: 138.965408\n",
            "Train Epoch: 79 [160/1400 (11%)]\tLoss: 143.162750\n",
            "Train Epoch: 79 [200/1400 (14%)]\tLoss: 126.962074\n",
            "Train Epoch: 79 [240/1400 (17%)]\tLoss: 149.394501\n",
            "Train Epoch: 79 [280/1400 (20%)]\tLoss: 155.578705\n",
            "Train Epoch: 79 [320/1400 (23%)]\tLoss: 132.507889\n",
            "Train Epoch: 79 [360/1400 (26%)]\tLoss: 157.924057\n",
            "Train Epoch: 79 [400/1400 (29%)]\tLoss: 147.633438\n",
            "Train Epoch: 79 [440/1400 (31%)]\tLoss: 132.703979\n",
            "Train Epoch: 79 [480/1400 (34%)]\tLoss: 132.638611\n",
            "Train Epoch: 79 [520/1400 (37%)]\tLoss: 134.414093\n",
            "Train Epoch: 79 [560/1400 (40%)]\tLoss: 129.060654\n",
            "Train Epoch: 79 [600/1400 (43%)]\tLoss: 132.418198\n",
            "Train Epoch: 79 [640/1400 (46%)]\tLoss: 154.517334\n",
            "Train Epoch: 79 [680/1400 (49%)]\tLoss: 140.991638\n",
            "Train Epoch: 79 [720/1400 (51%)]\tLoss: 138.220886\n",
            "Train Epoch: 79 [760/1400 (54%)]\tLoss: 157.639511\n",
            "Train Epoch: 79 [800/1400 (57%)]\tLoss: 133.820267\n",
            "Train Epoch: 79 [840/1400 (60%)]\tLoss: 148.373779\n",
            "Train Epoch: 79 [880/1400 (63%)]\tLoss: 135.825333\n",
            "Train Epoch: 79 [920/1400 (66%)]\tLoss: 141.111374\n",
            "Train Epoch: 79 [960/1400 (69%)]\tLoss: 115.652809\n",
            "Train Epoch: 79 [1000/1400 (71%)]\tLoss: 123.284927\n",
            "Train Epoch: 79 [1040/1400 (74%)]\tLoss: 144.331696\n",
            "Train Epoch: 79 [1080/1400 (77%)]\tLoss: 144.612183\n",
            "Train Epoch: 79 [1120/1400 (80%)]\tLoss: 127.668869\n",
            "Train Epoch: 79 [1160/1400 (83%)]\tLoss: 121.454666\n",
            "Train Epoch: 79 [1200/1400 (86%)]\tLoss: 133.317810\n",
            "Train Epoch: 79 [1240/1400 (89%)]\tLoss: 138.759460\n",
            "Train Epoch: 79 [1280/1400 (91%)]\tLoss: 136.520020\n",
            "Train Epoch: 79 [1320/1400 (94%)]\tLoss: 133.295685\n",
            "Train Epoch: 79 [1360/1400 (97%)]\tLoss: 136.173798\n",
            "====> Epoch: 79 Average loss: 138.0551\n",
            "====> Test set loss: 141.4291\n",
            "Train Epoch: 80 [0/1400 (0%)]\tLoss: 146.144089\n",
            "Train Epoch: 80 [40/1400 (3%)]\tLoss: 142.422501\n",
            "Train Epoch: 80 [80/1400 (6%)]\tLoss: 120.749207\n",
            "Train Epoch: 80 [120/1400 (9%)]\tLoss: 132.847046\n",
            "Train Epoch: 80 [160/1400 (11%)]\tLoss: 148.317551\n",
            "Train Epoch: 80 [200/1400 (14%)]\tLoss: 127.522842\n",
            "Train Epoch: 80 [240/1400 (17%)]\tLoss: 122.599617\n",
            "Train Epoch: 80 [280/1400 (20%)]\tLoss: 125.215889\n",
            "Train Epoch: 80 [320/1400 (23%)]\tLoss: 147.275696\n",
            "Train Epoch: 80 [360/1400 (26%)]\tLoss: 140.932297\n",
            "Train Epoch: 80 [400/1400 (29%)]\tLoss: 119.088898\n",
            "Train Epoch: 80 [440/1400 (31%)]\tLoss: 130.013031\n",
            "Train Epoch: 80 [480/1400 (34%)]\tLoss: 131.925049\n",
            "Train Epoch: 80 [520/1400 (37%)]\tLoss: 111.391930\n",
            "Train Epoch: 80 [560/1400 (40%)]\tLoss: 123.074425\n",
            "Train Epoch: 80 [600/1400 (43%)]\tLoss: 145.898941\n",
            "Train Epoch: 80 [640/1400 (46%)]\tLoss: 135.077789\n",
            "Train Epoch: 80 [680/1400 (49%)]\tLoss: 125.365364\n",
            "Train Epoch: 80 [720/1400 (51%)]\tLoss: 136.977417\n",
            "Train Epoch: 80 [760/1400 (54%)]\tLoss: 140.774216\n",
            "Train Epoch: 80 [800/1400 (57%)]\tLoss: 135.705124\n",
            "Train Epoch: 80 [840/1400 (60%)]\tLoss: 142.109253\n",
            "Train Epoch: 80 [880/1400 (63%)]\tLoss: 144.206284\n",
            "Train Epoch: 80 [920/1400 (66%)]\tLoss: 144.369064\n",
            "Train Epoch: 80 [960/1400 (69%)]\tLoss: 127.011833\n",
            "Train Epoch: 80 [1000/1400 (71%)]\tLoss: 159.164627\n",
            "Train Epoch: 80 [1040/1400 (74%)]\tLoss: 135.674759\n",
            "Train Epoch: 80 [1080/1400 (77%)]\tLoss: 141.011658\n",
            "Train Epoch: 80 [1120/1400 (80%)]\tLoss: 148.029373\n",
            "Train Epoch: 80 [1160/1400 (83%)]\tLoss: 136.394821\n",
            "Train Epoch: 80 [1200/1400 (86%)]\tLoss: 137.743362\n",
            "Train Epoch: 80 [1240/1400 (89%)]\tLoss: 147.682739\n",
            "Train Epoch: 80 [1280/1400 (91%)]\tLoss: 151.768631\n",
            "Train Epoch: 80 [1320/1400 (94%)]\tLoss: 138.424835\n",
            "Train Epoch: 80 [1360/1400 (97%)]\tLoss: 149.793610\n",
            "====> Epoch: 80 Average loss: 137.8883\n",
            "====> Test set loss: 141.5551\n",
            "Train Epoch: 81 [0/1400 (0%)]\tLoss: 127.231590\n",
            "Train Epoch: 81 [40/1400 (3%)]\tLoss: 153.398270\n",
            "Train Epoch: 81 [80/1400 (6%)]\tLoss: 140.996994\n",
            "Train Epoch: 81 [120/1400 (9%)]\tLoss: 125.245285\n",
            "Train Epoch: 81 [160/1400 (11%)]\tLoss: 139.911469\n",
            "Train Epoch: 81 [200/1400 (14%)]\tLoss: 119.511398\n",
            "Train Epoch: 81 [240/1400 (17%)]\tLoss: 152.480637\n",
            "Train Epoch: 81 [280/1400 (20%)]\tLoss: 139.073746\n",
            "Train Epoch: 81 [320/1400 (23%)]\tLoss: 145.723251\n",
            "Train Epoch: 81 [360/1400 (26%)]\tLoss: 150.371597\n",
            "Train Epoch: 81 [400/1400 (29%)]\tLoss: 131.964325\n",
            "Train Epoch: 81 [440/1400 (31%)]\tLoss: 133.140198\n",
            "Train Epoch: 81 [480/1400 (34%)]\tLoss: 133.826706\n",
            "Train Epoch: 81 [520/1400 (37%)]\tLoss: 136.046677\n",
            "Train Epoch: 81 [560/1400 (40%)]\tLoss: 150.564987\n",
            "Train Epoch: 81 [600/1400 (43%)]\tLoss: 138.972855\n",
            "Train Epoch: 81 [640/1400 (46%)]\tLoss: 129.837494\n",
            "Train Epoch: 81 [680/1400 (49%)]\tLoss: 145.263596\n",
            "Train Epoch: 81 [720/1400 (51%)]\tLoss: 149.843460\n",
            "Train Epoch: 81 [760/1400 (54%)]\tLoss: 133.648102\n",
            "Train Epoch: 81 [800/1400 (57%)]\tLoss: 139.091217\n",
            "Train Epoch: 81 [840/1400 (60%)]\tLoss: 163.011627\n",
            "Train Epoch: 81 [880/1400 (63%)]\tLoss: 165.497284\n",
            "Train Epoch: 81 [920/1400 (66%)]\tLoss: 137.788208\n",
            "Train Epoch: 81 [960/1400 (69%)]\tLoss: 120.839264\n",
            "Train Epoch: 81 [1000/1400 (71%)]\tLoss: 139.818588\n",
            "Train Epoch: 81 [1040/1400 (74%)]\tLoss: 144.586151\n",
            "Train Epoch: 81 [1080/1400 (77%)]\tLoss: 140.783279\n",
            "Train Epoch: 81 [1120/1400 (80%)]\tLoss: 169.662766\n",
            "Train Epoch: 81 [1160/1400 (83%)]\tLoss: 141.601135\n",
            "Train Epoch: 81 [1200/1400 (86%)]\tLoss: 120.671494\n",
            "Train Epoch: 81 [1240/1400 (89%)]\tLoss: 137.577545\n",
            "Train Epoch: 81 [1280/1400 (91%)]\tLoss: 127.680443\n",
            "Train Epoch: 81 [1320/1400 (94%)]\tLoss: 137.706146\n",
            "Train Epoch: 81 [1360/1400 (97%)]\tLoss: 115.118690\n",
            "====> Epoch: 81 Average loss: 137.8206\n",
            "====> Test set loss: 141.4327\n",
            "Train Epoch: 82 [0/1400 (0%)]\tLoss: 149.809143\n",
            "Train Epoch: 82 [40/1400 (3%)]\tLoss: 136.820648\n",
            "Train Epoch: 82 [80/1400 (6%)]\tLoss: 138.775192\n",
            "Train Epoch: 82 [120/1400 (9%)]\tLoss: 143.347198\n",
            "Train Epoch: 82 [160/1400 (11%)]\tLoss: 140.966064\n",
            "Train Epoch: 82 [200/1400 (14%)]\tLoss: 123.636002\n",
            "Train Epoch: 82 [240/1400 (17%)]\tLoss: 158.156433\n",
            "Train Epoch: 82 [280/1400 (20%)]\tLoss: 131.370621\n",
            "Train Epoch: 82 [320/1400 (23%)]\tLoss: 127.709549\n",
            "Train Epoch: 82 [360/1400 (26%)]\tLoss: 143.481339\n",
            "Train Epoch: 82 [400/1400 (29%)]\tLoss: 157.472076\n",
            "Train Epoch: 82 [440/1400 (31%)]\tLoss: 166.333115\n",
            "Train Epoch: 82 [480/1400 (34%)]\tLoss: 135.873322\n",
            "Train Epoch: 82 [520/1400 (37%)]\tLoss: 129.353104\n",
            "Train Epoch: 82 [560/1400 (40%)]\tLoss: 142.892548\n",
            "Train Epoch: 82 [600/1400 (43%)]\tLoss: 120.258202\n",
            "Train Epoch: 82 [640/1400 (46%)]\tLoss: 135.674484\n",
            "Train Epoch: 82 [680/1400 (49%)]\tLoss: 134.722885\n",
            "Train Epoch: 82 [720/1400 (51%)]\tLoss: 137.290710\n",
            "Train Epoch: 82 [760/1400 (54%)]\tLoss: 132.599579\n",
            "Train Epoch: 82 [800/1400 (57%)]\tLoss: 117.428177\n",
            "Train Epoch: 82 [840/1400 (60%)]\tLoss: 131.096695\n",
            "Train Epoch: 82 [880/1400 (63%)]\tLoss: 137.315414\n",
            "Train Epoch: 82 [920/1400 (66%)]\tLoss: 151.607697\n",
            "Train Epoch: 82 [960/1400 (69%)]\tLoss: 140.929108\n",
            "Train Epoch: 82 [1000/1400 (71%)]\tLoss: 129.976578\n",
            "Train Epoch: 82 [1040/1400 (74%)]\tLoss: 137.730988\n",
            "Train Epoch: 82 [1080/1400 (77%)]\tLoss: 138.905151\n",
            "Train Epoch: 82 [1120/1400 (80%)]\tLoss: 151.608887\n",
            "Train Epoch: 82 [1160/1400 (83%)]\tLoss: 129.604538\n",
            "Train Epoch: 82 [1200/1400 (86%)]\tLoss: 130.936295\n",
            "Train Epoch: 82 [1240/1400 (89%)]\tLoss: 146.233643\n",
            "Train Epoch: 82 [1280/1400 (91%)]\tLoss: 135.311035\n",
            "Train Epoch: 82 [1320/1400 (94%)]\tLoss: 138.475021\n",
            "Train Epoch: 82 [1360/1400 (97%)]\tLoss: 127.313568\n",
            "====> Epoch: 82 Average loss: 137.8067\n",
            "====> Test set loss: 141.4150\n",
            "Train Epoch: 83 [0/1400 (0%)]\tLoss: 140.207489\n",
            "Train Epoch: 83 [40/1400 (3%)]\tLoss: 131.082352\n",
            "Train Epoch: 83 [80/1400 (6%)]\tLoss: 111.116272\n",
            "Train Epoch: 83 [120/1400 (9%)]\tLoss: 138.957230\n",
            "Train Epoch: 83 [160/1400 (11%)]\tLoss: 130.380127\n",
            "Train Epoch: 83 [200/1400 (14%)]\tLoss: 133.485260\n",
            "Train Epoch: 83 [240/1400 (17%)]\tLoss: 154.782532\n",
            "Train Epoch: 83 [280/1400 (20%)]\tLoss: 139.912430\n",
            "Train Epoch: 83 [320/1400 (23%)]\tLoss: 125.265488\n",
            "Train Epoch: 83 [360/1400 (26%)]\tLoss: 144.692230\n",
            "Train Epoch: 83 [400/1400 (29%)]\tLoss: 134.267212\n",
            "Train Epoch: 83 [440/1400 (31%)]\tLoss: 126.752388\n",
            "Train Epoch: 83 [480/1400 (34%)]\tLoss: 140.132645\n",
            "Train Epoch: 83 [520/1400 (37%)]\tLoss: 143.466125\n",
            "Train Epoch: 83 [560/1400 (40%)]\tLoss: 144.562683\n",
            "Train Epoch: 83 [600/1400 (43%)]\tLoss: 118.104919\n",
            "Train Epoch: 83 [640/1400 (46%)]\tLoss: 154.259537\n",
            "Train Epoch: 83 [680/1400 (49%)]\tLoss: 163.441452\n",
            "Train Epoch: 83 [720/1400 (51%)]\tLoss: 132.943878\n",
            "Train Epoch: 83 [760/1400 (54%)]\tLoss: 147.822723\n",
            "Train Epoch: 83 [800/1400 (57%)]\tLoss: 132.230667\n",
            "Train Epoch: 83 [840/1400 (60%)]\tLoss: 137.529465\n",
            "Train Epoch: 83 [880/1400 (63%)]\tLoss: 152.820419\n",
            "Train Epoch: 83 [920/1400 (66%)]\tLoss: 153.797287\n",
            "Train Epoch: 83 [960/1400 (69%)]\tLoss: 122.964783\n",
            "Train Epoch: 83 [1000/1400 (71%)]\tLoss: 133.078339\n",
            "Train Epoch: 83 [1040/1400 (74%)]\tLoss: 135.730347\n",
            "Train Epoch: 83 [1080/1400 (77%)]\tLoss: 133.418854\n",
            "Train Epoch: 83 [1120/1400 (80%)]\tLoss: 130.564102\n",
            "Train Epoch: 83 [1160/1400 (83%)]\tLoss: 127.753578\n",
            "Train Epoch: 83 [1200/1400 (86%)]\tLoss: 151.166885\n",
            "Train Epoch: 83 [1240/1400 (89%)]\tLoss: 129.187820\n",
            "Train Epoch: 83 [1280/1400 (91%)]\tLoss: 120.982971\n",
            "Train Epoch: 83 [1320/1400 (94%)]\tLoss: 140.449890\n",
            "Train Epoch: 83 [1360/1400 (97%)]\tLoss: 156.753403\n",
            "====> Epoch: 83 Average loss: 137.8333\n",
            "====> Test set loss: 141.4937\n",
            "Train Epoch: 84 [0/1400 (0%)]\tLoss: 115.820885\n",
            "Train Epoch: 84 [40/1400 (3%)]\tLoss: 146.314285\n",
            "Train Epoch: 84 [80/1400 (6%)]\tLoss: 137.864441\n",
            "Train Epoch: 84 [120/1400 (9%)]\tLoss: 163.051529\n",
            "Train Epoch: 84 [160/1400 (11%)]\tLoss: 150.460876\n",
            "Train Epoch: 84 [200/1400 (14%)]\tLoss: 141.295166\n",
            "Train Epoch: 84 [240/1400 (17%)]\tLoss: 143.889160\n",
            "Train Epoch: 84 [280/1400 (20%)]\tLoss: 142.885132\n",
            "Train Epoch: 84 [320/1400 (23%)]\tLoss: 128.405624\n",
            "Train Epoch: 84 [360/1400 (26%)]\tLoss: 133.009766\n",
            "Train Epoch: 84 [400/1400 (29%)]\tLoss: 128.178986\n",
            "Train Epoch: 84 [440/1400 (31%)]\tLoss: 140.739059\n",
            "Train Epoch: 84 [480/1400 (34%)]\tLoss: 146.462158\n",
            "Train Epoch: 84 [520/1400 (37%)]\tLoss: 136.789017\n",
            "Train Epoch: 84 [560/1400 (40%)]\tLoss: 123.069305\n",
            "Train Epoch: 84 [600/1400 (43%)]\tLoss: 163.402176\n",
            "Train Epoch: 84 [640/1400 (46%)]\tLoss: 148.298981\n",
            "Train Epoch: 84 [680/1400 (49%)]\tLoss: 119.175797\n",
            "Train Epoch: 84 [720/1400 (51%)]\tLoss: 143.924667\n",
            "Train Epoch: 84 [760/1400 (54%)]\tLoss: 136.865189\n",
            "Train Epoch: 84 [800/1400 (57%)]\tLoss: 141.793015\n",
            "Train Epoch: 84 [840/1400 (60%)]\tLoss: 139.243942\n",
            "Train Epoch: 84 [880/1400 (63%)]\tLoss: 142.241882\n",
            "Train Epoch: 84 [920/1400 (66%)]\tLoss: 159.198135\n",
            "Train Epoch: 84 [960/1400 (69%)]\tLoss: 135.338654\n",
            "Train Epoch: 84 [1000/1400 (71%)]\tLoss: 120.437950\n",
            "Train Epoch: 84 [1040/1400 (74%)]\tLoss: 135.937820\n",
            "Train Epoch: 84 [1080/1400 (77%)]\tLoss: 128.510086\n",
            "Train Epoch: 84 [1120/1400 (80%)]\tLoss: 140.009079\n",
            "Train Epoch: 84 [1160/1400 (83%)]\tLoss: 141.889862\n",
            "Train Epoch: 84 [1200/1400 (86%)]\tLoss: 134.506668\n",
            "Train Epoch: 84 [1240/1400 (89%)]\tLoss: 141.009964\n",
            "Train Epoch: 84 [1280/1400 (91%)]\tLoss: 147.022888\n",
            "Train Epoch: 84 [1320/1400 (94%)]\tLoss: 144.183167\n",
            "Train Epoch: 84 [1360/1400 (97%)]\tLoss: 134.301636\n",
            "====> Epoch: 84 Average loss: 137.8868\n",
            "====> Test set loss: 141.3047\n",
            "Train Epoch: 85 [0/1400 (0%)]\tLoss: 131.107788\n",
            "Train Epoch: 85 [40/1400 (3%)]\tLoss: 159.870697\n",
            "Train Epoch: 85 [80/1400 (6%)]\tLoss: 116.588432\n",
            "Train Epoch: 85 [120/1400 (9%)]\tLoss: 124.457520\n",
            "Train Epoch: 85 [160/1400 (11%)]\tLoss: 139.791168\n",
            "Train Epoch: 85 [200/1400 (14%)]\tLoss: 129.350891\n",
            "Train Epoch: 85 [240/1400 (17%)]\tLoss: 156.948944\n",
            "Train Epoch: 85 [280/1400 (20%)]\tLoss: 164.143692\n",
            "Train Epoch: 85 [320/1400 (23%)]\tLoss: 140.715668\n",
            "Train Epoch: 85 [360/1400 (26%)]\tLoss: 139.134415\n",
            "Train Epoch: 85 [400/1400 (29%)]\tLoss: 132.438828\n",
            "Train Epoch: 85 [440/1400 (31%)]\tLoss: 148.882126\n",
            "Train Epoch: 85 [480/1400 (34%)]\tLoss: 142.232697\n",
            "Train Epoch: 85 [520/1400 (37%)]\tLoss: 125.894524\n",
            "Train Epoch: 85 [560/1400 (40%)]\tLoss: 132.437653\n",
            "Train Epoch: 85 [600/1400 (43%)]\tLoss: 149.593765\n",
            "Train Epoch: 85 [640/1400 (46%)]\tLoss: 137.305359\n",
            "Train Epoch: 85 [680/1400 (49%)]\tLoss: 140.076126\n",
            "Train Epoch: 85 [720/1400 (51%)]\tLoss: 128.228546\n",
            "Train Epoch: 85 [760/1400 (54%)]\tLoss: 122.657188\n",
            "Train Epoch: 85 [800/1400 (57%)]\tLoss: 133.418442\n",
            "Train Epoch: 85 [840/1400 (60%)]\tLoss: 142.841492\n",
            "Train Epoch: 85 [880/1400 (63%)]\tLoss: 132.773712\n",
            "Train Epoch: 85 [920/1400 (66%)]\tLoss: 138.110825\n",
            "Train Epoch: 85 [960/1400 (69%)]\tLoss: 150.196793\n",
            "Train Epoch: 85 [1000/1400 (71%)]\tLoss: 147.100708\n",
            "Train Epoch: 85 [1040/1400 (74%)]\tLoss: 131.708008\n",
            "Train Epoch: 85 [1080/1400 (77%)]\tLoss: 142.538071\n",
            "Train Epoch: 85 [1120/1400 (80%)]\tLoss: 152.507675\n",
            "Train Epoch: 85 [1160/1400 (83%)]\tLoss: 145.298630\n",
            "Train Epoch: 85 [1200/1400 (86%)]\tLoss: 166.984711\n",
            "Train Epoch: 85 [1240/1400 (89%)]\tLoss: 124.483025\n",
            "Train Epoch: 85 [1280/1400 (91%)]\tLoss: 134.423508\n",
            "Train Epoch: 85 [1320/1400 (94%)]\tLoss: 137.667328\n",
            "Train Epoch: 85 [1360/1400 (97%)]\tLoss: 127.835541\n",
            "====> Epoch: 85 Average loss: 137.7963\n",
            "====> Test set loss: 141.2773\n",
            "Train Epoch: 86 [0/1400 (0%)]\tLoss: 117.147087\n",
            "Train Epoch: 86 [40/1400 (3%)]\tLoss: 131.005966\n",
            "Train Epoch: 86 [80/1400 (6%)]\tLoss: 130.411835\n",
            "Train Epoch: 86 [120/1400 (9%)]\tLoss: 132.240372\n",
            "Train Epoch: 86 [160/1400 (11%)]\tLoss: 134.210846\n",
            "Train Epoch: 86 [200/1400 (14%)]\tLoss: 146.159027\n",
            "Train Epoch: 86 [240/1400 (17%)]\tLoss: 128.104309\n",
            "Train Epoch: 86 [280/1400 (20%)]\tLoss: 169.718750\n",
            "Train Epoch: 86 [320/1400 (23%)]\tLoss: 152.336533\n",
            "Train Epoch: 86 [360/1400 (26%)]\tLoss: 126.609283\n",
            "Train Epoch: 86 [400/1400 (29%)]\tLoss: 140.089569\n",
            "Train Epoch: 86 [440/1400 (31%)]\tLoss: 126.673485\n",
            "Train Epoch: 86 [480/1400 (34%)]\tLoss: 113.810486\n",
            "Train Epoch: 86 [520/1400 (37%)]\tLoss: 165.994461\n",
            "Train Epoch: 86 [560/1400 (40%)]\tLoss: 134.599854\n",
            "Train Epoch: 86 [600/1400 (43%)]\tLoss: 129.057587\n",
            "Train Epoch: 86 [640/1400 (46%)]\tLoss: 140.283493\n",
            "Train Epoch: 86 [680/1400 (49%)]\tLoss: 163.270432\n",
            "Train Epoch: 86 [720/1400 (51%)]\tLoss: 155.325073\n",
            "Train Epoch: 86 [760/1400 (54%)]\tLoss: 124.535072\n",
            "Train Epoch: 86 [800/1400 (57%)]\tLoss: 134.800293\n",
            "Train Epoch: 86 [840/1400 (60%)]\tLoss: 137.047440\n",
            "Train Epoch: 86 [880/1400 (63%)]\tLoss: 135.476166\n",
            "Train Epoch: 86 [920/1400 (66%)]\tLoss: 133.312546\n",
            "Train Epoch: 86 [960/1400 (69%)]\tLoss: 137.993256\n",
            "Train Epoch: 86 [1000/1400 (71%)]\tLoss: 144.142044\n",
            "Train Epoch: 86 [1040/1400 (74%)]\tLoss: 140.283264\n",
            "Train Epoch: 86 [1080/1400 (77%)]\tLoss: 147.444107\n",
            "Train Epoch: 86 [1120/1400 (80%)]\tLoss: 146.882111\n",
            "Train Epoch: 86 [1160/1400 (83%)]\tLoss: 147.464233\n",
            "Train Epoch: 86 [1200/1400 (86%)]\tLoss: 125.155098\n",
            "Train Epoch: 86 [1240/1400 (89%)]\tLoss: 116.168198\n",
            "Train Epoch: 86 [1280/1400 (91%)]\tLoss: 137.865967\n",
            "Train Epoch: 86 [1320/1400 (94%)]\tLoss: 137.305496\n",
            "Train Epoch: 86 [1360/1400 (97%)]\tLoss: 137.864349\n",
            "====> Epoch: 86 Average loss: 137.7492\n",
            "====> Test set loss: 141.2700\n",
            "Train Epoch: 87 [0/1400 (0%)]\tLoss: 143.485931\n",
            "Train Epoch: 87 [40/1400 (3%)]\tLoss: 129.404770\n",
            "Train Epoch: 87 [80/1400 (6%)]\tLoss: 123.109604\n",
            "Train Epoch: 87 [120/1400 (9%)]\tLoss: 134.844421\n",
            "Train Epoch: 87 [160/1400 (11%)]\tLoss: 138.052979\n",
            "Train Epoch: 87 [200/1400 (14%)]\tLoss: 129.352402\n",
            "Train Epoch: 87 [240/1400 (17%)]\tLoss: 143.478989\n",
            "Train Epoch: 87 [280/1400 (20%)]\tLoss: 120.402695\n",
            "Train Epoch: 87 [320/1400 (23%)]\tLoss: 152.690964\n",
            "Train Epoch: 87 [360/1400 (26%)]\tLoss: 160.599716\n",
            "Train Epoch: 87 [400/1400 (29%)]\tLoss: 143.551346\n",
            "Train Epoch: 87 [440/1400 (31%)]\tLoss: 131.307190\n",
            "Train Epoch: 87 [480/1400 (34%)]\tLoss: 131.486130\n",
            "Train Epoch: 87 [520/1400 (37%)]\tLoss: 139.713394\n",
            "Train Epoch: 87 [560/1400 (40%)]\tLoss: 149.173828\n",
            "Train Epoch: 87 [600/1400 (43%)]\tLoss: 137.315796\n",
            "Train Epoch: 87 [640/1400 (46%)]\tLoss: 132.974670\n",
            "Train Epoch: 87 [680/1400 (49%)]\tLoss: 124.515274\n",
            "Train Epoch: 87 [720/1400 (51%)]\tLoss: 142.167114\n",
            "Train Epoch: 87 [760/1400 (54%)]\tLoss: 115.125610\n",
            "Train Epoch: 87 [800/1400 (57%)]\tLoss: 120.381020\n",
            "Train Epoch: 87 [840/1400 (60%)]\tLoss: 123.942230\n",
            "Train Epoch: 87 [880/1400 (63%)]\tLoss: 164.804565\n",
            "Train Epoch: 87 [920/1400 (66%)]\tLoss: 158.348160\n",
            "Train Epoch: 87 [960/1400 (69%)]\tLoss: 171.393402\n",
            "Train Epoch: 87 [1000/1400 (71%)]\tLoss: 140.445450\n",
            "Train Epoch: 87 [1040/1400 (74%)]\tLoss: 138.602707\n",
            "Train Epoch: 87 [1080/1400 (77%)]\tLoss: 127.343201\n",
            "Train Epoch: 87 [1120/1400 (80%)]\tLoss: 123.402931\n",
            "Train Epoch: 87 [1160/1400 (83%)]\tLoss: 143.498047\n",
            "Train Epoch: 87 [1200/1400 (86%)]\tLoss: 176.379501\n",
            "Train Epoch: 87 [1240/1400 (89%)]\tLoss: 144.457947\n",
            "Train Epoch: 87 [1280/1400 (91%)]\tLoss: 141.389999\n",
            "Train Epoch: 87 [1320/1400 (94%)]\tLoss: 124.716766\n",
            "Train Epoch: 87 [1360/1400 (97%)]\tLoss: 137.130844\n",
            "====> Epoch: 87 Average loss: 137.6875\n",
            "====> Test set loss: 141.3744\n",
            "Train Epoch: 88 [0/1400 (0%)]\tLoss: 133.645630\n",
            "Train Epoch: 88 [40/1400 (3%)]\tLoss: 144.683960\n",
            "Train Epoch: 88 [80/1400 (6%)]\tLoss: 141.997208\n",
            "Train Epoch: 88 [120/1400 (9%)]\tLoss: 149.221146\n",
            "Train Epoch: 88 [160/1400 (11%)]\tLoss: 148.195862\n",
            "Train Epoch: 88 [200/1400 (14%)]\tLoss: 125.393814\n",
            "Train Epoch: 88 [240/1400 (17%)]\tLoss: 130.157654\n",
            "Train Epoch: 88 [280/1400 (20%)]\tLoss: 133.018097\n",
            "Train Epoch: 88 [320/1400 (23%)]\tLoss: 142.822250\n",
            "Train Epoch: 88 [360/1400 (26%)]\tLoss: 113.755753\n",
            "Train Epoch: 88 [400/1400 (29%)]\tLoss: 145.253494\n",
            "Train Epoch: 88 [440/1400 (31%)]\tLoss: 133.830597\n",
            "Train Epoch: 88 [480/1400 (34%)]\tLoss: 131.156937\n",
            "Train Epoch: 88 [520/1400 (37%)]\tLoss: 140.172394\n",
            "Train Epoch: 88 [560/1400 (40%)]\tLoss: 117.728348\n",
            "Train Epoch: 88 [600/1400 (43%)]\tLoss: 113.366089\n",
            "Train Epoch: 88 [640/1400 (46%)]\tLoss: 143.258636\n",
            "Train Epoch: 88 [680/1400 (49%)]\tLoss: 142.550064\n",
            "Train Epoch: 88 [720/1400 (51%)]\tLoss: 145.996231\n",
            "Train Epoch: 88 [760/1400 (54%)]\tLoss: 134.071228\n",
            "Train Epoch: 88 [800/1400 (57%)]\tLoss: 147.282486\n",
            "Train Epoch: 88 [840/1400 (60%)]\tLoss: 130.398727\n",
            "Train Epoch: 88 [880/1400 (63%)]\tLoss: 128.816147\n",
            "Train Epoch: 88 [920/1400 (66%)]\tLoss: 155.675568\n",
            "Train Epoch: 88 [960/1400 (69%)]\tLoss: 132.629807\n",
            "Train Epoch: 88 [1000/1400 (71%)]\tLoss: 134.212189\n",
            "Train Epoch: 88 [1040/1400 (74%)]\tLoss: 138.616226\n",
            "Train Epoch: 88 [1080/1400 (77%)]\tLoss: 141.624268\n",
            "Train Epoch: 88 [1120/1400 (80%)]\tLoss: 136.738235\n",
            "Train Epoch: 88 [1160/1400 (83%)]\tLoss: 105.455597\n",
            "Train Epoch: 88 [1200/1400 (86%)]\tLoss: 138.718109\n",
            "Train Epoch: 88 [1240/1400 (89%)]\tLoss: 138.581070\n",
            "Train Epoch: 88 [1280/1400 (91%)]\tLoss: 142.505341\n",
            "Train Epoch: 88 [1320/1400 (94%)]\tLoss: 130.907181\n",
            "Train Epoch: 88 [1360/1400 (97%)]\tLoss: 127.232391\n",
            "====> Epoch: 88 Average loss: 137.6309\n",
            "====> Test set loss: 141.2946\n",
            "Train Epoch: 89 [0/1400 (0%)]\tLoss: 153.421494\n",
            "Train Epoch: 89 [40/1400 (3%)]\tLoss: 123.834648\n",
            "Train Epoch: 89 [80/1400 (6%)]\tLoss: 150.127777\n",
            "Train Epoch: 89 [120/1400 (9%)]\tLoss: 157.814819\n",
            "Train Epoch: 89 [160/1400 (11%)]\tLoss: 146.671753\n",
            "Train Epoch: 89 [200/1400 (14%)]\tLoss: 127.891808\n",
            "Train Epoch: 89 [240/1400 (17%)]\tLoss: 151.032425\n",
            "Train Epoch: 89 [280/1400 (20%)]\tLoss: 137.174026\n",
            "Train Epoch: 89 [320/1400 (23%)]\tLoss: 131.469284\n",
            "Train Epoch: 89 [360/1400 (26%)]\tLoss: 124.834435\n",
            "Train Epoch: 89 [400/1400 (29%)]\tLoss: 164.895508\n",
            "Train Epoch: 89 [440/1400 (31%)]\tLoss: 158.717651\n",
            "Train Epoch: 89 [480/1400 (34%)]\tLoss: 147.970749\n",
            "Train Epoch: 89 [520/1400 (37%)]\tLoss: 140.721802\n",
            "Train Epoch: 89 [560/1400 (40%)]\tLoss: 150.547852\n",
            "Train Epoch: 89 [600/1400 (43%)]\tLoss: 133.569748\n",
            "Train Epoch: 89 [640/1400 (46%)]\tLoss: 142.936890\n",
            "Train Epoch: 89 [680/1400 (49%)]\tLoss: 152.149475\n",
            "Train Epoch: 89 [720/1400 (51%)]\tLoss: 146.316010\n",
            "Train Epoch: 89 [760/1400 (54%)]\tLoss: 140.767029\n",
            "Train Epoch: 89 [800/1400 (57%)]\tLoss: 160.537842\n",
            "Train Epoch: 89 [840/1400 (60%)]\tLoss: 148.038025\n",
            "Train Epoch: 89 [880/1400 (63%)]\tLoss: 120.331940\n",
            "Train Epoch: 89 [920/1400 (66%)]\tLoss: 143.045135\n",
            "Train Epoch: 89 [960/1400 (69%)]\tLoss: 120.072952\n",
            "Train Epoch: 89 [1000/1400 (71%)]\tLoss: 126.674377\n",
            "Train Epoch: 89 [1040/1400 (74%)]\tLoss: 116.480247\n",
            "Train Epoch: 89 [1080/1400 (77%)]\tLoss: 168.191437\n",
            "Train Epoch: 89 [1120/1400 (80%)]\tLoss: 137.060516\n",
            "Train Epoch: 89 [1160/1400 (83%)]\tLoss: 129.576431\n",
            "Train Epoch: 89 [1200/1400 (86%)]\tLoss: 156.599808\n",
            "Train Epoch: 89 [1240/1400 (89%)]\tLoss: 148.296829\n",
            "Train Epoch: 89 [1280/1400 (91%)]\tLoss: 138.884659\n",
            "Train Epoch: 89 [1320/1400 (94%)]\tLoss: 123.618835\n",
            "Train Epoch: 89 [1360/1400 (97%)]\tLoss: 131.881561\n",
            "====> Epoch: 89 Average loss: 137.6543\n",
            "====> Test set loss: 141.1620\n",
            "Train Epoch: 90 [0/1400 (0%)]\tLoss: 136.375854\n",
            "Train Epoch: 90 [40/1400 (3%)]\tLoss: 132.227509\n",
            "Train Epoch: 90 [80/1400 (6%)]\tLoss: 138.219482\n",
            "Train Epoch: 90 [120/1400 (9%)]\tLoss: 142.911942\n",
            "Train Epoch: 90 [160/1400 (11%)]\tLoss: 131.777679\n",
            "Train Epoch: 90 [200/1400 (14%)]\tLoss: 158.646484\n",
            "Train Epoch: 90 [240/1400 (17%)]\tLoss: 147.238525\n",
            "Train Epoch: 90 [280/1400 (20%)]\tLoss: 121.855125\n",
            "Train Epoch: 90 [320/1400 (23%)]\tLoss: 143.138489\n",
            "Train Epoch: 90 [360/1400 (26%)]\tLoss: 141.989517\n",
            "Train Epoch: 90 [400/1400 (29%)]\tLoss: 148.221924\n",
            "Train Epoch: 90 [440/1400 (31%)]\tLoss: 126.836601\n",
            "Train Epoch: 90 [480/1400 (34%)]\tLoss: 120.919624\n",
            "Train Epoch: 90 [520/1400 (37%)]\tLoss: 119.908356\n",
            "Train Epoch: 90 [560/1400 (40%)]\tLoss: 135.554184\n",
            "Train Epoch: 90 [600/1400 (43%)]\tLoss: 142.307816\n",
            "Train Epoch: 90 [640/1400 (46%)]\tLoss: 133.721420\n",
            "Train Epoch: 90 [680/1400 (49%)]\tLoss: 141.340302\n",
            "Train Epoch: 90 [720/1400 (51%)]\tLoss: 132.305130\n",
            "Train Epoch: 90 [760/1400 (54%)]\tLoss: 134.799957\n",
            "Train Epoch: 90 [800/1400 (57%)]\tLoss: 135.671906\n",
            "Train Epoch: 90 [840/1400 (60%)]\tLoss: 139.484711\n",
            "Train Epoch: 90 [880/1400 (63%)]\tLoss: 140.961639\n",
            "Train Epoch: 90 [920/1400 (66%)]\tLoss: 150.132172\n",
            "Train Epoch: 90 [960/1400 (69%)]\tLoss: 127.635178\n",
            "Train Epoch: 90 [1000/1400 (71%)]\tLoss: 155.046982\n",
            "Train Epoch: 90 [1040/1400 (74%)]\tLoss: 148.987671\n",
            "Train Epoch: 90 [1080/1400 (77%)]\tLoss: 178.545898\n",
            "Train Epoch: 90 [1120/1400 (80%)]\tLoss: 140.921585\n",
            "Train Epoch: 90 [1160/1400 (83%)]\tLoss: 140.429199\n",
            "Train Epoch: 90 [1200/1400 (86%)]\tLoss: 121.811493\n",
            "Train Epoch: 90 [1240/1400 (89%)]\tLoss: 131.432587\n",
            "Train Epoch: 90 [1280/1400 (91%)]\tLoss: 111.356804\n",
            "Train Epoch: 90 [1320/1400 (94%)]\tLoss: 131.698898\n",
            "Train Epoch: 90 [1360/1400 (97%)]\tLoss: 116.724701\n",
            "====> Epoch: 90 Average loss: 137.6129\n",
            "====> Test set loss: 141.0912\n",
            "Train Epoch: 91 [0/1400 (0%)]\tLoss: 129.919830\n",
            "Train Epoch: 91 [40/1400 (3%)]\tLoss: 142.434875\n",
            "Train Epoch: 91 [80/1400 (6%)]\tLoss: 130.340317\n",
            "Train Epoch: 91 [120/1400 (9%)]\tLoss: 144.111740\n",
            "Train Epoch: 91 [160/1400 (11%)]\tLoss: 128.158890\n",
            "Train Epoch: 91 [200/1400 (14%)]\tLoss: 131.459946\n",
            "Train Epoch: 91 [240/1400 (17%)]\tLoss: 139.359512\n",
            "Train Epoch: 91 [280/1400 (20%)]\tLoss: 121.108253\n",
            "Train Epoch: 91 [320/1400 (23%)]\tLoss: 145.385117\n",
            "Train Epoch: 91 [360/1400 (26%)]\tLoss: 111.538971\n",
            "Train Epoch: 91 [400/1400 (29%)]\tLoss: 160.790771\n",
            "Train Epoch: 91 [440/1400 (31%)]\tLoss: 120.241455\n",
            "Train Epoch: 91 [480/1400 (34%)]\tLoss: 124.688751\n",
            "Train Epoch: 91 [520/1400 (37%)]\tLoss: 139.327301\n",
            "Train Epoch: 91 [560/1400 (40%)]\tLoss: 139.449280\n",
            "Train Epoch: 91 [600/1400 (43%)]\tLoss: 142.097656\n",
            "Train Epoch: 91 [640/1400 (46%)]\tLoss: 130.996719\n",
            "Train Epoch: 91 [680/1400 (49%)]\tLoss: 139.327133\n",
            "Train Epoch: 91 [720/1400 (51%)]\tLoss: 125.458130\n",
            "Train Epoch: 91 [760/1400 (54%)]\tLoss: 133.703308\n",
            "Train Epoch: 91 [800/1400 (57%)]\tLoss: 141.876312\n",
            "Train Epoch: 91 [840/1400 (60%)]\tLoss: 145.011658\n",
            "Train Epoch: 91 [880/1400 (63%)]\tLoss: 125.211891\n",
            "Train Epoch: 91 [920/1400 (66%)]\tLoss: 137.860687\n",
            "Train Epoch: 91 [960/1400 (69%)]\tLoss: 154.117249\n",
            "Train Epoch: 91 [1000/1400 (71%)]\tLoss: 151.324081\n",
            "Train Epoch: 91 [1040/1400 (74%)]\tLoss: 147.290527\n",
            "Train Epoch: 91 [1080/1400 (77%)]\tLoss: 126.346710\n",
            "Train Epoch: 91 [1120/1400 (80%)]\tLoss: 133.709229\n",
            "Train Epoch: 91 [1160/1400 (83%)]\tLoss: 120.474243\n",
            "Train Epoch: 91 [1200/1400 (86%)]\tLoss: 164.787109\n",
            "Train Epoch: 91 [1240/1400 (89%)]\tLoss: 129.559479\n",
            "Train Epoch: 91 [1280/1400 (91%)]\tLoss: 141.283463\n",
            "Train Epoch: 91 [1320/1400 (94%)]\tLoss: 142.315033\n",
            "Train Epoch: 91 [1360/1400 (97%)]\tLoss: 130.808701\n",
            "====> Epoch: 91 Average loss: 137.5413\n",
            "====> Test set loss: 141.1225\n",
            "Train Epoch: 92 [0/1400 (0%)]\tLoss: 141.491913\n",
            "Train Epoch: 92 [40/1400 (3%)]\tLoss: 138.254639\n",
            "Train Epoch: 92 [80/1400 (6%)]\tLoss: 157.019821\n",
            "Train Epoch: 92 [120/1400 (9%)]\tLoss: 120.860146\n",
            "Train Epoch: 92 [160/1400 (11%)]\tLoss: 152.963623\n",
            "Train Epoch: 92 [200/1400 (14%)]\tLoss: 133.431763\n",
            "Train Epoch: 92 [240/1400 (17%)]\tLoss: 114.590324\n",
            "Train Epoch: 92 [280/1400 (20%)]\tLoss: 123.213921\n",
            "Train Epoch: 92 [320/1400 (23%)]\tLoss: 135.401642\n",
            "Train Epoch: 92 [360/1400 (26%)]\tLoss: 132.649139\n",
            "Train Epoch: 92 [400/1400 (29%)]\tLoss: 149.753113\n",
            "Train Epoch: 92 [440/1400 (31%)]\tLoss: 148.095886\n",
            "Train Epoch: 92 [480/1400 (34%)]\tLoss: 150.551361\n",
            "Train Epoch: 92 [520/1400 (37%)]\tLoss: 127.414093\n",
            "Train Epoch: 92 [560/1400 (40%)]\tLoss: 132.225739\n",
            "Train Epoch: 92 [600/1400 (43%)]\tLoss: 138.426651\n",
            "Train Epoch: 92 [640/1400 (46%)]\tLoss: 145.939682\n",
            "Train Epoch: 92 [680/1400 (49%)]\tLoss: 119.026947\n",
            "Train Epoch: 92 [720/1400 (51%)]\tLoss: 160.208801\n",
            "Train Epoch: 92 [760/1400 (54%)]\tLoss: 131.051682\n",
            "Train Epoch: 92 [800/1400 (57%)]\tLoss: 139.024353\n",
            "Train Epoch: 92 [840/1400 (60%)]\tLoss: 139.836227\n",
            "Train Epoch: 92 [880/1400 (63%)]\tLoss: 134.559647\n",
            "Train Epoch: 92 [920/1400 (66%)]\tLoss: 113.035439\n",
            "Train Epoch: 92 [960/1400 (69%)]\tLoss: 135.349854\n",
            "Train Epoch: 92 [1000/1400 (71%)]\tLoss: 142.801193\n",
            "Train Epoch: 92 [1040/1400 (74%)]\tLoss: 145.429718\n",
            "Train Epoch: 92 [1080/1400 (77%)]\tLoss: 131.023590\n",
            "Train Epoch: 92 [1120/1400 (80%)]\tLoss: 155.065765\n",
            "Train Epoch: 92 [1160/1400 (83%)]\tLoss: 142.246338\n",
            "Train Epoch: 92 [1200/1400 (86%)]\tLoss: 122.771507\n",
            "Train Epoch: 92 [1240/1400 (89%)]\tLoss: 128.855179\n",
            "Train Epoch: 92 [1280/1400 (91%)]\tLoss: 142.393692\n",
            "Train Epoch: 92 [1320/1400 (94%)]\tLoss: 122.763634\n",
            "Train Epoch: 92 [1360/1400 (97%)]\tLoss: 144.047546\n",
            "====> Epoch: 92 Average loss: 137.5480\n",
            "====> Test set loss: 140.9203\n",
            "Train Epoch: 93 [0/1400 (0%)]\tLoss: 144.284210\n",
            "Train Epoch: 93 [40/1400 (3%)]\tLoss: 144.609283\n",
            "Train Epoch: 93 [80/1400 (6%)]\tLoss: 145.750168\n",
            "Train Epoch: 93 [120/1400 (9%)]\tLoss: 117.461121\n",
            "Train Epoch: 93 [160/1400 (11%)]\tLoss: 139.549225\n",
            "Train Epoch: 93 [200/1400 (14%)]\tLoss: 145.315674\n",
            "Train Epoch: 93 [240/1400 (17%)]\tLoss: 147.043945\n",
            "Train Epoch: 93 [280/1400 (20%)]\tLoss: 137.044800\n",
            "Train Epoch: 93 [320/1400 (23%)]\tLoss: 133.836258\n",
            "Train Epoch: 93 [360/1400 (26%)]\tLoss: 123.583717\n",
            "Train Epoch: 93 [400/1400 (29%)]\tLoss: 140.916473\n",
            "Train Epoch: 93 [440/1400 (31%)]\tLoss: 147.777969\n",
            "Train Epoch: 93 [480/1400 (34%)]\tLoss: 153.535233\n",
            "Train Epoch: 93 [520/1400 (37%)]\tLoss: 138.730087\n",
            "Train Epoch: 93 [560/1400 (40%)]\tLoss: 147.315216\n",
            "Train Epoch: 93 [600/1400 (43%)]\tLoss: 128.079727\n",
            "Train Epoch: 93 [640/1400 (46%)]\tLoss: 145.591736\n",
            "Train Epoch: 93 [680/1400 (49%)]\tLoss: 150.974716\n",
            "Train Epoch: 93 [720/1400 (51%)]\tLoss: 141.580780\n",
            "Train Epoch: 93 [760/1400 (54%)]\tLoss: 146.591324\n",
            "Train Epoch: 93 [800/1400 (57%)]\tLoss: 142.441025\n",
            "Train Epoch: 93 [840/1400 (60%)]\tLoss: 135.129791\n",
            "Train Epoch: 93 [880/1400 (63%)]\tLoss: 136.773819\n",
            "Train Epoch: 93 [920/1400 (66%)]\tLoss: 103.175201\n",
            "Train Epoch: 93 [960/1400 (69%)]\tLoss: 143.369537\n",
            "Train Epoch: 93 [1000/1400 (71%)]\tLoss: 124.701027\n",
            "Train Epoch: 93 [1040/1400 (74%)]\tLoss: 125.316864\n",
            "Train Epoch: 93 [1080/1400 (77%)]\tLoss: 121.589096\n",
            "Train Epoch: 93 [1120/1400 (80%)]\tLoss: 121.739479\n",
            "Train Epoch: 93 [1160/1400 (83%)]\tLoss: 137.511414\n",
            "Train Epoch: 93 [1200/1400 (86%)]\tLoss: 151.933182\n",
            "Train Epoch: 93 [1240/1400 (89%)]\tLoss: 137.990997\n",
            "Train Epoch: 93 [1280/1400 (91%)]\tLoss: 144.562668\n",
            "Train Epoch: 93 [1320/1400 (94%)]\tLoss: 122.253754\n",
            "Train Epoch: 93 [1360/1400 (97%)]\tLoss: 133.838577\n",
            "====> Epoch: 93 Average loss: 137.5574\n",
            "====> Test set loss: 141.1270\n",
            "Train Epoch: 94 [0/1400 (0%)]\tLoss: 141.056396\n",
            "Train Epoch: 94 [40/1400 (3%)]\tLoss: 129.709229\n",
            "Train Epoch: 94 [80/1400 (6%)]\tLoss: 149.895309\n",
            "Train Epoch: 94 [120/1400 (9%)]\tLoss: 115.913834\n",
            "Train Epoch: 94 [160/1400 (11%)]\tLoss: 138.323669\n",
            "Train Epoch: 94 [200/1400 (14%)]\tLoss: 142.224243\n",
            "Train Epoch: 94 [240/1400 (17%)]\tLoss: 146.096802\n",
            "Train Epoch: 94 [280/1400 (20%)]\tLoss: 124.668709\n",
            "Train Epoch: 94 [320/1400 (23%)]\tLoss: 153.868500\n",
            "Train Epoch: 94 [360/1400 (26%)]\tLoss: 125.920975\n",
            "Train Epoch: 94 [400/1400 (29%)]\tLoss: 142.475601\n",
            "Train Epoch: 94 [440/1400 (31%)]\tLoss: 147.688354\n",
            "Train Epoch: 94 [480/1400 (34%)]\tLoss: 139.478439\n",
            "Train Epoch: 94 [520/1400 (37%)]\tLoss: 127.223763\n",
            "Train Epoch: 94 [560/1400 (40%)]\tLoss: 128.777649\n",
            "Train Epoch: 94 [600/1400 (43%)]\tLoss: 132.637695\n",
            "Train Epoch: 94 [640/1400 (46%)]\tLoss: 129.691360\n",
            "Train Epoch: 94 [680/1400 (49%)]\tLoss: 142.182663\n",
            "Train Epoch: 94 [720/1400 (51%)]\tLoss: 141.977783\n",
            "Train Epoch: 94 [760/1400 (54%)]\tLoss: 149.167786\n",
            "Train Epoch: 94 [800/1400 (57%)]\tLoss: 140.268097\n",
            "Train Epoch: 94 [840/1400 (60%)]\tLoss: 142.767242\n",
            "Train Epoch: 94 [880/1400 (63%)]\tLoss: 145.879257\n",
            "Train Epoch: 94 [920/1400 (66%)]\tLoss: 158.442673\n",
            "Train Epoch: 94 [960/1400 (69%)]\tLoss: 127.602493\n",
            "Train Epoch: 94 [1000/1400 (71%)]\tLoss: 152.196808\n",
            "Train Epoch: 94 [1040/1400 (74%)]\tLoss: 143.272537\n",
            "Train Epoch: 94 [1080/1400 (77%)]\tLoss: 130.216873\n",
            "Train Epoch: 94 [1120/1400 (80%)]\tLoss: 118.409012\n",
            "Train Epoch: 94 [1160/1400 (83%)]\tLoss: 137.051102\n",
            "Train Epoch: 94 [1200/1400 (86%)]\tLoss: 133.744110\n",
            "Train Epoch: 94 [1240/1400 (89%)]\tLoss: 128.841263\n",
            "Train Epoch: 94 [1280/1400 (91%)]\tLoss: 121.188240\n",
            "Train Epoch: 94 [1320/1400 (94%)]\tLoss: 129.504150\n",
            "Train Epoch: 94 [1360/1400 (97%)]\tLoss: 144.944000\n",
            "====> Epoch: 94 Average loss: 137.5422\n",
            "====> Test set loss: 141.1508\n",
            "Train Epoch: 95 [0/1400 (0%)]\tLoss: 137.993637\n",
            "Train Epoch: 95 [40/1400 (3%)]\tLoss: 147.475479\n",
            "Train Epoch: 95 [80/1400 (6%)]\tLoss: 146.194321\n",
            "Train Epoch: 95 [120/1400 (9%)]\tLoss: 137.980652\n",
            "Train Epoch: 95 [160/1400 (11%)]\tLoss: 130.575668\n",
            "Train Epoch: 95 [200/1400 (14%)]\tLoss: 131.296600\n",
            "Train Epoch: 95 [240/1400 (17%)]\tLoss: 147.840973\n",
            "Train Epoch: 95 [280/1400 (20%)]\tLoss: 167.671204\n",
            "Train Epoch: 95 [320/1400 (23%)]\tLoss: 147.221237\n",
            "Train Epoch: 95 [360/1400 (26%)]\tLoss: 128.078995\n",
            "Train Epoch: 95 [400/1400 (29%)]\tLoss: 153.429031\n",
            "Train Epoch: 95 [440/1400 (31%)]\tLoss: 132.226059\n",
            "Train Epoch: 95 [480/1400 (34%)]\tLoss: 131.981308\n",
            "Train Epoch: 95 [520/1400 (37%)]\tLoss: 147.832291\n",
            "Train Epoch: 95 [560/1400 (40%)]\tLoss: 148.596924\n",
            "Train Epoch: 95 [600/1400 (43%)]\tLoss: 132.771484\n",
            "Train Epoch: 95 [640/1400 (46%)]\tLoss: 126.336281\n",
            "Train Epoch: 95 [680/1400 (49%)]\tLoss: 163.302002\n",
            "Train Epoch: 95 [720/1400 (51%)]\tLoss: 122.000198\n",
            "Train Epoch: 95 [760/1400 (54%)]\tLoss: 140.419067\n",
            "Train Epoch: 95 [800/1400 (57%)]\tLoss: 131.813583\n",
            "Train Epoch: 95 [840/1400 (60%)]\tLoss: 123.962662\n",
            "Train Epoch: 95 [880/1400 (63%)]\tLoss: 141.808731\n",
            "Train Epoch: 95 [920/1400 (66%)]\tLoss: 118.499718\n",
            "Train Epoch: 95 [960/1400 (69%)]\tLoss: 125.632393\n",
            "Train Epoch: 95 [1000/1400 (71%)]\tLoss: 130.234482\n",
            "Train Epoch: 95 [1040/1400 (74%)]\tLoss: 137.546219\n",
            "Train Epoch: 95 [1080/1400 (77%)]\tLoss: 114.700485\n",
            "Train Epoch: 95 [1120/1400 (80%)]\tLoss: 127.223175\n",
            "Train Epoch: 95 [1160/1400 (83%)]\tLoss: 149.494812\n",
            "Train Epoch: 95 [1200/1400 (86%)]\tLoss: 124.420898\n",
            "Train Epoch: 95 [1240/1400 (89%)]\tLoss: 148.621796\n",
            "Train Epoch: 95 [1280/1400 (91%)]\tLoss: 128.396088\n",
            "Train Epoch: 95 [1320/1400 (94%)]\tLoss: 126.207825\n",
            "Train Epoch: 95 [1360/1400 (97%)]\tLoss: 146.704407\n",
            "====> Epoch: 95 Average loss: 137.4860\n",
            "====> Test set loss: 141.0991\n",
            "Train Epoch: 96 [0/1400 (0%)]\tLoss: 132.278564\n",
            "Train Epoch: 96 [40/1400 (3%)]\tLoss: 142.947739\n",
            "Train Epoch: 96 [80/1400 (6%)]\tLoss: 124.514847\n",
            "Train Epoch: 96 [120/1400 (9%)]\tLoss: 130.812332\n",
            "Train Epoch: 96 [160/1400 (11%)]\tLoss: 155.596176\n",
            "Train Epoch: 96 [200/1400 (14%)]\tLoss: 136.686096\n",
            "Train Epoch: 96 [240/1400 (17%)]\tLoss: 135.914322\n",
            "Train Epoch: 96 [280/1400 (20%)]\tLoss: 121.432602\n",
            "Train Epoch: 96 [320/1400 (23%)]\tLoss: 142.025528\n",
            "Train Epoch: 96 [360/1400 (26%)]\tLoss: 123.975014\n",
            "Train Epoch: 96 [400/1400 (29%)]\tLoss: 148.266144\n",
            "Train Epoch: 96 [440/1400 (31%)]\tLoss: 132.285828\n",
            "Train Epoch: 96 [480/1400 (34%)]\tLoss: 120.900673\n",
            "Train Epoch: 96 [520/1400 (37%)]\tLoss: 128.605515\n",
            "Train Epoch: 96 [560/1400 (40%)]\tLoss: 146.563232\n",
            "Train Epoch: 96 [600/1400 (43%)]\tLoss: 149.990067\n",
            "Train Epoch: 96 [640/1400 (46%)]\tLoss: 126.131737\n",
            "Train Epoch: 96 [680/1400 (49%)]\tLoss: 133.341354\n",
            "Train Epoch: 96 [720/1400 (51%)]\tLoss: 151.164108\n",
            "Train Epoch: 96 [760/1400 (54%)]\tLoss: 116.110382\n",
            "Train Epoch: 96 [800/1400 (57%)]\tLoss: 145.694626\n",
            "Train Epoch: 96 [840/1400 (60%)]\tLoss: 159.372986\n",
            "Train Epoch: 96 [880/1400 (63%)]\tLoss: 136.301041\n",
            "Train Epoch: 96 [920/1400 (66%)]\tLoss: 138.482910\n",
            "Train Epoch: 96 [960/1400 (69%)]\tLoss: 131.620361\n",
            "Train Epoch: 96 [1000/1400 (71%)]\tLoss: 148.100830\n",
            "Train Epoch: 96 [1040/1400 (74%)]\tLoss: 123.769936\n",
            "Train Epoch: 96 [1080/1400 (77%)]\tLoss: 135.773697\n",
            "Train Epoch: 96 [1120/1400 (80%)]\tLoss: 139.718506\n",
            "Train Epoch: 96 [1160/1400 (83%)]\tLoss: 126.634300\n",
            "Train Epoch: 96 [1200/1400 (86%)]\tLoss: 132.945587\n",
            "Train Epoch: 96 [1240/1400 (89%)]\tLoss: 146.246582\n",
            "Train Epoch: 96 [1280/1400 (91%)]\tLoss: 141.610504\n",
            "Train Epoch: 96 [1320/1400 (94%)]\tLoss: 117.517799\n",
            "Train Epoch: 96 [1360/1400 (97%)]\tLoss: 155.661591\n",
            "====> Epoch: 96 Average loss: 137.4002\n",
            "====> Test set loss: 141.1842\n",
            "Train Epoch: 97 [0/1400 (0%)]\tLoss: 149.138916\n",
            "Train Epoch: 97 [40/1400 (3%)]\tLoss: 127.192497\n",
            "Train Epoch: 97 [80/1400 (6%)]\tLoss: 126.360886\n",
            "Train Epoch: 97 [120/1400 (9%)]\tLoss: 156.075378\n",
            "Train Epoch: 97 [160/1400 (11%)]\tLoss: 172.760880\n",
            "Train Epoch: 97 [200/1400 (14%)]\tLoss: 151.420822\n",
            "Train Epoch: 97 [240/1400 (17%)]\tLoss: 139.144165\n",
            "Train Epoch: 97 [280/1400 (20%)]\tLoss: 133.081512\n",
            "Train Epoch: 97 [320/1400 (23%)]\tLoss: 147.133255\n",
            "Train Epoch: 97 [360/1400 (26%)]\tLoss: 135.305023\n",
            "Train Epoch: 97 [400/1400 (29%)]\tLoss: 124.643402\n",
            "Train Epoch: 97 [440/1400 (31%)]\tLoss: 121.041885\n",
            "Train Epoch: 97 [480/1400 (34%)]\tLoss: 139.261612\n",
            "Train Epoch: 97 [520/1400 (37%)]\tLoss: 159.543686\n",
            "Train Epoch: 97 [560/1400 (40%)]\tLoss: 139.279572\n",
            "Train Epoch: 97 [600/1400 (43%)]\tLoss: 129.486984\n",
            "Train Epoch: 97 [640/1400 (46%)]\tLoss: 143.292114\n",
            "Train Epoch: 97 [680/1400 (49%)]\tLoss: 135.674545\n",
            "Train Epoch: 97 [720/1400 (51%)]\tLoss: 153.649384\n",
            "Train Epoch: 97 [760/1400 (54%)]\tLoss: 125.668175\n",
            "Train Epoch: 97 [800/1400 (57%)]\tLoss: 137.949585\n",
            "Train Epoch: 97 [840/1400 (60%)]\tLoss: 131.600586\n",
            "Train Epoch: 97 [880/1400 (63%)]\tLoss: 112.407356\n",
            "Train Epoch: 97 [920/1400 (66%)]\tLoss: 122.516037\n",
            "Train Epoch: 97 [960/1400 (69%)]\tLoss: 138.717575\n",
            "Train Epoch: 97 [1000/1400 (71%)]\tLoss: 149.818436\n",
            "Train Epoch: 97 [1040/1400 (74%)]\tLoss: 146.558350\n",
            "Train Epoch: 97 [1080/1400 (77%)]\tLoss: 144.888611\n",
            "Train Epoch: 97 [1120/1400 (80%)]\tLoss: 128.417297\n",
            "Train Epoch: 97 [1160/1400 (83%)]\tLoss: 133.382645\n",
            "Train Epoch: 97 [1200/1400 (86%)]\tLoss: 131.269379\n",
            "Train Epoch: 97 [1240/1400 (89%)]\tLoss: 131.060196\n",
            "Train Epoch: 97 [1280/1400 (91%)]\tLoss: 142.080246\n",
            "Train Epoch: 97 [1320/1400 (94%)]\tLoss: 133.284058\n",
            "Train Epoch: 97 [1360/1400 (97%)]\tLoss: 129.820877\n",
            "====> Epoch: 97 Average loss: 137.4325\n",
            "====> Test set loss: 141.0291\n",
            "Train Epoch: 98 [0/1400 (0%)]\tLoss: 165.625427\n",
            "Train Epoch: 98 [40/1400 (3%)]\tLoss: 123.116730\n",
            "Train Epoch: 98 [80/1400 (6%)]\tLoss: 145.196167\n",
            "Train Epoch: 98 [120/1400 (9%)]\tLoss: 131.849182\n",
            "Train Epoch: 98 [160/1400 (11%)]\tLoss: 142.131958\n",
            "Train Epoch: 98 [200/1400 (14%)]\tLoss: 132.507660\n",
            "Train Epoch: 98 [240/1400 (17%)]\tLoss: 150.333466\n",
            "Train Epoch: 98 [280/1400 (20%)]\tLoss: 161.819656\n",
            "Train Epoch: 98 [320/1400 (23%)]\tLoss: 125.751747\n",
            "Train Epoch: 98 [360/1400 (26%)]\tLoss: 141.488403\n",
            "Train Epoch: 98 [400/1400 (29%)]\tLoss: 143.409332\n",
            "Train Epoch: 98 [440/1400 (31%)]\tLoss: 135.955841\n",
            "Train Epoch: 98 [480/1400 (34%)]\tLoss: 147.506577\n",
            "Train Epoch: 98 [520/1400 (37%)]\tLoss: 143.090912\n",
            "Train Epoch: 98 [560/1400 (40%)]\tLoss: 120.324936\n",
            "Train Epoch: 98 [600/1400 (43%)]\tLoss: 138.248901\n",
            "Train Epoch: 98 [640/1400 (46%)]\tLoss: 139.011780\n",
            "Train Epoch: 98 [680/1400 (49%)]\tLoss: 148.511948\n",
            "Train Epoch: 98 [720/1400 (51%)]\tLoss: 122.131256\n",
            "Train Epoch: 98 [760/1400 (54%)]\tLoss: 135.187042\n",
            "Train Epoch: 98 [800/1400 (57%)]\tLoss: 147.637192\n",
            "Train Epoch: 98 [840/1400 (60%)]\tLoss: 143.846283\n",
            "Train Epoch: 98 [880/1400 (63%)]\tLoss: 145.797531\n",
            "Train Epoch: 98 [920/1400 (66%)]\tLoss: 113.389481\n",
            "Train Epoch: 98 [960/1400 (69%)]\tLoss: 136.781067\n",
            "Train Epoch: 98 [1000/1400 (71%)]\tLoss: 147.564331\n",
            "Train Epoch: 98 [1040/1400 (74%)]\tLoss: 128.306702\n",
            "Train Epoch: 98 [1080/1400 (77%)]\tLoss: 159.512802\n",
            "Train Epoch: 98 [1120/1400 (80%)]\tLoss: 149.167953\n",
            "Train Epoch: 98 [1160/1400 (83%)]\tLoss: 152.147446\n",
            "Train Epoch: 98 [1200/1400 (86%)]\tLoss: 135.156662\n",
            "Train Epoch: 98 [1240/1400 (89%)]\tLoss: 135.512680\n",
            "Train Epoch: 98 [1280/1400 (91%)]\tLoss: 142.314209\n",
            "Train Epoch: 98 [1320/1400 (94%)]\tLoss: 133.176514\n",
            "Train Epoch: 98 [1360/1400 (97%)]\tLoss: 143.015915\n",
            "====> Epoch: 98 Average loss: 137.3627\n",
            "====> Test set loss: 140.9800\n",
            "Train Epoch: 99 [0/1400 (0%)]\tLoss: 130.426971\n",
            "Train Epoch: 99 [40/1400 (3%)]\tLoss: 128.818695\n",
            "Train Epoch: 99 [80/1400 (6%)]\tLoss: 164.064438\n",
            "Train Epoch: 99 [120/1400 (9%)]\tLoss: 120.999474\n",
            "Train Epoch: 99 [160/1400 (11%)]\tLoss: 114.676476\n",
            "Train Epoch: 99 [200/1400 (14%)]\tLoss: 131.914520\n",
            "Train Epoch: 99 [240/1400 (17%)]\tLoss: 143.950668\n",
            "Train Epoch: 99 [280/1400 (20%)]\tLoss: 140.892075\n",
            "Train Epoch: 99 [320/1400 (23%)]\tLoss: 114.669128\n",
            "Train Epoch: 99 [360/1400 (26%)]\tLoss: 130.478760\n",
            "Train Epoch: 99 [400/1400 (29%)]\tLoss: 132.414383\n",
            "Train Epoch: 99 [440/1400 (31%)]\tLoss: 147.767761\n",
            "Train Epoch: 99 [480/1400 (34%)]\tLoss: 151.580246\n",
            "Train Epoch: 99 [520/1400 (37%)]\tLoss: 125.941208\n",
            "Train Epoch: 99 [560/1400 (40%)]\tLoss: 134.516434\n",
            "Train Epoch: 99 [600/1400 (43%)]\tLoss: 146.269913\n",
            "Train Epoch: 99 [640/1400 (46%)]\tLoss: 131.171326\n",
            "Train Epoch: 99 [680/1400 (49%)]\tLoss: 118.329018\n",
            "Train Epoch: 99 [720/1400 (51%)]\tLoss: 134.188629\n",
            "Train Epoch: 99 [760/1400 (54%)]\tLoss: 144.886383\n",
            "Train Epoch: 99 [800/1400 (57%)]\tLoss: 131.690536\n",
            "Train Epoch: 99 [840/1400 (60%)]\tLoss: 133.742126\n",
            "Train Epoch: 99 [880/1400 (63%)]\tLoss: 139.573349\n",
            "Train Epoch: 99 [920/1400 (66%)]\tLoss: 128.736145\n",
            "Train Epoch: 99 [960/1400 (69%)]\tLoss: 150.861435\n",
            "Train Epoch: 99 [1000/1400 (71%)]\tLoss: 136.048492\n",
            "Train Epoch: 99 [1040/1400 (74%)]\tLoss: 133.353836\n",
            "Train Epoch: 99 [1080/1400 (77%)]\tLoss: 130.659683\n",
            "Train Epoch: 99 [1120/1400 (80%)]\tLoss: 135.425644\n",
            "Train Epoch: 99 [1160/1400 (83%)]\tLoss: 115.002823\n",
            "Train Epoch: 99 [1200/1400 (86%)]\tLoss: 130.734985\n",
            "Train Epoch: 99 [1240/1400 (89%)]\tLoss: 134.923019\n",
            "Train Epoch: 99 [1280/1400 (91%)]\tLoss: 136.032928\n",
            "Train Epoch: 99 [1320/1400 (94%)]\tLoss: 152.963699\n",
            "Train Epoch: 99 [1360/1400 (97%)]\tLoss: 131.378845\n",
            "====> Epoch: 99 Average loss: 137.3556\n",
            "====> Test set loss: 141.0394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "-6P65EyaV7wW",
        "outputId": "86ea9382-86f2-49e1-afa5-a252782c9aef"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# training loop\n",
        "\n",
        "\n",
        "# testing\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(val_losses) / 4 ,label=\"val\")\n",
        "plt.plot(np.array(train_losses) / 4, label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f899f266390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 601
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7nIOv6SERD"
      },
      "source": [
        "train_loader    = DataLoader(training_data, batch_size=1, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "cRO4y1VvC8RC",
        "outputId": "8006d421-15eb-4155-de91-930039bcd437"
      },
      "source": [
        "#data, y = next(iter(test_loader))\n",
        "recon_batch, mu, logvar = cvae(keepx.cuda(), keepy.cuda())\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(keepx[1])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8BoNgf3LVhcE",
        "outputId": "8e515643-26f6-4f18-cbd5-07577c57c8e4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(recon_batch.view(4, 27, 27)[1].cpu().detach().numpy())\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "-tsSxp6ATXJh",
        "outputId": "72048d24-770d-45e6-a9d1-c872f57f519d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "a = []\n",
        "c = y.clone().detach() #[[12.691395, 8.735944, 8.666785]]\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample = torch.randn(1, 10)\n",
        "    sample = cvae.decoder(sample, c)\n",
        "    sample = sample.view(1, 27, 27)[0]\n",
        "    a = sample\n",
        "\n",
        "    plt.imshow(sample)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96UY0mNDVcOd"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Prediction by ML, FCN, CNN\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO54CVQo83tX"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from torch.nn import functional as F\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaVHHaLf2ZcO"
      },
      "source": [
        "# flatten the matrix\n",
        "edge_flat = np.array(edge_index)\n",
        "edge_flat = edge_flat.reshape(1750, -1)\n",
        "\n",
        "# split\n",
        "x_train, x_test, y_train, y_test = train_test_split(edge_flat, label, test_size=0.15, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq2O3Wtr5OVO",
        "outputId": "90f1035c-3e9c-4d4c-e4db-813db3cf1203"
      },
      "source": [
        "reg = MultiOutputRegressor(GradientBoostingRegressor(), n_jobs=-1)\n",
        "reg.fit(x_train, y_train)\n",
        "print(reg.score(x_train, y_train))\n",
        "\n",
        "pred = reg.predict(x_test)\n",
        "\n",
        "F.l1_loss(torch.tensor(pred),y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6387636684688572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.9041, dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 623
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxeMsBQk88me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0e37dd-01cb-47d1-cc6f-0b6472bc1b22"
      },
      "source": [
        "torch.sqrt(F.mse_loss(torch.tensor(pred),y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.0929, dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 624
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuH-6W3O88o6",
        "outputId": "b435ddcd-dc1e-40ae-f136-3a7c1bef6018"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 512)\n",
        "        self.fc2 = nn.Linear(512, 230)\n",
        "        self.fc3 = nn.Linear(230, 182)\n",
        "        self.fc4 = nn.Linear(182, 128)\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "        self.fc6 = nn.Linear(64, 32)\n",
        "        self.fc7 = nn.Linear(32, 9)\n",
        "        self.fc8 = nn.Linear(9, 3)\n",
        "        \n",
        "\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        nn.init.xavier_uniform_(self.fc7.weight)\n",
        "        nn.init.xavier_uniform_(self.fc8.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.fc5(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.fc6(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.fc7(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        \n",
        "        \n",
        "        return self.fc8(x)\n",
        "\n",
        "\n",
        "model = DNN().cuda()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=230, bias=True)\n",
            "  (fc3): Linear(in_features=230, out_features=182, bias=True)\n",
            "  (fc4): Linear(in_features=182, out_features=128, bias=True)\n",
            "  (fc5): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc6): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc7): Linear(in_features=32, out_features=9, bias=True)\n",
            "  (fc8): Linear(in_features=9, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DloanETD-Hb9"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0EURXj1THiD",
        "outputId": "e1a5d81f-06fd-434d-9812-9e7327667ac8"
      },
      "source": [
        "x, y = next(iter(train_loader))\n",
        "x, y = x.cuda(), y.cuda()\n",
        "\n",
        "o = model(x)\n",
        "loss=loss_function(o, y)\n",
        "print(x.shape, o.shape, loss.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 27, 27]) torch.Size([4, 3]) torch.Size([]) torch.Size([4, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1mCAkAZV5aC"
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NznP4-ev88uo"
      },
      "source": [
        "def train(model, train_losses, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        o = model(x)\n",
        "        loss = loss_function(o, y)\n",
        "        \n",
        "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    print('====> Epoch: {} loss: {:.4f}'.format(e, train_loss / len(train_loader)))\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "def test(model, val_losses):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda().float(), y.cuda()       \n",
        "            o = model(x)\n",
        "            loss = loss_function(o, y)\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))\n",
        "    val_losses.append(test_loss / len(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9USolZP88rZ"
      },
      "source": [
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk1f6oL2bSNt",
        "outputId": "9476d286-a884-416a-adef-9628b6463769"
      },
      "source": [
        "x, y = next(iter(test_loader))\n",
        "o    = model(x.cuda())\n",
        "print(\"mae: \", F.l1_loss(o, y.cuda()))\n",
        "print(y[:], \"\\n\", o[:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mae:  tensor(2.9072, device='cuda:0', grad_fn=<L1LossBackward>)\n",
            "tensor([[20.4802, 13.7561,  9.1044],\n",
            "        [12.0543,  0.6352,  7.2949],\n",
            "        [12.8651, 11.9096,  8.6483],\n",
            "        [14.9428,  6.6858,  3.0543]]) \n",
            " tensor([[16.0255, 11.5789, 10.6445],\n",
            "        [12.1432,  5.3963, 10.7889],\n",
            "        [ 5.8671, 13.1397,  8.5982],\n",
            "        [19.0985, 10.6055,  5.0710]], device='cuda:0', grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFtTgtPPHbR2"
      },
      "source": [
        "#writer.flush()\n",
        "#writer.close()\n",
        "#!pip install torch torchvision\n",
        "#!pip install tensorboard\n",
        "#!pip install tensorboard --upgrade\n",
        "#!tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "IfuDEdB5a1gj",
        "outputId": "05b49f88-858e-4d27-aa49-03748375b4b4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# training loop\n",
        "\n",
        "\n",
        "# testing\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0c83e723d0>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOE9DHHvGaae"
      },
      "source": [
        "\n",
        "\n",
        "### CNN on Adjaceny matrix\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdGIPzsKGn7x",
        "outputId": "4d71daf7-cdd8-4cd8-e215-7ed8ae569fe1"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "use_cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2MLlK5OGoDU",
        "outputId": "c63eeac2-b4a5-45d2-f1db-1779ab5fbc66"
      },
      "source": [
        "class E2EBlock(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=False):\n",
        "        super(E2EBlock, self).__init__()\n",
        "        self.cnn1 = torch.nn.Conv2d(in_dim, out_dim, (1, 27), bias=bias)\n",
        "        self.cnn2 = torch.nn.Conv2d(in_dim, out_dim, (27, 1), bias=bias)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.cnn1.weight)\n",
        "        nn.init.xavier_uniform_(self.cnn2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.cnn1(x)\n",
        "        b = self.cnn2(x)\n",
        "        return torch.cat([a]*27, 3) + torch.cat([b]*27, 2)\n",
        "\n",
        "class BrainNetCNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BrainNetCNN, self).__init__()\n",
        "        \n",
        "        self.e2econv1 = E2EBlock(1, 16, bias=True)\n",
        "        self.conv1_bn = nn.BatchNorm2d(16)\n",
        "        self.e2econv2 = E2EBlock(16, 32, bias=True)\n",
        "        self.conv2_bn = nn.BatchNorm2d(32)\n",
        "        self.e2econv3 = E2EBlock(32, 64, bias=True)\n",
        "        self.conv3_bn = nn.BatchNorm2d(64)\n",
        "        self.e2econv4 = E2EBlock(64, 128, bias=True)\n",
        "        self.conv4_bn = nn.BatchNorm2d(128)\n",
        "        self.e2econv5 = E2EBlock(128, 256, bias=True)\n",
        "        self.conv5_bn = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.E2N = torch.nn.Conv2d(256, 64, (1, 27))\n",
        "        self.N2G = torch.nn.Conv2d(64, 729, (27, 1))\n",
        "\n",
        "        self.dense1 = torch.nn.Linear(729, 512)\n",
        "        self.dense1_bn = nn.BatchNorm1d(512)\n",
        "        self.dense2 = torch.nn.Linear(512, 256)\n",
        "        self.dense2_bn = nn.BatchNorm1d(256)\n",
        "        self.dense3 = torch.nn.Linear(256, 64)\n",
        "        self.dense3_bn = nn.BatchNorm1d(64)\n",
        "        self.dense4 = torch.nn.Linear(64, 16)\n",
        "        self.dense4_bn = nn.BatchNorm1d(16)\n",
        "        self.dense5 = torch.nn.Linear(16, 3)\n",
        "        \n",
        "\n",
        "        nn.init.xavier_uniform_(self.E2N.weight)\n",
        "        nn.init.xavier_uniform_(self.N2G.weight)\n",
        "        nn.init.xavier_uniform_(self.dense1.weight)\n",
        "        nn.init.xavier_uniform_(self.dense2.weight)\n",
        "        nn.init.xavier_uniform_(self.dense3.weight)   \n",
        "        nn.init.xavier_uniform_(self.dense4.weight)\n",
        "        nn.init.xavier_uniform_(self.dense5.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        out = F.relu(self.e2econv1(x))\n",
        "        out = self.conv1_bn(out)\n",
        "        out = F.relu(self.e2econv2(out))\n",
        "        out = self.conv2_bn(out)\n",
        "        out = F.relu(self.e2econv3(out))\n",
        "        out = self.conv3_bn(out)\n",
        "        out = F.relu(self.e2econv4(out))\n",
        "        out = self.conv4_bn(out)\n",
        "        out = F.relu(self.e2econv5(out))\n",
        "        out = self.conv5_bn(out)\n",
        "        #out = F.max_pool2d(out, 2)  # 적용한 마지막 output kernel size, kernel 영향 + 풀링 영향으로 줄어든 사이즈\n",
        "\n",
        "        out = F.relu(self.E2N(out))\n",
        "        out = F.relu(self.N2G(out))\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.dense1(out))\n",
        "        out = self.dense1_bn(out)\n",
        "        out = F.relu(self.dense2(out))\n",
        "        out = self.dense2_bn(out)\n",
        "        out = F.relu(self.dense3(out))\n",
        "        out = self.dense3_bn(out)\n",
        "        out = F.relu(self.dense4(out))\n",
        "        out = self.dense4_bn(out)\n",
        "\n",
        "        return self.dense5(out)\n",
        "\n",
        "model = BrainNetCNN().cuda()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BrainNetCNN(\n",
              "  (e2econv1): E2EBlock(\n",
              "    (cnn1): Conv2d(1, 16, kernel_size=(1, 27), stride=(1, 1))\n",
              "    (cnn2): Conv2d(1, 16, kernel_size=(27, 1), stride=(1, 1))\n",
              "  )\n",
              "  (conv1_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (e2econv2): E2EBlock(\n",
              "    (cnn1): Conv2d(16, 32, kernel_size=(1, 27), stride=(1, 1))\n",
              "    (cnn2): Conv2d(16, 32, kernel_size=(27, 1), stride=(1, 1))\n",
              "  )\n",
              "  (conv2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (e2econv3): E2EBlock(\n",
              "    (cnn1): Conv2d(32, 64, kernel_size=(1, 27), stride=(1, 1))\n",
              "    (cnn2): Conv2d(32, 64, kernel_size=(27, 1), stride=(1, 1))\n",
              "  )\n",
              "  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (e2econv4): E2EBlock(\n",
              "    (cnn1): Conv2d(64, 128, kernel_size=(1, 27), stride=(1, 1))\n",
              "    (cnn2): Conv2d(64, 128, kernel_size=(27, 1), stride=(1, 1))\n",
              "  )\n",
              "  (conv4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (e2econv5): E2EBlock(\n",
              "    (cnn1): Conv2d(128, 256, kernel_size=(1, 27), stride=(1, 1))\n",
              "    (cnn2): Conv2d(128, 256, kernel_size=(27, 1), stride=(1, 1))\n",
              "  )\n",
              "  (conv5_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (E2N): Conv2d(256, 64, kernel_size=(1, 27), stride=(1, 1))\n",
              "  (N2G): Conv2d(64, 729, kernel_size=(27, 1), stride=(1, 1))\n",
              "  (dense1): Linear(in_features=729, out_features=512, bias=True)\n",
              "  (dense1_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dense2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (dense2_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dense3): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (dense3_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dense4): Linear(in_features=64, out_features=16, bias=True)\n",
              "  (dense4_bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dense5): Linear(in_features=16, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 440
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKPZ1CrfGoOl"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-7tClFeGoUE"
      },
      "source": [
        "def train(model, train_losses, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        o = model(x)\n",
        "        loss = loss_function(o, y)\n",
        "        \n",
        "        #writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    print('====> Epoch: {} loss: {:.4f}'.format(e, train_loss / len(train_loader)))\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "def test(model, val_losses):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda().float(), y.cuda()       \n",
        "            o = model(x)\n",
        "            loss = loss_function(o, y)\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))\n",
        "    val_losses.append(test_loss / len(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpuc936wGoYw",
        "outputId": "603f6c2b-09e0-436d-cfb4-e48ba0bc5b9f"
      },
      "source": [
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 122.6827\n",
            "====> Test set loss: 104.0986\n",
            "====> Epoch: 2 loss: 118.2249\n",
            "====> Test set loss: 111.0464\n",
            "====> Epoch: 3 loss: 113.9889\n",
            "====> Test set loss: 114.8996\n",
            "====> Epoch: 4 loss: 108.9469\n",
            "====> Test set loss: 105.1876\n",
            "====> Epoch: 5 loss: 103.3735\n",
            "====> Test set loss: 100.0688\n",
            "====> Epoch: 6 loss: 98.7228\n",
            "====> Test set loss: 96.6087\n",
            "====> Epoch: 7 loss: 92.6836\n",
            "====> Test set loss: 92.1360\n",
            "====> Epoch: 8 loss: 88.7487\n",
            "====> Test set loss: 78.5460\n",
            "====> Epoch: 9 loss: 82.6384\n",
            "====> Test set loss: 73.0160\n",
            "====> Epoch: 10 loss: 76.5772\n",
            "====> Test set loss: 72.2264\n",
            "====> Epoch: 11 loss: 70.4618\n",
            "====> Test set loss: 62.9362\n",
            "====> Epoch: 12 loss: 64.5120\n",
            "====> Test set loss: 122.4833\n",
            "====> Epoch: 13 loss: 58.4345\n",
            "====> Test set loss: 59.5637\n",
            "====> Epoch: 14 loss: 51.3506\n",
            "====> Test set loss: 48.4533\n",
            "====> Epoch: 15 loss: 46.9381\n",
            "====> Test set loss: 42.0679\n",
            "====> Epoch: 16 loss: 42.9921\n",
            "====> Test set loss: 39.7580\n",
            "====> Epoch: 17 loss: 39.4568\n",
            "====> Test set loss: 35.7173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXqAyLmOLG-o",
        "outputId": "32382c2b-5b1d-457b-9083-5a4911cd6b77"
      },
      "source": [
        "x, y = next(iter(test_loader))\n",
        "o    = model(x.cuda())\n",
        "print(\"mae: \", F.l1_loss(o, y.cuda()))\n",
        "print(y[:], \"\\n\", o[:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mae:  tensor(3.8366, device='cuda:0', grad_fn=<L1LossBackward>)\n",
            "tensor([[20.4802, 13.7561,  9.1044],\n",
            "        [12.0543,  0.6352,  7.2949],\n",
            "        [12.8651, 11.9096,  8.6483],\n",
            "        [14.9428,  6.6858,  3.0543]]) \n",
            " tensor([[11.6477, 10.4337, 10.2797],\n",
            "        [ 6.2220,  5.2064,  9.6002],\n",
            "        [ 9.3058,  8.8468,  6.7479],\n",
            "        [ 9.2598,  8.2150,  7.3201]], device='cuda:0', grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_l8Yu-3LKFX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0UyjoEzEsbd"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### CNN\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVpA4MfVH1JX"
      },
      "source": [
        "train_loader    = DataLoader(training_data, batch_size=1, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8qULSQuIJnn",
        "outputId": "901b67ef-a56c-45e8-e8c7-5890514c42c2"
      },
      "source": [
        "x, y = next(iter(train_loader))\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 27, 27])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpwmpMHEQsWp"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "epochs = 50\n",
        "batch_idx = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8QDQ3-dFL5T",
        "outputId": "4680c14f-b9aa-4af2-f1f6-7446e925a0ae"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, 1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.fc1 = nn.Linear(32*12*12, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 3)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "       # x = F.relu(self.conv2(x))\n",
        "        #x = F.max_pool2d(x, 2)\n",
        "        #x = self.dropout1(x)\n",
        "        \n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "model = CNN().cuda()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (dropout1): Dropout(p=0.25, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuk_aSiPFZud",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d8cc01b-3ed9-47fe-c5d8-261c1501e2fd"
      },
      "source": [
        "def test(net):\n",
        "    net.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, y in test_loader:\n",
        "            data, y = data.cuda(), y.cuda()\n",
        "            \n",
        "            output = net(data.float())\n",
        "            loss = criterion(output, y)\n",
        "            test_loss += loss.item()\n",
        "            \n",
        "            #val_losses.append(loss_function(recon, data, mu, log_var).item())\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "# Train\n",
        "for e in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data, y in train_loader:\n",
        "        data, y = data.cuda(), y.cuda()\n",
        "\n",
        "        optimizer.zero_grad()   \n",
        "        output = model(data.float())\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        #if batch_idx % 10  == 0:\n",
        "        #print('Loss: {:.6f}'.format(loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(e, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "    test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n",
            "torch.Size([4, 27, 27])\n",
            "torch.Size([4, 1, 27, 27])\n",
            "torch.Size([4, 32, 25, 25])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 32, 12, 12])\n",
            "torch.Size([4, 4608])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-315-23442f5d9611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_zero_grad_profile_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_for_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_enter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zW7SALOSu1X"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# VAE + latent space optimization\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2FN36OeJou4"
      },
      "source": [
        "train_loader    = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=4, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDL20CYSS5G3"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1  = nn.Linear(729, 512)\n",
        "        self.fc2  = nn.Linear(512, 256)\n",
        "        self.fc3  = nn.Linear(256, 128)\n",
        "        self.fc4  = nn.Linear(128, 64)\n",
        "        \n",
        "        self.fc51  = nn.Linear(64, 5)\n",
        "        self.fc52  = nn.Linear(64, 5)\n",
        "\n",
        "        self.fc6  = nn.Linear(5, 64)\n",
        "        self.fc7  = nn.Linear(64, 128)\n",
        "        self.fc8  = nn.Linear(128, 256)\n",
        "        self.fc9  = nn.Linear(256, 512)\n",
        "        self.fc10  = nn.Linear(512, 729)\n",
        "\n",
        "        self.side_fc1 = nn.Linear(5, 16)\n",
        "        self.side_fc2 = nn.Linear(16, 32)\n",
        "        self.side_fc3 = nn.Linear(32, 32)\n",
        "        self.side_fc4 = nn.Linear(32, 16)\n",
        "        self.side_fc5 = nn.Linear(16, 8)\n",
        "        self.side_fc6 = nn.Linear(8, 3)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc51.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc52.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc7.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc8.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc9.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc10.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.side_fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.side_fc2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.side_fc3.weight)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        h1 = F.relu(self.fc2(h1))\n",
        "        h1 = F.relu(self.fc3(h1))\n",
        "        h1 = F.relu(self.fc4(h1))\n",
        "        return self.fc51(h1), self.fc52(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc6(z))\n",
        "        h3 = F.relu(self.fc7(h3))\n",
        "        h3 = F.relu(self.fc8(h3))\n",
        "        h3 = F.relu(self.fc9(h3))\n",
        "        return torch.sigmoid(self.fc10(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 729))\n",
        "\n",
        "        x = F.relu(self.side_fc1(mu))\n",
        "        x = F.relu(self.side_fc2(x))\n",
        "        x = F.relu(self.side_fc3(x))\n",
        "        x = F.relu(self.side_fc4(x))\n",
        "        x = F.relu(self.side_fc5(x))\n",
        "        pred = self.side_fc6(x)\n",
        "\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar, pred\n",
        "\n",
        "\n",
        "val_losses = []\n",
        "train_losses = []\n",
        "\n",
        "model = VAE().cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3spWwD6j3gRS",
        "outputId": "f708d18d-e344-4cd5-f991-7759d87c88bd"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (fc1): Linear(in_features=729, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc51): Linear(in_features=64, out_features=5, bias=True)\n",
              "  (fc52): Linear(in_features=64, out_features=5, bias=True)\n",
              "  (fc6): Linear(in_features=5, out_features=64, bias=True)\n",
              "  (fc7): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc8): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc9): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (fc10): Linear(in_features=512, out_features=729, bias=True)\n",
              "  (side_fc1): Linear(in_features=5, out_features=16, bias=True)\n",
              "  (side_fc2): Linear(in_features=16, out_features=32, bias=True)\n",
              "  (side_fc3): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (side_fc4): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (side_fc5): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (side_fc6): Linear(in_features=8, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 613
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml-VtL4LTjJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e456973d-131c-4880-ef2d-aff8d19766fe"
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 729), reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, y) in enumerate(train_loader):\n",
        "        data, y = data.cuda(), y.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar, pred = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar) + F.mse_loss(pred, y)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "d = []\n",
        "r = []\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, y) in enumerate(test_loader):\n",
        "            data, y = data.cuda(), y.cuda()\n",
        "\n",
        "            recon_batch, mu, logvar, pred = model(data)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item() + F.mse_loss(pred, y).item()\n",
        "\n",
        "            val_losses.append(loss_function(recon_batch, data, mu, logvar).item())\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 1)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(4, 27, 27)[:n]])\n",
        "                d.append(data)\n",
        "                r.append(recon_batch)\n",
        "\n",
        "                #save_image(comparison.cpu(),\n",
        "                #         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1400 (0%)]\tLoss: 517.777649\n",
            "Train Epoch: 1 [40/1400 (3%)]\tLoss: 532.662292\n",
            "Train Epoch: 1 [80/1400 (6%)]\tLoss: 543.317200\n",
            "Train Epoch: 1 [120/1400 (9%)]\tLoss: 514.298584\n",
            "Train Epoch: 1 [160/1400 (11%)]\tLoss: 518.490356\n",
            "Train Epoch: 1 [200/1400 (14%)]\tLoss: 518.435059\n",
            "Train Epoch: 1 [240/1400 (17%)]\tLoss: 552.592407\n",
            "Train Epoch: 1 [280/1400 (20%)]\tLoss: 525.216064\n",
            "Train Epoch: 1 [320/1400 (23%)]\tLoss: 510.213928\n",
            "Train Epoch: 1 [360/1400 (26%)]\tLoss: 559.365479\n",
            "Train Epoch: 1 [400/1400 (29%)]\tLoss: 522.075012\n",
            "Train Epoch: 1 [440/1400 (31%)]\tLoss: 540.719177\n",
            "Train Epoch: 1 [480/1400 (34%)]\tLoss: 507.727173\n",
            "Train Epoch: 1 [520/1400 (37%)]\tLoss: 508.146729\n",
            "Train Epoch: 1 [560/1400 (40%)]\tLoss: 525.140198\n",
            "Train Epoch: 1 [600/1400 (43%)]\tLoss: 517.356995\n",
            "Train Epoch: 1 [640/1400 (46%)]\tLoss: 535.984497\n",
            "Train Epoch: 1 [680/1400 (49%)]\tLoss: 526.020264\n",
            "Train Epoch: 1 [720/1400 (51%)]\tLoss: 502.527557\n",
            "Train Epoch: 1 [760/1400 (54%)]\tLoss: 513.635803\n",
            "Train Epoch: 1 [800/1400 (57%)]\tLoss: 492.192444\n",
            "Train Epoch: 1 [840/1400 (60%)]\tLoss: 533.949097\n",
            "Train Epoch: 1 [880/1400 (63%)]\tLoss: 514.836304\n",
            "Train Epoch: 1 [920/1400 (66%)]\tLoss: 488.865356\n",
            "Train Epoch: 1 [960/1400 (69%)]\tLoss: 514.849731\n",
            "Train Epoch: 1 [1000/1400 (71%)]\tLoss: 476.809723\n",
            "Train Epoch: 1 [1040/1400 (74%)]\tLoss: 512.087402\n",
            "Train Epoch: 1 [1080/1400 (77%)]\tLoss: 484.823547\n",
            "Train Epoch: 1 [1120/1400 (80%)]\tLoss: 505.363861\n",
            "Train Epoch: 1 [1160/1400 (83%)]\tLoss: 488.535095\n",
            "Train Epoch: 1 [1200/1400 (86%)]\tLoss: 464.201874\n",
            "Train Epoch: 1 [1240/1400 (89%)]\tLoss: 488.375793\n",
            "Train Epoch: 1 [1280/1400 (91%)]\tLoss: 431.245697\n",
            "Train Epoch: 1 [1320/1400 (94%)]\tLoss: 431.380066\n",
            "Train Epoch: 1 [1360/1400 (97%)]\tLoss: 416.568542\n",
            "====> Epoch: 1 Average loss: 507.3379\n",
            "====> Test set loss: 426.6422\n",
            "Train Epoch: 2 [0/1400 (0%)]\tLoss: 423.457031\n",
            "Train Epoch: 2 [40/1400 (3%)]\tLoss: 436.566040\n",
            "Train Epoch: 2 [80/1400 (6%)]\tLoss: 375.412140\n",
            "Train Epoch: 2 [120/1400 (9%)]\tLoss: 372.888489\n",
            "Train Epoch: 2 [160/1400 (11%)]\tLoss: 381.472992\n",
            "Train Epoch: 2 [200/1400 (14%)]\tLoss: 367.876892\n",
            "Train Epoch: 2 [240/1400 (17%)]\tLoss: 331.560822\n",
            "Train Epoch: 2 [280/1400 (20%)]\tLoss: 315.399689\n",
            "Train Epoch: 2 [320/1400 (23%)]\tLoss: 310.952057\n",
            "Train Epoch: 2 [360/1400 (26%)]\tLoss: 288.724396\n",
            "Train Epoch: 2 [400/1400 (29%)]\tLoss: 296.019928\n",
            "Train Epoch: 2 [440/1400 (31%)]\tLoss: 289.955566\n",
            "Train Epoch: 2 [480/1400 (34%)]\tLoss: 289.626251\n",
            "Train Epoch: 2 [520/1400 (37%)]\tLoss: 247.436737\n",
            "Train Epoch: 2 [560/1400 (40%)]\tLoss: 222.339600\n",
            "Train Epoch: 2 [600/1400 (43%)]\tLoss: 241.125854\n",
            "Train Epoch: 2 [640/1400 (46%)]\tLoss: 248.342377\n",
            "Train Epoch: 2 [680/1400 (49%)]\tLoss: 243.929626\n",
            "Train Epoch: 2 [720/1400 (51%)]\tLoss: 251.596359\n",
            "Train Epoch: 2 [760/1400 (54%)]\tLoss: 240.445831\n",
            "Train Epoch: 2 [800/1400 (57%)]\tLoss: 219.163681\n",
            "Train Epoch: 2 [840/1400 (60%)]\tLoss: 206.297760\n",
            "Train Epoch: 2 [880/1400 (63%)]\tLoss: 262.117462\n",
            "Train Epoch: 2 [920/1400 (66%)]\tLoss: 234.842346\n",
            "Train Epoch: 2 [960/1400 (69%)]\tLoss: 223.036377\n",
            "Train Epoch: 2 [1000/1400 (71%)]\tLoss: 241.030930\n",
            "Train Epoch: 2 [1040/1400 (74%)]\tLoss: 196.011688\n",
            "Train Epoch: 2 [1080/1400 (77%)]\tLoss: 211.166534\n",
            "Train Epoch: 2 [1120/1400 (80%)]\tLoss: 189.014664\n",
            "Train Epoch: 2 [1160/1400 (83%)]\tLoss: 208.089493\n",
            "Train Epoch: 2 [1200/1400 (86%)]\tLoss: 191.426208\n",
            "Train Epoch: 2 [1240/1400 (89%)]\tLoss: 245.677261\n",
            "Train Epoch: 2 [1280/1400 (91%)]\tLoss: 197.191284\n",
            "Train Epoch: 2 [1320/1400 (94%)]\tLoss: 213.450241\n",
            "Train Epoch: 2 [1360/1400 (97%)]\tLoss: 163.682281\n",
            "====> Epoch: 2 Average loss: 263.7811\n",
            "====> Test set loss: 209.2906\n",
            "Train Epoch: 3 [0/1400 (0%)]\tLoss: 216.274017\n",
            "Train Epoch: 3 [40/1400 (3%)]\tLoss: 216.156250\n",
            "Train Epoch: 3 [80/1400 (6%)]\tLoss: 240.744690\n",
            "Train Epoch: 3 [120/1400 (9%)]\tLoss: 222.850540\n",
            "Train Epoch: 3 [160/1400 (11%)]\tLoss: 258.736481\n",
            "Train Epoch: 3 [200/1400 (14%)]\tLoss: 211.446808\n",
            "Train Epoch: 3 [240/1400 (17%)]\tLoss: 172.924042\n",
            "Train Epoch: 3 [280/1400 (20%)]\tLoss: 229.607056\n",
            "Train Epoch: 3 [320/1400 (23%)]\tLoss: 223.378906\n",
            "Train Epoch: 3 [360/1400 (26%)]\tLoss: 159.470688\n",
            "Train Epoch: 3 [400/1400 (29%)]\tLoss: 252.338028\n",
            "Train Epoch: 3 [440/1400 (31%)]\tLoss: 262.842651\n",
            "Train Epoch: 3 [480/1400 (34%)]\tLoss: 157.358917\n",
            "Train Epoch: 3 [520/1400 (37%)]\tLoss: 265.700378\n",
            "Train Epoch: 3 [560/1400 (40%)]\tLoss: 149.854568\n",
            "Train Epoch: 3 [600/1400 (43%)]\tLoss: 202.229904\n",
            "Train Epoch: 3 [640/1400 (46%)]\tLoss: 175.090790\n",
            "Train Epoch: 3 [680/1400 (49%)]\tLoss: 191.334656\n",
            "Train Epoch: 3 [720/1400 (51%)]\tLoss: 184.236862\n",
            "Train Epoch: 3 [760/1400 (54%)]\tLoss: 155.346603\n",
            "Train Epoch: 3 [800/1400 (57%)]\tLoss: 187.492065\n",
            "Train Epoch: 3 [840/1400 (60%)]\tLoss: 200.812988\n",
            "Train Epoch: 3 [880/1400 (63%)]\tLoss: 203.522003\n",
            "Train Epoch: 3 [920/1400 (66%)]\tLoss: 177.435593\n",
            "Train Epoch: 3 [960/1400 (69%)]\tLoss: 181.501907\n",
            "Train Epoch: 3 [1000/1400 (71%)]\tLoss: 226.337219\n",
            "Train Epoch: 3 [1040/1400 (74%)]\tLoss: 193.265259\n",
            "Train Epoch: 3 [1080/1400 (77%)]\tLoss: 169.347473\n",
            "Train Epoch: 3 [1120/1400 (80%)]\tLoss: 159.754684\n",
            "Train Epoch: 3 [1160/1400 (83%)]\tLoss: 188.421448\n",
            "Train Epoch: 3 [1200/1400 (86%)]\tLoss: 204.368225\n",
            "Train Epoch: 3 [1240/1400 (89%)]\tLoss: 158.696838\n",
            "Train Epoch: 3 [1280/1400 (91%)]\tLoss: 172.313751\n",
            "Train Epoch: 3 [1320/1400 (94%)]\tLoss: 154.585510\n",
            "Train Epoch: 3 [1360/1400 (97%)]\tLoss: 167.214798\n",
            "====> Epoch: 3 Average loss: 196.8555\n",
            "====> Test set loss: 194.4104\n",
            "Train Epoch: 4 [0/1400 (0%)]\tLoss: 239.818054\n",
            "Train Epoch: 4 [40/1400 (3%)]\tLoss: 158.051498\n",
            "Train Epoch: 4 [80/1400 (6%)]\tLoss: 179.818115\n",
            "Train Epoch: 4 [120/1400 (9%)]\tLoss: 160.052979\n",
            "Train Epoch: 4 [160/1400 (11%)]\tLoss: 222.102402\n",
            "Train Epoch: 4 [200/1400 (14%)]\tLoss: 178.446655\n",
            "Train Epoch: 4 [240/1400 (17%)]\tLoss: 211.082047\n",
            "Train Epoch: 4 [280/1400 (20%)]\tLoss: 184.041824\n",
            "Train Epoch: 4 [320/1400 (23%)]\tLoss: 172.793091\n",
            "Train Epoch: 4 [360/1400 (26%)]\tLoss: 133.199081\n",
            "Train Epoch: 4 [400/1400 (29%)]\tLoss: 178.884872\n",
            "Train Epoch: 4 [440/1400 (31%)]\tLoss: 203.061020\n",
            "Train Epoch: 4 [480/1400 (34%)]\tLoss: 174.798233\n",
            "Train Epoch: 4 [520/1400 (37%)]\tLoss: 169.316925\n",
            "Train Epoch: 4 [560/1400 (40%)]\tLoss: 198.961044\n",
            "Train Epoch: 4 [600/1400 (43%)]\tLoss: 164.633484\n",
            "Train Epoch: 4 [640/1400 (46%)]\tLoss: 168.015411\n",
            "Train Epoch: 4 [680/1400 (49%)]\tLoss: 174.196472\n",
            "Train Epoch: 4 [720/1400 (51%)]\tLoss: 164.726883\n",
            "Train Epoch: 4 [760/1400 (54%)]\tLoss: 262.061920\n",
            "Train Epoch: 4 [800/1400 (57%)]\tLoss: 172.751602\n",
            "Train Epoch: 4 [840/1400 (60%)]\tLoss: 188.643463\n",
            "Train Epoch: 4 [880/1400 (63%)]\tLoss: 207.102448\n",
            "Train Epoch: 4 [920/1400 (66%)]\tLoss: 215.639099\n",
            "Train Epoch: 4 [960/1400 (69%)]\tLoss: 224.877838\n",
            "Train Epoch: 4 [1000/1400 (71%)]\tLoss: 175.411514\n",
            "Train Epoch: 4 [1040/1400 (74%)]\tLoss: 174.228165\n",
            "Train Epoch: 4 [1080/1400 (77%)]\tLoss: 249.502838\n",
            "Train Epoch: 4 [1120/1400 (80%)]\tLoss: 148.326050\n",
            "Train Epoch: 4 [1160/1400 (83%)]\tLoss: 201.077301\n",
            "Train Epoch: 4 [1200/1400 (86%)]\tLoss: 207.514786\n",
            "Train Epoch: 4 [1240/1400 (89%)]\tLoss: 210.845840\n",
            "Train Epoch: 4 [1280/1400 (91%)]\tLoss: 191.481079\n",
            "Train Epoch: 4 [1320/1400 (94%)]\tLoss: 153.454605\n",
            "Train Epoch: 4 [1360/1400 (97%)]\tLoss: 162.587616\n",
            "====> Epoch: 4 Average loss: 189.6362\n",
            "====> Test set loss: 188.8460\n",
            "Train Epoch: 5 [0/1400 (0%)]\tLoss: 204.712814\n",
            "Train Epoch: 5 [40/1400 (3%)]\tLoss: 170.202560\n",
            "Train Epoch: 5 [80/1400 (6%)]\tLoss: 207.982559\n",
            "Train Epoch: 5 [120/1400 (9%)]\tLoss: 144.564545\n",
            "Train Epoch: 5 [160/1400 (11%)]\tLoss: 163.710739\n",
            "Train Epoch: 5 [200/1400 (14%)]\tLoss: 162.006287\n",
            "Train Epoch: 5 [240/1400 (17%)]\tLoss: 184.335327\n",
            "Train Epoch: 5 [280/1400 (20%)]\tLoss: 156.622177\n",
            "Train Epoch: 5 [320/1400 (23%)]\tLoss: 212.485657\n",
            "Train Epoch: 5 [360/1400 (26%)]\tLoss: 171.326721\n",
            "Train Epoch: 5 [400/1400 (29%)]\tLoss: 174.480606\n",
            "Train Epoch: 5 [440/1400 (31%)]\tLoss: 170.677429\n",
            "Train Epoch: 5 [480/1400 (34%)]\tLoss: 177.970169\n",
            "Train Epoch: 5 [520/1400 (37%)]\tLoss: 223.394363\n",
            "Train Epoch: 5 [560/1400 (40%)]\tLoss: 157.118423\n",
            "Train Epoch: 5 [600/1400 (43%)]\tLoss: 136.675659\n",
            "Train Epoch: 5 [640/1400 (46%)]\tLoss: 179.644379\n",
            "Train Epoch: 5 [680/1400 (49%)]\tLoss: 178.660446\n",
            "Train Epoch: 5 [720/1400 (51%)]\tLoss: 188.210129\n",
            "Train Epoch: 5 [760/1400 (54%)]\tLoss: 156.138687\n",
            "Train Epoch: 5 [800/1400 (57%)]\tLoss: 180.092621\n",
            "Train Epoch: 5 [840/1400 (60%)]\tLoss: 184.001099\n",
            "Train Epoch: 5 [880/1400 (63%)]\tLoss: 170.209091\n",
            "Train Epoch: 5 [920/1400 (66%)]\tLoss: 196.232559\n",
            "Train Epoch: 5 [960/1400 (69%)]\tLoss: 182.597824\n",
            "Train Epoch: 5 [1000/1400 (71%)]\tLoss: 213.713486\n",
            "Train Epoch: 5 [1040/1400 (74%)]\tLoss: 175.017975\n",
            "Train Epoch: 5 [1080/1400 (77%)]\tLoss: 151.767426\n",
            "Train Epoch: 5 [1120/1400 (80%)]\tLoss: 185.371201\n",
            "Train Epoch: 5 [1160/1400 (83%)]\tLoss: 165.918701\n",
            "Train Epoch: 5 [1200/1400 (86%)]\tLoss: 161.521606\n",
            "Train Epoch: 5 [1240/1400 (89%)]\tLoss: 171.963501\n",
            "Train Epoch: 5 [1280/1400 (91%)]\tLoss: 230.080490\n",
            "Train Epoch: 5 [1320/1400 (94%)]\tLoss: 165.529724\n",
            "Train Epoch: 5 [1360/1400 (97%)]\tLoss: 189.845444\n",
            "====> Epoch: 5 Average loss: 184.8286\n",
            "====> Test set loss: 186.9860\n",
            "Train Epoch: 6 [0/1400 (0%)]\tLoss: 191.489380\n",
            "Train Epoch: 6 [40/1400 (3%)]\tLoss: 163.717743\n",
            "Train Epoch: 6 [80/1400 (6%)]\tLoss: 166.456192\n",
            "Train Epoch: 6 [120/1400 (9%)]\tLoss: 194.570709\n",
            "Train Epoch: 6 [160/1400 (11%)]\tLoss: 172.507172\n",
            "Train Epoch: 6 [200/1400 (14%)]\tLoss: 153.049835\n",
            "Train Epoch: 6 [240/1400 (17%)]\tLoss: 228.552673\n",
            "Train Epoch: 6 [280/1400 (20%)]\tLoss: 234.070404\n",
            "Train Epoch: 6 [320/1400 (23%)]\tLoss: 158.717285\n",
            "Train Epoch: 6 [360/1400 (26%)]\tLoss: 163.722733\n",
            "Train Epoch: 6 [400/1400 (29%)]\tLoss: 175.806946\n",
            "Train Epoch: 6 [440/1400 (31%)]\tLoss: 147.777618\n",
            "Train Epoch: 6 [480/1400 (34%)]\tLoss: 172.687439\n",
            "Train Epoch: 6 [520/1400 (37%)]\tLoss: 197.282364\n",
            "Train Epoch: 6 [560/1400 (40%)]\tLoss: 173.765579\n",
            "Train Epoch: 6 [600/1400 (43%)]\tLoss: 164.828644\n",
            "Train Epoch: 6 [640/1400 (46%)]\tLoss: 140.468552\n",
            "Train Epoch: 6 [680/1400 (49%)]\tLoss: 210.163757\n",
            "Train Epoch: 6 [720/1400 (51%)]\tLoss: 152.555374\n",
            "Train Epoch: 6 [760/1400 (54%)]\tLoss: 306.821198\n",
            "Train Epoch: 6 [800/1400 (57%)]\tLoss: 183.738022\n",
            "Train Epoch: 6 [840/1400 (60%)]\tLoss: 168.254562\n",
            "Train Epoch: 6 [880/1400 (63%)]\tLoss: 219.379883\n",
            "Train Epoch: 6 [920/1400 (66%)]\tLoss: 179.435425\n",
            "Train Epoch: 6 [960/1400 (69%)]\tLoss: 166.535797\n",
            "Train Epoch: 6 [1000/1400 (71%)]\tLoss: 189.016739\n",
            "Train Epoch: 6 [1040/1400 (74%)]\tLoss: 187.317596\n",
            "Train Epoch: 6 [1080/1400 (77%)]\tLoss: 144.122055\n",
            "Train Epoch: 6 [1120/1400 (80%)]\tLoss: 162.132355\n",
            "Train Epoch: 6 [1160/1400 (83%)]\tLoss: 194.782104\n",
            "Train Epoch: 6 [1200/1400 (86%)]\tLoss: 199.752808\n",
            "Train Epoch: 6 [1240/1400 (89%)]\tLoss: 196.918137\n",
            "Train Epoch: 6 [1280/1400 (91%)]\tLoss: 179.991226\n",
            "Train Epoch: 6 [1320/1400 (94%)]\tLoss: 184.816895\n",
            "Train Epoch: 6 [1360/1400 (97%)]\tLoss: 172.146362\n",
            "====> Epoch: 6 Average loss: 182.3525\n",
            "====> Test set loss: 183.6361\n",
            "Train Epoch: 7 [0/1400 (0%)]\tLoss: 166.992905\n",
            "Train Epoch: 7 [40/1400 (3%)]\tLoss: 173.865219\n",
            "Train Epoch: 7 [80/1400 (6%)]\tLoss: 199.415466\n",
            "Train Epoch: 7 [120/1400 (9%)]\tLoss: 137.933411\n",
            "Train Epoch: 7 [160/1400 (11%)]\tLoss: 160.143661\n",
            "Train Epoch: 7 [200/1400 (14%)]\tLoss: 153.768631\n",
            "Train Epoch: 7 [240/1400 (17%)]\tLoss: 163.214325\n",
            "Train Epoch: 7 [280/1400 (20%)]\tLoss: 182.610321\n",
            "Train Epoch: 7 [320/1400 (23%)]\tLoss: 222.275208\n",
            "Train Epoch: 7 [360/1400 (26%)]\tLoss: 233.414368\n",
            "Train Epoch: 7 [400/1400 (29%)]\tLoss: 177.341049\n",
            "Train Epoch: 7 [440/1400 (31%)]\tLoss: 185.019714\n",
            "Train Epoch: 7 [480/1400 (34%)]\tLoss: 194.961685\n",
            "Train Epoch: 7 [520/1400 (37%)]\tLoss: 170.673325\n",
            "Train Epoch: 7 [560/1400 (40%)]\tLoss: 216.236786\n",
            "Train Epoch: 7 [600/1400 (43%)]\tLoss: 206.366562\n",
            "Train Epoch: 7 [640/1400 (46%)]\tLoss: 146.974030\n",
            "Train Epoch: 7 [680/1400 (49%)]\tLoss: 194.056976\n",
            "Train Epoch: 7 [720/1400 (51%)]\tLoss: 164.231079\n",
            "Train Epoch: 7 [760/1400 (54%)]\tLoss: 142.198242\n",
            "Train Epoch: 7 [800/1400 (57%)]\tLoss: 199.202560\n",
            "Train Epoch: 7 [840/1400 (60%)]\tLoss: 183.612122\n",
            "Train Epoch: 7 [880/1400 (63%)]\tLoss: 129.202988\n",
            "Train Epoch: 7 [920/1400 (66%)]\tLoss: 155.395142\n",
            "Train Epoch: 7 [960/1400 (69%)]\tLoss: 175.393021\n",
            "Train Epoch: 7 [1000/1400 (71%)]\tLoss: 221.547226\n",
            "Train Epoch: 7 [1040/1400 (74%)]\tLoss: 229.387955\n",
            "Train Epoch: 7 [1080/1400 (77%)]\tLoss: 175.437607\n",
            "Train Epoch: 7 [1120/1400 (80%)]\tLoss: 159.376770\n",
            "Train Epoch: 7 [1160/1400 (83%)]\tLoss: 165.811432\n",
            "Train Epoch: 7 [1200/1400 (86%)]\tLoss: 141.341873\n",
            "Train Epoch: 7 [1240/1400 (89%)]\tLoss: 200.536331\n",
            "Train Epoch: 7 [1280/1400 (91%)]\tLoss: 152.123932\n",
            "Train Epoch: 7 [1320/1400 (94%)]\tLoss: 156.550232\n",
            "Train Epoch: 7 [1360/1400 (97%)]\tLoss: 216.011627\n",
            "====> Epoch: 7 Average loss: 179.4963\n",
            "====> Test set loss: 182.1120\n",
            "Train Epoch: 8 [0/1400 (0%)]\tLoss: 121.855782\n",
            "Train Epoch: 8 [40/1400 (3%)]\tLoss: 266.724640\n",
            "Train Epoch: 8 [80/1400 (6%)]\tLoss: 153.418533\n",
            "Train Epoch: 8 [120/1400 (9%)]\tLoss: 210.675568\n",
            "Train Epoch: 8 [160/1400 (11%)]\tLoss: 203.154602\n",
            "Train Epoch: 8 [200/1400 (14%)]\tLoss: 249.412445\n",
            "Train Epoch: 8 [240/1400 (17%)]\tLoss: 208.556412\n",
            "Train Epoch: 8 [280/1400 (20%)]\tLoss: 192.161697\n",
            "Train Epoch: 8 [320/1400 (23%)]\tLoss: 153.041153\n",
            "Train Epoch: 8 [360/1400 (26%)]\tLoss: 166.327072\n",
            "Train Epoch: 8 [400/1400 (29%)]\tLoss: 300.367706\n",
            "Train Epoch: 8 [440/1400 (31%)]\tLoss: 166.286438\n",
            "Train Epoch: 8 [480/1400 (34%)]\tLoss: 132.222061\n",
            "Train Epoch: 8 [520/1400 (37%)]\tLoss: 171.859558\n",
            "Train Epoch: 8 [560/1400 (40%)]\tLoss: 168.839630\n",
            "Train Epoch: 8 [600/1400 (43%)]\tLoss: 152.275833\n",
            "Train Epoch: 8 [640/1400 (46%)]\tLoss: 197.042892\n",
            "Train Epoch: 8 [680/1400 (49%)]\tLoss: 190.979141\n",
            "Train Epoch: 8 [720/1400 (51%)]\tLoss: 197.150558\n",
            "Train Epoch: 8 [760/1400 (54%)]\tLoss: 186.800659\n",
            "Train Epoch: 8 [800/1400 (57%)]\tLoss: 136.102097\n",
            "Train Epoch: 8 [840/1400 (60%)]\tLoss: 153.822922\n",
            "Train Epoch: 8 [880/1400 (63%)]\tLoss: 131.358246\n",
            "Train Epoch: 8 [920/1400 (66%)]\tLoss: 215.736084\n",
            "Train Epoch: 8 [960/1400 (69%)]\tLoss: 187.647858\n",
            "Train Epoch: 8 [1000/1400 (71%)]\tLoss: 174.837082\n",
            "Train Epoch: 8 [1040/1400 (74%)]\tLoss: 189.915497\n",
            "Train Epoch: 8 [1080/1400 (77%)]\tLoss: 168.003967\n",
            "Train Epoch: 8 [1120/1400 (80%)]\tLoss: 197.873383\n",
            "Train Epoch: 8 [1160/1400 (83%)]\tLoss: 147.078598\n",
            "Train Epoch: 8 [1200/1400 (86%)]\tLoss: 191.502014\n",
            "Train Epoch: 8 [1240/1400 (89%)]\tLoss: 186.644104\n",
            "Train Epoch: 8 [1280/1400 (91%)]\tLoss: 243.087677\n",
            "Train Epoch: 8 [1320/1400 (94%)]\tLoss: 172.819656\n",
            "Train Epoch: 8 [1360/1400 (97%)]\tLoss: 208.280640\n",
            "====> Epoch: 8 Average loss: 177.8410\n",
            "====> Test set loss: 179.7445\n",
            "Train Epoch: 9 [0/1400 (0%)]\tLoss: 198.046616\n",
            "Train Epoch: 9 [40/1400 (3%)]\tLoss: 145.934631\n",
            "Train Epoch: 9 [80/1400 (6%)]\tLoss: 147.415970\n",
            "Train Epoch: 9 [120/1400 (9%)]\tLoss: 185.330048\n",
            "Train Epoch: 9 [160/1400 (11%)]\tLoss: 195.071091\n",
            "Train Epoch: 9 [200/1400 (14%)]\tLoss: 207.623535\n",
            "Train Epoch: 9 [240/1400 (17%)]\tLoss: 194.718201\n",
            "Train Epoch: 9 [280/1400 (20%)]\tLoss: 161.964767\n",
            "Train Epoch: 9 [320/1400 (23%)]\tLoss: 175.669205\n",
            "Train Epoch: 9 [360/1400 (26%)]\tLoss: 163.877594\n",
            "Train Epoch: 9 [400/1400 (29%)]\tLoss: 204.493378\n",
            "Train Epoch: 9 [440/1400 (31%)]\tLoss: 178.623367\n",
            "Train Epoch: 9 [480/1400 (34%)]\tLoss: 173.825668\n",
            "Train Epoch: 9 [520/1400 (37%)]\tLoss: 168.416229\n",
            "Train Epoch: 9 [560/1400 (40%)]\tLoss: 259.989197\n",
            "Train Epoch: 9 [600/1400 (43%)]\tLoss: 166.313232\n",
            "Train Epoch: 9 [640/1400 (46%)]\tLoss: 209.908783\n",
            "Train Epoch: 9 [680/1400 (49%)]\tLoss: 159.484161\n",
            "Train Epoch: 9 [720/1400 (51%)]\tLoss: 179.938141\n",
            "Train Epoch: 9 [760/1400 (54%)]\tLoss: 213.497589\n",
            "Train Epoch: 9 [800/1400 (57%)]\tLoss: 180.814545\n",
            "Train Epoch: 9 [840/1400 (60%)]\tLoss: 215.802689\n",
            "Train Epoch: 9 [880/1400 (63%)]\tLoss: 223.974899\n",
            "Train Epoch: 9 [920/1400 (66%)]\tLoss: 216.276917\n",
            "Train Epoch: 9 [960/1400 (69%)]\tLoss: 184.829056\n",
            "Train Epoch: 9 [1000/1400 (71%)]\tLoss: 163.850677\n",
            "Train Epoch: 9 [1040/1400 (74%)]\tLoss: 147.872833\n",
            "Train Epoch: 9 [1080/1400 (77%)]\tLoss: 204.713470\n",
            "Train Epoch: 9 [1120/1400 (80%)]\tLoss: 138.538818\n",
            "Train Epoch: 9 [1160/1400 (83%)]\tLoss: 173.863190\n",
            "Train Epoch: 9 [1200/1400 (86%)]\tLoss: 211.419220\n",
            "Train Epoch: 9 [1240/1400 (89%)]\tLoss: 167.115662\n",
            "Train Epoch: 9 [1280/1400 (91%)]\tLoss: 170.178207\n",
            "Train Epoch: 9 [1320/1400 (94%)]\tLoss: 158.940277\n",
            "Train Epoch: 9 [1360/1400 (97%)]\tLoss: 156.729233\n",
            "====> Epoch: 9 Average loss: 176.5257\n",
            "====> Test set loss: 178.6364\n",
            "Train Epoch: 10 [0/1400 (0%)]\tLoss: 168.274261\n",
            "Train Epoch: 10 [40/1400 (3%)]\tLoss: 178.292633\n",
            "Train Epoch: 10 [80/1400 (6%)]\tLoss: 161.903534\n",
            "Train Epoch: 10 [120/1400 (9%)]\tLoss: 140.479065\n",
            "Train Epoch: 10 [160/1400 (11%)]\tLoss: 179.649048\n",
            "Train Epoch: 10 [200/1400 (14%)]\tLoss: 168.157333\n",
            "Train Epoch: 10 [240/1400 (17%)]\tLoss: 191.072845\n",
            "Train Epoch: 10 [280/1400 (20%)]\tLoss: 192.221588\n",
            "Train Epoch: 10 [320/1400 (23%)]\tLoss: 194.535492\n",
            "Train Epoch: 10 [360/1400 (26%)]\tLoss: 148.174759\n",
            "Train Epoch: 10 [400/1400 (29%)]\tLoss: 182.341278\n",
            "Train Epoch: 10 [440/1400 (31%)]\tLoss: 159.527039\n",
            "Train Epoch: 10 [480/1400 (34%)]\tLoss: 162.377213\n",
            "Train Epoch: 10 [520/1400 (37%)]\tLoss: 178.177399\n",
            "Train Epoch: 10 [560/1400 (40%)]\tLoss: 159.193176\n",
            "Train Epoch: 10 [600/1400 (43%)]\tLoss: 168.082321\n",
            "Train Epoch: 10 [640/1400 (46%)]\tLoss: 152.105667\n",
            "Train Epoch: 10 [680/1400 (49%)]\tLoss: 163.845062\n",
            "Train Epoch: 10 [720/1400 (51%)]\tLoss: 222.620941\n",
            "Train Epoch: 10 [760/1400 (54%)]\tLoss: 213.578461\n",
            "Train Epoch: 10 [800/1400 (57%)]\tLoss: 144.759735\n",
            "Train Epoch: 10 [840/1400 (60%)]\tLoss: 221.692535\n",
            "Train Epoch: 10 [880/1400 (63%)]\tLoss: 187.803513\n",
            "Train Epoch: 10 [920/1400 (66%)]\tLoss: 174.729736\n",
            "Train Epoch: 10 [960/1400 (69%)]\tLoss: 160.798050\n",
            "Train Epoch: 10 [1000/1400 (71%)]\tLoss: 197.131470\n",
            "Train Epoch: 10 [1040/1400 (74%)]\tLoss: 158.144379\n",
            "Train Epoch: 10 [1080/1400 (77%)]\tLoss: 212.649216\n",
            "Train Epoch: 10 [1120/1400 (80%)]\tLoss: 166.389053\n",
            "Train Epoch: 10 [1160/1400 (83%)]\tLoss: 190.821655\n",
            "Train Epoch: 10 [1200/1400 (86%)]\tLoss: 153.902222\n",
            "Train Epoch: 10 [1240/1400 (89%)]\tLoss: 186.476318\n",
            "Train Epoch: 10 [1280/1400 (91%)]\tLoss: 180.392456\n",
            "Train Epoch: 10 [1320/1400 (94%)]\tLoss: 153.784119\n",
            "Train Epoch: 10 [1360/1400 (97%)]\tLoss: 133.285782\n",
            "====> Epoch: 10 Average loss: 175.1770\n",
            "====> Test set loss: 178.2326\n",
            "Train Epoch: 11 [0/1400 (0%)]\tLoss: 245.876938\n",
            "Train Epoch: 11 [40/1400 (3%)]\tLoss: 138.965912\n",
            "Train Epoch: 11 [80/1400 (6%)]\tLoss: 132.887253\n",
            "Train Epoch: 11 [120/1400 (9%)]\tLoss: 127.608055\n",
            "Train Epoch: 11 [160/1400 (11%)]\tLoss: 195.342712\n",
            "Train Epoch: 11 [200/1400 (14%)]\tLoss: 154.180664\n",
            "Train Epoch: 11 [240/1400 (17%)]\tLoss: 163.128586\n",
            "Train Epoch: 11 [280/1400 (20%)]\tLoss: 174.397278\n",
            "Train Epoch: 11 [320/1400 (23%)]\tLoss: 190.703888\n",
            "Train Epoch: 11 [360/1400 (26%)]\tLoss: 152.775818\n",
            "Train Epoch: 11 [400/1400 (29%)]\tLoss: 163.360168\n",
            "Train Epoch: 11 [440/1400 (31%)]\tLoss: 176.909271\n",
            "Train Epoch: 11 [480/1400 (34%)]\tLoss: 155.070740\n",
            "Train Epoch: 11 [520/1400 (37%)]\tLoss: 178.481659\n",
            "Train Epoch: 11 [560/1400 (40%)]\tLoss: 179.760910\n",
            "Train Epoch: 11 [600/1400 (43%)]\tLoss: 157.520630\n",
            "Train Epoch: 11 [640/1400 (46%)]\tLoss: 192.746643\n",
            "Train Epoch: 11 [680/1400 (49%)]\tLoss: 160.747543\n",
            "Train Epoch: 11 [720/1400 (51%)]\tLoss: 148.879425\n",
            "Train Epoch: 11 [760/1400 (54%)]\tLoss: 182.238312\n",
            "Train Epoch: 11 [800/1400 (57%)]\tLoss: 193.024887\n",
            "Train Epoch: 11 [840/1400 (60%)]\tLoss: 152.353912\n",
            "Train Epoch: 11 [880/1400 (63%)]\tLoss: 166.149384\n",
            "Train Epoch: 11 [920/1400 (66%)]\tLoss: 172.839157\n",
            "Train Epoch: 11 [960/1400 (69%)]\tLoss: 164.771622\n",
            "Train Epoch: 11 [1000/1400 (71%)]\tLoss: 177.902451\n",
            "Train Epoch: 11 [1040/1400 (74%)]\tLoss: 157.104797\n",
            "Train Epoch: 11 [1080/1400 (77%)]\tLoss: 171.255737\n",
            "Train Epoch: 11 [1120/1400 (80%)]\tLoss: 182.139496\n",
            "Train Epoch: 11 [1160/1400 (83%)]\tLoss: 193.005569\n",
            "Train Epoch: 11 [1200/1400 (86%)]\tLoss: 114.048637\n",
            "Train Epoch: 11 [1240/1400 (89%)]\tLoss: 217.434784\n",
            "Train Epoch: 11 [1280/1400 (91%)]\tLoss: 172.691406\n",
            "Train Epoch: 11 [1320/1400 (94%)]\tLoss: 167.012207\n",
            "Train Epoch: 11 [1360/1400 (97%)]\tLoss: 187.918671\n",
            "====> Epoch: 11 Average loss: 174.2243\n",
            "====> Test set loss: 176.7892\n",
            "Train Epoch: 12 [0/1400 (0%)]\tLoss: 128.248901\n",
            "Train Epoch: 12 [40/1400 (3%)]\tLoss: 156.656448\n",
            "Train Epoch: 12 [80/1400 (6%)]\tLoss: 146.341949\n",
            "Train Epoch: 12 [120/1400 (9%)]\tLoss: 176.425858\n",
            "Train Epoch: 12 [160/1400 (11%)]\tLoss: 157.365021\n",
            "Train Epoch: 12 [200/1400 (14%)]\tLoss: 223.690140\n",
            "Train Epoch: 12 [240/1400 (17%)]\tLoss: 154.479736\n",
            "Train Epoch: 12 [280/1400 (20%)]\tLoss: 169.492828\n",
            "Train Epoch: 12 [320/1400 (23%)]\tLoss: 150.462967\n",
            "Train Epoch: 12 [360/1400 (26%)]\tLoss: 128.067200\n",
            "Train Epoch: 12 [400/1400 (29%)]\tLoss: 175.969025\n",
            "Train Epoch: 12 [440/1400 (31%)]\tLoss: 192.730759\n",
            "Train Epoch: 12 [480/1400 (34%)]\tLoss: 176.065216\n",
            "Train Epoch: 12 [520/1400 (37%)]\tLoss: 171.041763\n",
            "Train Epoch: 12 [560/1400 (40%)]\tLoss: 242.088623\n",
            "Train Epoch: 12 [600/1400 (43%)]\tLoss: 191.838593\n",
            "Train Epoch: 12 [640/1400 (46%)]\tLoss: 156.231400\n",
            "Train Epoch: 12 [680/1400 (49%)]\tLoss: 212.485947\n",
            "Train Epoch: 12 [720/1400 (51%)]\tLoss: 184.193436\n",
            "Train Epoch: 12 [760/1400 (54%)]\tLoss: 144.087769\n",
            "Train Epoch: 12 [800/1400 (57%)]\tLoss: 160.950546\n",
            "Train Epoch: 12 [840/1400 (60%)]\tLoss: 162.664246\n",
            "Train Epoch: 12 [880/1400 (63%)]\tLoss: 142.595184\n",
            "Train Epoch: 12 [920/1400 (66%)]\tLoss: 170.609634\n",
            "Train Epoch: 12 [960/1400 (69%)]\tLoss: 144.918427\n",
            "Train Epoch: 12 [1000/1400 (71%)]\tLoss: 164.597702\n",
            "Train Epoch: 12 [1040/1400 (74%)]\tLoss: 207.555710\n",
            "Train Epoch: 12 [1080/1400 (77%)]\tLoss: 219.045166\n",
            "Train Epoch: 12 [1120/1400 (80%)]\tLoss: 171.325287\n",
            "Train Epoch: 12 [1160/1400 (83%)]\tLoss: 168.062988\n",
            "Train Epoch: 12 [1200/1400 (86%)]\tLoss: 227.287964\n",
            "Train Epoch: 12 [1240/1400 (89%)]\tLoss: 161.188354\n",
            "Train Epoch: 12 [1280/1400 (91%)]\tLoss: 218.597900\n",
            "Train Epoch: 12 [1320/1400 (94%)]\tLoss: 141.410492\n",
            "Train Epoch: 12 [1360/1400 (97%)]\tLoss: 183.956207\n",
            "====> Epoch: 12 Average loss: 173.2737\n",
            "====> Test set loss: 176.6563\n",
            "Train Epoch: 13 [0/1400 (0%)]\tLoss: 146.077240\n",
            "Train Epoch: 13 [40/1400 (3%)]\tLoss: 207.027283\n",
            "Train Epoch: 13 [80/1400 (6%)]\tLoss: 148.311371\n",
            "Train Epoch: 13 [120/1400 (9%)]\tLoss: 154.405197\n",
            "Train Epoch: 13 [160/1400 (11%)]\tLoss: 147.293274\n",
            "Train Epoch: 13 [200/1400 (14%)]\tLoss: 187.546341\n",
            "Train Epoch: 13 [240/1400 (17%)]\tLoss: 172.602081\n",
            "Train Epoch: 13 [280/1400 (20%)]\tLoss: 192.633102\n",
            "Train Epoch: 13 [320/1400 (23%)]\tLoss: 153.999161\n",
            "Train Epoch: 13 [360/1400 (26%)]\tLoss: 144.115051\n",
            "Train Epoch: 13 [400/1400 (29%)]\tLoss: 171.492371\n",
            "Train Epoch: 13 [440/1400 (31%)]\tLoss: 140.325287\n",
            "Train Epoch: 13 [480/1400 (34%)]\tLoss: 176.885239\n",
            "Train Epoch: 13 [520/1400 (37%)]\tLoss: 129.875122\n",
            "Train Epoch: 13 [560/1400 (40%)]\tLoss: 134.161987\n",
            "Train Epoch: 13 [600/1400 (43%)]\tLoss: 162.839859\n",
            "Train Epoch: 13 [640/1400 (46%)]\tLoss: 154.477539\n",
            "Train Epoch: 13 [680/1400 (49%)]\tLoss: 168.316666\n",
            "Train Epoch: 13 [720/1400 (51%)]\tLoss: 167.659668\n",
            "Train Epoch: 13 [760/1400 (54%)]\tLoss: 155.919189\n",
            "Train Epoch: 13 [800/1400 (57%)]\tLoss: 197.492371\n",
            "Train Epoch: 13 [840/1400 (60%)]\tLoss: 147.066025\n",
            "Train Epoch: 13 [880/1400 (63%)]\tLoss: 159.212631\n",
            "Train Epoch: 13 [920/1400 (66%)]\tLoss: 162.957550\n",
            "Train Epoch: 13 [960/1400 (69%)]\tLoss: 134.851166\n",
            "Train Epoch: 13 [1000/1400 (71%)]\tLoss: 161.085327\n",
            "Train Epoch: 13 [1040/1400 (74%)]\tLoss: 152.455231\n",
            "Train Epoch: 13 [1080/1400 (77%)]\tLoss: 161.946304\n",
            "Train Epoch: 13 [1120/1400 (80%)]\tLoss: 154.176910\n",
            "Train Epoch: 13 [1160/1400 (83%)]\tLoss: 192.498596\n",
            "Train Epoch: 13 [1200/1400 (86%)]\tLoss: 178.657547\n",
            "Train Epoch: 13 [1240/1400 (89%)]\tLoss: 161.836700\n",
            "Train Epoch: 13 [1280/1400 (91%)]\tLoss: 184.477142\n",
            "Train Epoch: 13 [1320/1400 (94%)]\tLoss: 145.454926\n",
            "Train Epoch: 13 [1360/1400 (97%)]\tLoss: 153.611298\n",
            "====> Epoch: 13 Average loss: 172.7214\n",
            "====> Test set loss: 175.4163\n",
            "Train Epoch: 14 [0/1400 (0%)]\tLoss: 135.268921\n",
            "Train Epoch: 14 [40/1400 (3%)]\tLoss: 189.228607\n",
            "Train Epoch: 14 [80/1400 (6%)]\tLoss: 164.158981\n",
            "Train Epoch: 14 [120/1400 (9%)]\tLoss: 125.938911\n",
            "Train Epoch: 14 [160/1400 (11%)]\tLoss: 142.133514\n",
            "Train Epoch: 14 [200/1400 (14%)]\tLoss: 222.697891\n",
            "Train Epoch: 14 [240/1400 (17%)]\tLoss: 168.065277\n",
            "Train Epoch: 14 [280/1400 (20%)]\tLoss: 203.032715\n",
            "Train Epoch: 14 [320/1400 (23%)]\tLoss: 163.254105\n",
            "Train Epoch: 14 [360/1400 (26%)]\tLoss: 139.670837\n",
            "Train Epoch: 14 [400/1400 (29%)]\tLoss: 177.860977\n",
            "Train Epoch: 14 [440/1400 (31%)]\tLoss: 189.346054\n",
            "Train Epoch: 14 [480/1400 (34%)]\tLoss: 194.621765\n",
            "Train Epoch: 14 [520/1400 (37%)]\tLoss: 208.637970\n",
            "Train Epoch: 14 [560/1400 (40%)]\tLoss: 157.286835\n",
            "Train Epoch: 14 [600/1400 (43%)]\tLoss: 233.917419\n",
            "Train Epoch: 14 [640/1400 (46%)]\tLoss: 202.433350\n",
            "Train Epoch: 14 [680/1400 (49%)]\tLoss: 185.906418\n",
            "Train Epoch: 14 [720/1400 (51%)]\tLoss: 171.284058\n",
            "Train Epoch: 14 [760/1400 (54%)]\tLoss: 180.804749\n",
            "Train Epoch: 14 [800/1400 (57%)]\tLoss: 174.781845\n",
            "Train Epoch: 14 [840/1400 (60%)]\tLoss: 183.770538\n",
            "Train Epoch: 14 [880/1400 (63%)]\tLoss: 140.706421\n",
            "Train Epoch: 14 [920/1400 (66%)]\tLoss: 136.147827\n",
            "Train Epoch: 14 [960/1400 (69%)]\tLoss: 149.031021\n",
            "Train Epoch: 14 [1000/1400 (71%)]\tLoss: 151.305939\n",
            "Train Epoch: 14 [1040/1400 (74%)]\tLoss: 139.368942\n",
            "Train Epoch: 14 [1080/1400 (77%)]\tLoss: 157.233063\n",
            "Train Epoch: 14 [1120/1400 (80%)]\tLoss: 179.961655\n",
            "Train Epoch: 14 [1160/1400 (83%)]\tLoss: 184.218658\n",
            "Train Epoch: 14 [1200/1400 (86%)]\tLoss: 143.113846\n",
            "Train Epoch: 14 [1240/1400 (89%)]\tLoss: 162.934113\n",
            "Train Epoch: 14 [1280/1400 (91%)]\tLoss: 196.123688\n",
            "Train Epoch: 14 [1320/1400 (94%)]\tLoss: 152.503571\n",
            "Train Epoch: 14 [1360/1400 (97%)]\tLoss: 158.104858\n",
            "====> Epoch: 14 Average loss: 172.2167\n",
            "====> Test set loss: 174.9293\n",
            "Train Epoch: 15 [0/1400 (0%)]\tLoss: 221.176941\n",
            "Train Epoch: 15 [40/1400 (3%)]\tLoss: 195.184967\n",
            "Train Epoch: 15 [80/1400 (6%)]\tLoss: 142.331848\n",
            "Train Epoch: 15 [120/1400 (9%)]\tLoss: 146.107147\n",
            "Train Epoch: 15 [160/1400 (11%)]\tLoss: 155.499557\n",
            "Train Epoch: 15 [200/1400 (14%)]\tLoss: 178.280029\n",
            "Train Epoch: 15 [240/1400 (17%)]\tLoss: 161.480179\n",
            "Train Epoch: 15 [280/1400 (20%)]\tLoss: 158.674927\n",
            "Train Epoch: 15 [320/1400 (23%)]\tLoss: 144.488312\n",
            "Train Epoch: 15 [360/1400 (26%)]\tLoss: 198.149384\n",
            "Train Epoch: 15 [400/1400 (29%)]\tLoss: 162.633041\n",
            "Train Epoch: 15 [440/1400 (31%)]\tLoss: 162.876816\n",
            "Train Epoch: 15 [480/1400 (34%)]\tLoss: 137.332031\n",
            "Train Epoch: 15 [520/1400 (37%)]\tLoss: 161.443314\n",
            "Train Epoch: 15 [560/1400 (40%)]\tLoss: 128.178024\n",
            "Train Epoch: 15 [600/1400 (43%)]\tLoss: 188.203094\n",
            "Train Epoch: 15 [640/1400 (46%)]\tLoss: 187.628143\n",
            "Train Epoch: 15 [680/1400 (49%)]\tLoss: 148.998886\n",
            "Train Epoch: 15 [720/1400 (51%)]\tLoss: 134.072311\n",
            "Train Epoch: 15 [760/1400 (54%)]\tLoss: 140.254776\n",
            "Train Epoch: 15 [800/1400 (57%)]\tLoss: 190.718689\n",
            "Train Epoch: 15 [840/1400 (60%)]\tLoss: 160.614807\n",
            "Train Epoch: 15 [880/1400 (63%)]\tLoss: 205.252197\n",
            "Train Epoch: 15 [920/1400 (66%)]\tLoss: 149.155396\n",
            "Train Epoch: 15 [960/1400 (69%)]\tLoss: 164.302338\n",
            "Train Epoch: 15 [1000/1400 (71%)]\tLoss: 169.708389\n",
            "Train Epoch: 15 [1040/1400 (74%)]\tLoss: 145.283371\n",
            "Train Epoch: 15 [1080/1400 (77%)]\tLoss: 141.009888\n",
            "Train Epoch: 15 [1120/1400 (80%)]\tLoss: 145.155701\n",
            "Train Epoch: 15 [1160/1400 (83%)]\tLoss: 182.036072\n",
            "Train Epoch: 15 [1200/1400 (86%)]\tLoss: 153.347961\n",
            "Train Epoch: 15 [1240/1400 (89%)]\tLoss: 195.853882\n",
            "Train Epoch: 15 [1280/1400 (91%)]\tLoss: 157.392929\n",
            "Train Epoch: 15 [1320/1400 (94%)]\tLoss: 174.753494\n",
            "Train Epoch: 15 [1360/1400 (97%)]\tLoss: 135.603516\n",
            "====> Epoch: 15 Average loss: 171.5544\n",
            "====> Test set loss: 174.5923\n",
            "Train Epoch: 16 [0/1400 (0%)]\tLoss: 172.759171\n",
            "Train Epoch: 16 [40/1400 (3%)]\tLoss: 162.550278\n",
            "Train Epoch: 16 [80/1400 (6%)]\tLoss: 188.705734\n",
            "Train Epoch: 16 [120/1400 (9%)]\tLoss: 153.786240\n",
            "Train Epoch: 16 [160/1400 (11%)]\tLoss: 187.967773\n",
            "Train Epoch: 16 [200/1400 (14%)]\tLoss: 208.996292\n",
            "Train Epoch: 16 [240/1400 (17%)]\tLoss: 154.994705\n",
            "Train Epoch: 16 [280/1400 (20%)]\tLoss: 209.088593\n",
            "Train Epoch: 16 [320/1400 (23%)]\tLoss: 157.590439\n",
            "Train Epoch: 16 [360/1400 (26%)]\tLoss: 182.083405\n",
            "Train Epoch: 16 [400/1400 (29%)]\tLoss: 170.103119\n",
            "Train Epoch: 16 [440/1400 (31%)]\tLoss: 161.648788\n",
            "Train Epoch: 16 [480/1400 (34%)]\tLoss: 189.223495\n",
            "Train Epoch: 16 [520/1400 (37%)]\tLoss: 140.849243\n",
            "Train Epoch: 16 [560/1400 (40%)]\tLoss: 217.948685\n",
            "Train Epoch: 16 [600/1400 (43%)]\tLoss: 210.106598\n",
            "Train Epoch: 16 [640/1400 (46%)]\tLoss: 156.791473\n",
            "Train Epoch: 16 [680/1400 (49%)]\tLoss: 150.747070\n",
            "Train Epoch: 16 [720/1400 (51%)]\tLoss: 180.311386\n",
            "Train Epoch: 16 [760/1400 (54%)]\tLoss: 173.058975\n",
            "Train Epoch: 16 [800/1400 (57%)]\tLoss: 212.722778\n",
            "Train Epoch: 16 [840/1400 (60%)]\tLoss: 159.828308\n",
            "Train Epoch: 16 [880/1400 (63%)]\tLoss: 125.025154\n",
            "Train Epoch: 16 [920/1400 (66%)]\tLoss: 203.132874\n",
            "Train Epoch: 16 [960/1400 (69%)]\tLoss: 190.373840\n",
            "Train Epoch: 16 [1000/1400 (71%)]\tLoss: 160.452866\n",
            "Train Epoch: 16 [1040/1400 (74%)]\tLoss: 145.057785\n",
            "Train Epoch: 16 [1080/1400 (77%)]\tLoss: 132.472519\n",
            "Train Epoch: 16 [1120/1400 (80%)]\tLoss: 131.669357\n",
            "Train Epoch: 16 [1160/1400 (83%)]\tLoss: 143.690643\n",
            "Train Epoch: 16 [1200/1400 (86%)]\tLoss: 213.207230\n",
            "Train Epoch: 16 [1240/1400 (89%)]\tLoss: 196.538620\n",
            "Train Epoch: 16 [1280/1400 (91%)]\tLoss: 155.662582\n",
            "Train Epoch: 16 [1320/1400 (94%)]\tLoss: 183.822433\n",
            "Train Epoch: 16 [1360/1400 (97%)]\tLoss: 202.702988\n",
            "====> Epoch: 16 Average loss: 171.1356\n",
            "====> Test set loss: 174.0630\n",
            "Train Epoch: 17 [0/1400 (0%)]\tLoss: 155.104401\n",
            "Train Epoch: 17 [40/1400 (3%)]\tLoss: 221.211517\n",
            "Train Epoch: 17 [80/1400 (6%)]\tLoss: 205.314987\n",
            "Train Epoch: 17 [120/1400 (9%)]\tLoss: 132.056015\n",
            "Train Epoch: 17 [160/1400 (11%)]\tLoss: 178.087601\n",
            "Train Epoch: 17 [200/1400 (14%)]\tLoss: 193.141006\n",
            "Train Epoch: 17 [240/1400 (17%)]\tLoss: 140.939346\n",
            "Train Epoch: 17 [280/1400 (20%)]\tLoss: 168.326157\n",
            "Train Epoch: 17 [320/1400 (23%)]\tLoss: 192.060959\n",
            "Train Epoch: 17 [360/1400 (26%)]\tLoss: 236.279846\n",
            "Train Epoch: 17 [400/1400 (29%)]\tLoss: 205.892166\n",
            "Train Epoch: 17 [440/1400 (31%)]\tLoss: 171.484497\n",
            "Train Epoch: 17 [480/1400 (34%)]\tLoss: 184.356079\n",
            "Train Epoch: 17 [520/1400 (37%)]\tLoss: 136.737167\n",
            "Train Epoch: 17 [560/1400 (40%)]\tLoss: 206.541901\n",
            "Train Epoch: 17 [600/1400 (43%)]\tLoss: 201.131531\n",
            "Train Epoch: 17 [640/1400 (46%)]\tLoss: 189.781952\n",
            "Train Epoch: 17 [680/1400 (49%)]\tLoss: 186.345200\n",
            "Train Epoch: 17 [720/1400 (51%)]\tLoss: 176.169617\n",
            "Train Epoch: 17 [760/1400 (54%)]\tLoss: 164.976181\n",
            "Train Epoch: 17 [800/1400 (57%)]\tLoss: 174.790192\n",
            "Train Epoch: 17 [840/1400 (60%)]\tLoss: 186.285828\n",
            "Train Epoch: 17 [880/1400 (63%)]\tLoss: 139.334717\n",
            "Train Epoch: 17 [920/1400 (66%)]\tLoss: 179.998550\n",
            "Train Epoch: 17 [960/1400 (69%)]\tLoss: 187.431320\n",
            "Train Epoch: 17 [1000/1400 (71%)]\tLoss: 182.099701\n",
            "Train Epoch: 17 [1040/1400 (74%)]\tLoss: 173.527908\n",
            "Train Epoch: 17 [1080/1400 (77%)]\tLoss: 135.104462\n",
            "Train Epoch: 17 [1120/1400 (80%)]\tLoss: 185.873184\n",
            "Train Epoch: 17 [1160/1400 (83%)]\tLoss: 220.048798\n",
            "Train Epoch: 17 [1200/1400 (86%)]\tLoss: 248.424866\n",
            "Train Epoch: 17 [1240/1400 (89%)]\tLoss: 160.907043\n",
            "Train Epoch: 17 [1280/1400 (91%)]\tLoss: 184.151260\n",
            "Train Epoch: 17 [1320/1400 (94%)]\tLoss: 165.018692\n",
            "Train Epoch: 17 [1360/1400 (97%)]\tLoss: 196.756073\n",
            "====> Epoch: 17 Average loss: 170.7546\n",
            "====> Test set loss: 173.6708\n",
            "Train Epoch: 18 [0/1400 (0%)]\tLoss: 187.725113\n",
            "Train Epoch: 18 [40/1400 (3%)]\tLoss: 182.922424\n",
            "Train Epoch: 18 [80/1400 (6%)]\tLoss: 167.131500\n",
            "Train Epoch: 18 [120/1400 (9%)]\tLoss: 146.910233\n",
            "Train Epoch: 18 [160/1400 (11%)]\tLoss: 132.537766\n",
            "Train Epoch: 18 [200/1400 (14%)]\tLoss: 178.078491\n",
            "Train Epoch: 18 [240/1400 (17%)]\tLoss: 151.812897\n",
            "Train Epoch: 18 [280/1400 (20%)]\tLoss: 222.346283\n",
            "Train Epoch: 18 [320/1400 (23%)]\tLoss: 232.740875\n",
            "Train Epoch: 18 [360/1400 (26%)]\tLoss: 167.949341\n",
            "Train Epoch: 18 [400/1400 (29%)]\tLoss: 195.218109\n",
            "Train Epoch: 18 [440/1400 (31%)]\tLoss: 222.068436\n",
            "Train Epoch: 18 [480/1400 (34%)]\tLoss: 159.701569\n",
            "Train Epoch: 18 [520/1400 (37%)]\tLoss: 179.393494\n",
            "Train Epoch: 18 [560/1400 (40%)]\tLoss: 202.324066\n",
            "Train Epoch: 18 [600/1400 (43%)]\tLoss: 192.131500\n",
            "Train Epoch: 18 [640/1400 (46%)]\tLoss: 146.815811\n",
            "Train Epoch: 18 [680/1400 (49%)]\tLoss: 149.505325\n",
            "Train Epoch: 18 [720/1400 (51%)]\tLoss: 149.189758\n",
            "Train Epoch: 18 [760/1400 (54%)]\tLoss: 201.490051\n",
            "Train Epoch: 18 [800/1400 (57%)]\tLoss: 133.578949\n",
            "Train Epoch: 18 [840/1400 (60%)]\tLoss: 163.328827\n",
            "Train Epoch: 18 [880/1400 (63%)]\tLoss: 203.434570\n",
            "Train Epoch: 18 [920/1400 (66%)]\tLoss: 159.345917\n",
            "Train Epoch: 18 [960/1400 (69%)]\tLoss: 141.240311\n",
            "Train Epoch: 18 [1000/1400 (71%)]\tLoss: 169.692551\n",
            "Train Epoch: 18 [1040/1400 (74%)]\tLoss: 158.390060\n",
            "Train Epoch: 18 [1080/1400 (77%)]\tLoss: 206.501358\n",
            "Train Epoch: 18 [1120/1400 (80%)]\tLoss: 211.487671\n",
            "Train Epoch: 18 [1160/1400 (83%)]\tLoss: 155.966370\n",
            "Train Epoch: 18 [1200/1400 (86%)]\tLoss: 140.044891\n",
            "Train Epoch: 18 [1240/1400 (89%)]\tLoss: 158.884521\n",
            "Train Epoch: 18 [1280/1400 (91%)]\tLoss: 146.405563\n",
            "Train Epoch: 18 [1320/1400 (94%)]\tLoss: 129.008698\n",
            "Train Epoch: 18 [1360/1400 (97%)]\tLoss: 194.336731\n",
            "====> Epoch: 18 Average loss: 170.4428\n",
            "====> Test set loss: 173.4103\n",
            "Train Epoch: 19 [0/1400 (0%)]\tLoss: 156.605560\n",
            "Train Epoch: 19 [40/1400 (3%)]\tLoss: 246.586151\n",
            "Train Epoch: 19 [80/1400 (6%)]\tLoss: 139.776886\n",
            "Train Epoch: 19 [120/1400 (9%)]\tLoss: 172.979126\n",
            "Train Epoch: 19 [160/1400 (11%)]\tLoss: 189.388397\n",
            "Train Epoch: 19 [200/1400 (14%)]\tLoss: 203.959045\n",
            "Train Epoch: 19 [240/1400 (17%)]\tLoss: 165.281921\n",
            "Train Epoch: 19 [280/1400 (20%)]\tLoss: 176.868713\n",
            "Train Epoch: 19 [320/1400 (23%)]\tLoss: 173.553101\n",
            "Train Epoch: 19 [360/1400 (26%)]\tLoss: 156.099792\n",
            "Train Epoch: 19 [400/1400 (29%)]\tLoss: 152.203720\n",
            "Train Epoch: 19 [440/1400 (31%)]\tLoss: 133.024567\n",
            "Train Epoch: 19 [480/1400 (34%)]\tLoss: 153.641602\n",
            "Train Epoch: 19 [520/1400 (37%)]\tLoss: 144.275299\n",
            "Train Epoch: 19 [560/1400 (40%)]\tLoss: 157.142151\n",
            "Train Epoch: 19 [600/1400 (43%)]\tLoss: 164.697830\n",
            "Train Epoch: 19 [640/1400 (46%)]\tLoss: 153.799774\n",
            "Train Epoch: 19 [680/1400 (49%)]\tLoss: 148.908218\n",
            "Train Epoch: 19 [720/1400 (51%)]\tLoss: 132.321518\n",
            "Train Epoch: 19 [760/1400 (54%)]\tLoss: 183.102402\n",
            "Train Epoch: 19 [800/1400 (57%)]\tLoss: 136.756119\n",
            "Train Epoch: 19 [840/1400 (60%)]\tLoss: 166.939285\n",
            "Train Epoch: 19 [880/1400 (63%)]\tLoss: 195.026413\n",
            "Train Epoch: 19 [920/1400 (66%)]\tLoss: 168.618271\n",
            "Train Epoch: 19 [960/1400 (69%)]\tLoss: 147.915070\n",
            "Train Epoch: 19 [1000/1400 (71%)]\tLoss: 147.520294\n",
            "Train Epoch: 19 [1040/1400 (74%)]\tLoss: 160.665329\n",
            "Train Epoch: 19 [1080/1400 (77%)]\tLoss: 183.559723\n",
            "Train Epoch: 19 [1120/1400 (80%)]\tLoss: 213.923798\n",
            "Train Epoch: 19 [1160/1400 (83%)]\tLoss: 183.646988\n",
            "Train Epoch: 19 [1200/1400 (86%)]\tLoss: 152.831192\n",
            "Train Epoch: 19 [1240/1400 (89%)]\tLoss: 143.972809\n",
            "Train Epoch: 19 [1280/1400 (91%)]\tLoss: 133.270737\n",
            "Train Epoch: 19 [1320/1400 (94%)]\tLoss: 152.543961\n",
            "Train Epoch: 19 [1360/1400 (97%)]\tLoss: 168.724457\n",
            "====> Epoch: 19 Average loss: 170.0419\n",
            "====> Test set loss: 172.7380\n",
            "Train Epoch: 20 [0/1400 (0%)]\tLoss: 179.401917\n",
            "Train Epoch: 20 [40/1400 (3%)]\tLoss: 175.498657\n",
            "Train Epoch: 20 [80/1400 (6%)]\tLoss: 152.268799\n",
            "Train Epoch: 20 [120/1400 (9%)]\tLoss: 135.239380\n",
            "Train Epoch: 20 [160/1400 (11%)]\tLoss: 146.565125\n",
            "Train Epoch: 20 [200/1400 (14%)]\tLoss: 180.295334\n",
            "Train Epoch: 20 [240/1400 (17%)]\tLoss: 188.476105\n",
            "Train Epoch: 20 [280/1400 (20%)]\tLoss: 124.882225\n",
            "Train Epoch: 20 [320/1400 (23%)]\tLoss: 234.639862\n",
            "Train Epoch: 20 [360/1400 (26%)]\tLoss: 184.478363\n",
            "Train Epoch: 20 [400/1400 (29%)]\tLoss: 133.305099\n",
            "Train Epoch: 20 [440/1400 (31%)]\tLoss: 153.919312\n",
            "Train Epoch: 20 [480/1400 (34%)]\tLoss: 163.801102\n",
            "Train Epoch: 20 [520/1400 (37%)]\tLoss: 187.063019\n",
            "Train Epoch: 20 [560/1400 (40%)]\tLoss: 156.508911\n",
            "Train Epoch: 20 [600/1400 (43%)]\tLoss: 133.741501\n",
            "Train Epoch: 20 [640/1400 (46%)]\tLoss: 149.302460\n",
            "Train Epoch: 20 [680/1400 (49%)]\tLoss: 177.550171\n",
            "Train Epoch: 20 [720/1400 (51%)]\tLoss: 170.422501\n",
            "Train Epoch: 20 [760/1400 (54%)]\tLoss: 119.831451\n",
            "Train Epoch: 20 [800/1400 (57%)]\tLoss: 155.568939\n",
            "Train Epoch: 20 [840/1400 (60%)]\tLoss: 160.633499\n",
            "Train Epoch: 20 [880/1400 (63%)]\tLoss: 173.097015\n",
            "Train Epoch: 20 [920/1400 (66%)]\tLoss: 139.249756\n",
            "Train Epoch: 20 [960/1400 (69%)]\tLoss: 129.590790\n",
            "Train Epoch: 20 [1000/1400 (71%)]\tLoss: 159.177841\n",
            "Train Epoch: 20 [1040/1400 (74%)]\tLoss: 139.637024\n",
            "Train Epoch: 20 [1080/1400 (77%)]\tLoss: 162.074463\n",
            "Train Epoch: 20 [1120/1400 (80%)]\tLoss: 170.645996\n",
            "Train Epoch: 20 [1160/1400 (83%)]\tLoss: 145.549164\n",
            "Train Epoch: 20 [1200/1400 (86%)]\tLoss: 172.097458\n",
            "Train Epoch: 20 [1240/1400 (89%)]\tLoss: 124.020432\n",
            "Train Epoch: 20 [1280/1400 (91%)]\tLoss: 123.797424\n",
            "Train Epoch: 20 [1320/1400 (94%)]\tLoss: 168.320389\n",
            "Train Epoch: 20 [1360/1400 (97%)]\tLoss: 150.057053\n",
            "====> Epoch: 20 Average loss: 169.7596\n",
            "====> Test set loss: 172.7402\n",
            "Train Epoch: 21 [0/1400 (0%)]\tLoss: 176.604874\n",
            "Train Epoch: 21 [40/1400 (3%)]\tLoss: 188.245361\n",
            "Train Epoch: 21 [80/1400 (6%)]\tLoss: 174.281784\n",
            "Train Epoch: 21 [120/1400 (9%)]\tLoss: 149.540283\n",
            "Train Epoch: 21 [160/1400 (11%)]\tLoss: 216.886536\n",
            "Train Epoch: 21 [200/1400 (14%)]\tLoss: 210.140854\n",
            "Train Epoch: 21 [240/1400 (17%)]\tLoss: 161.263428\n",
            "Train Epoch: 21 [280/1400 (20%)]\tLoss: 142.536072\n",
            "Train Epoch: 21 [320/1400 (23%)]\tLoss: 222.769119\n",
            "Train Epoch: 21 [360/1400 (26%)]\tLoss: 142.376007\n",
            "Train Epoch: 21 [400/1400 (29%)]\tLoss: 171.048782\n",
            "Train Epoch: 21 [440/1400 (31%)]\tLoss: 127.320023\n",
            "Train Epoch: 21 [480/1400 (34%)]\tLoss: 154.367233\n",
            "Train Epoch: 21 [520/1400 (37%)]\tLoss: 186.359833\n",
            "Train Epoch: 21 [560/1400 (40%)]\tLoss: 159.808197\n",
            "Train Epoch: 21 [600/1400 (43%)]\tLoss: 180.162338\n",
            "Train Epoch: 21 [640/1400 (46%)]\tLoss: 151.329239\n",
            "Train Epoch: 21 [680/1400 (49%)]\tLoss: 154.694382\n",
            "Train Epoch: 21 [720/1400 (51%)]\tLoss: 180.049469\n",
            "Train Epoch: 21 [760/1400 (54%)]\tLoss: 182.155579\n",
            "Train Epoch: 21 [800/1400 (57%)]\tLoss: 143.319656\n",
            "Train Epoch: 21 [840/1400 (60%)]\tLoss: 144.687408\n",
            "Train Epoch: 21 [880/1400 (63%)]\tLoss: 157.195862\n",
            "Train Epoch: 21 [920/1400 (66%)]\tLoss: 159.787689\n",
            "Train Epoch: 21 [960/1400 (69%)]\tLoss: 207.781799\n",
            "Train Epoch: 21 [1000/1400 (71%)]\tLoss: 142.469589\n",
            "Train Epoch: 21 [1040/1400 (74%)]\tLoss: 142.314804\n",
            "Train Epoch: 21 [1080/1400 (77%)]\tLoss: 176.784714\n",
            "Train Epoch: 21 [1120/1400 (80%)]\tLoss: 182.504013\n",
            "Train Epoch: 21 [1160/1400 (83%)]\tLoss: 171.571899\n",
            "Train Epoch: 21 [1200/1400 (86%)]\tLoss: 137.479843\n",
            "Train Epoch: 21 [1240/1400 (89%)]\tLoss: 164.794403\n",
            "Train Epoch: 21 [1280/1400 (91%)]\tLoss: 158.959961\n",
            "Train Epoch: 21 [1320/1400 (94%)]\tLoss: 162.785812\n",
            "Train Epoch: 21 [1360/1400 (97%)]\tLoss: 176.735245\n",
            "====> Epoch: 21 Average loss: 169.3825\n",
            "====> Test set loss: 172.3970\n",
            "Train Epoch: 22 [0/1400 (0%)]\tLoss: 135.052612\n",
            "Train Epoch: 22 [40/1400 (3%)]\tLoss: 127.823067\n",
            "Train Epoch: 22 [80/1400 (6%)]\tLoss: 181.785797\n",
            "Train Epoch: 22 [120/1400 (9%)]\tLoss: 182.884308\n",
            "Train Epoch: 22 [160/1400 (11%)]\tLoss: 175.768097\n",
            "Train Epoch: 22 [200/1400 (14%)]\tLoss: 182.353806\n",
            "Train Epoch: 22 [240/1400 (17%)]\tLoss: 133.126755\n",
            "Train Epoch: 22 [280/1400 (20%)]\tLoss: 146.973984\n",
            "Train Epoch: 22 [320/1400 (23%)]\tLoss: 162.862061\n",
            "Train Epoch: 22 [360/1400 (26%)]\tLoss: 203.034821\n",
            "Train Epoch: 22 [400/1400 (29%)]\tLoss: 184.373688\n",
            "Train Epoch: 22 [440/1400 (31%)]\tLoss: 145.988693\n",
            "Train Epoch: 22 [480/1400 (34%)]\tLoss: 152.515137\n",
            "Train Epoch: 22 [520/1400 (37%)]\tLoss: 156.092285\n",
            "Train Epoch: 22 [560/1400 (40%)]\tLoss: 188.851837\n",
            "Train Epoch: 22 [600/1400 (43%)]\tLoss: 159.496170\n",
            "Train Epoch: 22 [640/1400 (46%)]\tLoss: 196.668884\n",
            "Train Epoch: 22 [680/1400 (49%)]\tLoss: 160.384125\n",
            "Train Epoch: 22 [720/1400 (51%)]\tLoss: 212.616623\n",
            "Train Epoch: 22 [760/1400 (54%)]\tLoss: 157.654114\n",
            "Train Epoch: 22 [800/1400 (57%)]\tLoss: 181.920181\n",
            "Train Epoch: 22 [840/1400 (60%)]\tLoss: 176.365784\n",
            "Train Epoch: 22 [880/1400 (63%)]\tLoss: 131.733734\n",
            "Train Epoch: 22 [920/1400 (66%)]\tLoss: 196.819275\n",
            "Train Epoch: 22 [960/1400 (69%)]\tLoss: 145.356094\n",
            "Train Epoch: 22 [1000/1400 (71%)]\tLoss: 142.813202\n",
            "Train Epoch: 22 [1040/1400 (74%)]\tLoss: 161.748611\n",
            "Train Epoch: 22 [1080/1400 (77%)]\tLoss: 166.157074\n",
            "Train Epoch: 22 [1120/1400 (80%)]\tLoss: 166.336411\n",
            "Train Epoch: 22 [1160/1400 (83%)]\tLoss: 164.654572\n",
            "Train Epoch: 22 [1200/1400 (86%)]\tLoss: 143.953064\n",
            "Train Epoch: 22 [1240/1400 (89%)]\tLoss: 138.499649\n",
            "Train Epoch: 22 [1280/1400 (91%)]\tLoss: 149.134613\n",
            "Train Epoch: 22 [1320/1400 (94%)]\tLoss: 163.548111\n",
            "Train Epoch: 22 [1360/1400 (97%)]\tLoss: 160.141327\n",
            "====> Epoch: 22 Average loss: 169.0121\n",
            "====> Test set loss: 171.7369\n",
            "Train Epoch: 23 [0/1400 (0%)]\tLoss: 191.081604\n",
            "Train Epoch: 23 [40/1400 (3%)]\tLoss: 170.690781\n",
            "Train Epoch: 23 [80/1400 (6%)]\tLoss: 187.389572\n",
            "Train Epoch: 23 [120/1400 (9%)]\tLoss: 157.370285\n",
            "Train Epoch: 23 [160/1400 (11%)]\tLoss: 158.262924\n",
            "Train Epoch: 23 [200/1400 (14%)]\tLoss: 152.260773\n",
            "Train Epoch: 23 [240/1400 (17%)]\tLoss: 212.918625\n",
            "Train Epoch: 23 [280/1400 (20%)]\tLoss: 136.989685\n",
            "Train Epoch: 23 [320/1400 (23%)]\tLoss: 222.795685\n",
            "Train Epoch: 23 [360/1400 (26%)]\tLoss: 141.774170\n",
            "Train Epoch: 23 [400/1400 (29%)]\tLoss: 210.524353\n",
            "Train Epoch: 23 [440/1400 (31%)]\tLoss: 152.210083\n",
            "Train Epoch: 23 [480/1400 (34%)]\tLoss: 133.562103\n",
            "Train Epoch: 23 [520/1400 (37%)]\tLoss: 200.128448\n",
            "Train Epoch: 23 [560/1400 (40%)]\tLoss: 165.297363\n",
            "Train Epoch: 23 [600/1400 (43%)]\tLoss: 157.278625\n",
            "Train Epoch: 23 [640/1400 (46%)]\tLoss: 148.942490\n",
            "Train Epoch: 23 [680/1400 (49%)]\tLoss: 121.637955\n",
            "Train Epoch: 23 [720/1400 (51%)]\tLoss: 179.481842\n",
            "Train Epoch: 23 [760/1400 (54%)]\tLoss: 155.932846\n",
            "Train Epoch: 23 [800/1400 (57%)]\tLoss: 176.874634\n",
            "Train Epoch: 23 [840/1400 (60%)]\tLoss: 152.071442\n",
            "Train Epoch: 23 [880/1400 (63%)]\tLoss: 148.096741\n",
            "Train Epoch: 23 [920/1400 (66%)]\tLoss: 176.535614\n",
            "Train Epoch: 23 [960/1400 (69%)]\tLoss: 165.203186\n",
            "Train Epoch: 23 [1000/1400 (71%)]\tLoss: 127.986778\n",
            "Train Epoch: 23 [1040/1400 (74%)]\tLoss: 161.486649\n",
            "Train Epoch: 23 [1080/1400 (77%)]\tLoss: 193.992447\n",
            "Train Epoch: 23 [1120/1400 (80%)]\tLoss: 122.681076\n",
            "Train Epoch: 23 [1160/1400 (83%)]\tLoss: 189.017090\n",
            "Train Epoch: 23 [1200/1400 (86%)]\tLoss: 144.868317\n",
            "Train Epoch: 23 [1240/1400 (89%)]\tLoss: 130.086334\n",
            "Train Epoch: 23 [1280/1400 (91%)]\tLoss: 222.875061\n",
            "Train Epoch: 23 [1320/1400 (94%)]\tLoss: 226.507141\n",
            "Train Epoch: 23 [1360/1400 (97%)]\tLoss: 164.928818\n",
            "====> Epoch: 23 Average loss: 168.4668\n",
            "====> Test set loss: 171.2185\n",
            "Train Epoch: 24 [0/1400 (0%)]\tLoss: 128.865417\n",
            "Train Epoch: 24 [40/1400 (3%)]\tLoss: 157.872528\n",
            "Train Epoch: 24 [80/1400 (6%)]\tLoss: 131.542694\n",
            "Train Epoch: 24 [120/1400 (9%)]\tLoss: 153.853180\n",
            "Train Epoch: 24 [160/1400 (11%)]\tLoss: 204.861572\n",
            "Train Epoch: 24 [200/1400 (14%)]\tLoss: 152.308655\n",
            "Train Epoch: 24 [240/1400 (17%)]\tLoss: 219.311386\n",
            "Train Epoch: 24 [280/1400 (20%)]\tLoss: 175.137115\n",
            "Train Epoch: 24 [320/1400 (23%)]\tLoss: 183.734314\n",
            "Train Epoch: 24 [360/1400 (26%)]\tLoss: 133.562805\n",
            "Train Epoch: 24 [400/1400 (29%)]\tLoss: 174.794128\n",
            "Train Epoch: 24 [440/1400 (31%)]\tLoss: 161.628906\n",
            "Train Epoch: 24 [480/1400 (34%)]\tLoss: 155.546478\n",
            "Train Epoch: 24 [520/1400 (37%)]\tLoss: 181.429413\n",
            "Train Epoch: 24 [560/1400 (40%)]\tLoss: 122.766487\n",
            "Train Epoch: 24 [600/1400 (43%)]\tLoss: 146.035980\n",
            "Train Epoch: 24 [640/1400 (46%)]\tLoss: 148.404953\n",
            "Train Epoch: 24 [680/1400 (49%)]\tLoss: 173.169693\n",
            "Train Epoch: 24 [720/1400 (51%)]\tLoss: 182.758560\n",
            "Train Epoch: 24 [760/1400 (54%)]\tLoss: 182.903320\n",
            "Train Epoch: 24 [800/1400 (57%)]\tLoss: 174.671478\n",
            "Train Epoch: 24 [840/1400 (60%)]\tLoss: 194.249817\n",
            "Train Epoch: 24 [880/1400 (63%)]\tLoss: 174.367981\n",
            "Train Epoch: 24 [920/1400 (66%)]\tLoss: 159.390396\n",
            "Train Epoch: 24 [960/1400 (69%)]\tLoss: 245.049316\n",
            "Train Epoch: 24 [1000/1400 (71%)]\tLoss: 171.664551\n",
            "Train Epoch: 24 [1040/1400 (74%)]\tLoss: 193.105011\n",
            "Train Epoch: 24 [1080/1400 (77%)]\tLoss: 151.648499\n",
            "Train Epoch: 24 [1120/1400 (80%)]\tLoss: 191.508423\n",
            "Train Epoch: 24 [1160/1400 (83%)]\tLoss: 174.131378\n",
            "Train Epoch: 24 [1200/1400 (86%)]\tLoss: 147.260162\n",
            "Train Epoch: 24 [1240/1400 (89%)]\tLoss: 146.427429\n",
            "Train Epoch: 24 [1280/1400 (91%)]\tLoss: 150.782928\n",
            "Train Epoch: 24 [1320/1400 (94%)]\tLoss: 188.985916\n",
            "Train Epoch: 24 [1360/1400 (97%)]\tLoss: 161.487305\n",
            "====> Epoch: 24 Average loss: 168.0733\n",
            "====> Test set loss: 170.5565\n",
            "Train Epoch: 25 [0/1400 (0%)]\tLoss: 148.139374\n",
            "Train Epoch: 25 [40/1400 (3%)]\tLoss: 149.858002\n",
            "Train Epoch: 25 [80/1400 (6%)]\tLoss: 168.314133\n",
            "Train Epoch: 25 [120/1400 (9%)]\tLoss: 141.092407\n",
            "Train Epoch: 25 [160/1400 (11%)]\tLoss: 154.494492\n",
            "Train Epoch: 25 [200/1400 (14%)]\tLoss: 161.470566\n",
            "Train Epoch: 25 [240/1400 (17%)]\tLoss: 143.879639\n",
            "Train Epoch: 25 [280/1400 (20%)]\tLoss: 138.932159\n",
            "Train Epoch: 25 [320/1400 (23%)]\tLoss: 168.455353\n",
            "Train Epoch: 25 [360/1400 (26%)]\tLoss: 154.432739\n",
            "Train Epoch: 25 [400/1400 (29%)]\tLoss: 236.116608\n",
            "Train Epoch: 25 [440/1400 (31%)]\tLoss: 140.810730\n",
            "Train Epoch: 25 [480/1400 (34%)]\tLoss: 197.949493\n",
            "Train Epoch: 25 [520/1400 (37%)]\tLoss: 148.909775\n",
            "Train Epoch: 25 [560/1400 (40%)]\tLoss: 170.989822\n",
            "Train Epoch: 25 [600/1400 (43%)]\tLoss: 184.835815\n",
            "Train Epoch: 25 [640/1400 (46%)]\tLoss: 151.951187\n",
            "Train Epoch: 25 [680/1400 (49%)]\tLoss: 152.224228\n",
            "Train Epoch: 25 [720/1400 (51%)]\tLoss: 156.303909\n",
            "Train Epoch: 25 [760/1400 (54%)]\tLoss: 165.456879\n",
            "Train Epoch: 25 [800/1400 (57%)]\tLoss: 185.033340\n",
            "Train Epoch: 25 [840/1400 (60%)]\tLoss: 129.533646\n",
            "Train Epoch: 25 [880/1400 (63%)]\tLoss: 207.072861\n",
            "Train Epoch: 25 [920/1400 (66%)]\tLoss: 157.276840\n",
            "Train Epoch: 25 [960/1400 (69%)]\tLoss: 194.237625\n",
            "Train Epoch: 25 [1000/1400 (71%)]\tLoss: 181.923752\n",
            "Train Epoch: 25 [1040/1400 (74%)]\tLoss: 130.417618\n",
            "Train Epoch: 25 [1080/1400 (77%)]\tLoss: 202.004517\n",
            "Train Epoch: 25 [1120/1400 (80%)]\tLoss: 159.081512\n",
            "Train Epoch: 25 [1160/1400 (83%)]\tLoss: 147.038452\n",
            "Train Epoch: 25 [1200/1400 (86%)]\tLoss: 180.999725\n",
            "Train Epoch: 25 [1240/1400 (89%)]\tLoss: 204.687195\n",
            "Train Epoch: 25 [1280/1400 (91%)]\tLoss: 150.133087\n",
            "Train Epoch: 25 [1320/1400 (94%)]\tLoss: 220.894135\n",
            "Train Epoch: 25 [1360/1400 (97%)]\tLoss: 162.051224\n",
            "====> Epoch: 25 Average loss: 167.4067\n",
            "====> Test set loss: 169.9011\n",
            "Train Epoch: 26 [0/1400 (0%)]\tLoss: 197.433365\n",
            "Train Epoch: 26 [40/1400 (3%)]\tLoss: 143.603516\n",
            "Train Epoch: 26 [80/1400 (6%)]\tLoss: 172.388687\n",
            "Train Epoch: 26 [120/1400 (9%)]\tLoss: 171.769958\n",
            "Train Epoch: 26 [160/1400 (11%)]\tLoss: 169.221390\n",
            "Train Epoch: 26 [200/1400 (14%)]\tLoss: 157.330414\n",
            "Train Epoch: 26 [240/1400 (17%)]\tLoss: 199.835602\n",
            "Train Epoch: 26 [280/1400 (20%)]\tLoss: 140.185287\n",
            "Train Epoch: 26 [320/1400 (23%)]\tLoss: 170.377365\n",
            "Train Epoch: 26 [360/1400 (26%)]\tLoss: 180.434723\n",
            "Train Epoch: 26 [400/1400 (29%)]\tLoss: 150.586395\n",
            "Train Epoch: 26 [440/1400 (31%)]\tLoss: 127.737999\n",
            "Train Epoch: 26 [480/1400 (34%)]\tLoss: 139.418274\n",
            "Train Epoch: 26 [520/1400 (37%)]\tLoss: 179.418457\n",
            "Train Epoch: 26 [560/1400 (40%)]\tLoss: 162.180862\n",
            "Train Epoch: 26 [600/1400 (43%)]\tLoss: 187.389221\n",
            "Train Epoch: 26 [640/1400 (46%)]\tLoss: 164.508362\n",
            "Train Epoch: 26 [680/1400 (49%)]\tLoss: 178.232117\n",
            "Train Epoch: 26 [720/1400 (51%)]\tLoss: 183.700180\n",
            "Train Epoch: 26 [760/1400 (54%)]\tLoss: 173.195663\n",
            "Train Epoch: 26 [800/1400 (57%)]\tLoss: 182.931305\n",
            "Train Epoch: 26 [840/1400 (60%)]\tLoss: 156.098541\n",
            "Train Epoch: 26 [880/1400 (63%)]\tLoss: 149.932724\n",
            "Train Epoch: 26 [920/1400 (66%)]\tLoss: 142.288345\n",
            "Train Epoch: 26 [960/1400 (69%)]\tLoss: 143.513031\n",
            "Train Epoch: 26 [1000/1400 (71%)]\tLoss: 167.544891\n",
            "Train Epoch: 26 [1040/1400 (74%)]\tLoss: 162.861099\n",
            "Train Epoch: 26 [1080/1400 (77%)]\tLoss: 134.130112\n",
            "Train Epoch: 26 [1120/1400 (80%)]\tLoss: 143.979156\n",
            "Train Epoch: 26 [1160/1400 (83%)]\tLoss: 163.045380\n",
            "Train Epoch: 26 [1200/1400 (86%)]\tLoss: 167.125885\n",
            "Train Epoch: 26 [1240/1400 (89%)]\tLoss: 160.327347\n",
            "Train Epoch: 26 [1280/1400 (91%)]\tLoss: 154.065353\n",
            "Train Epoch: 26 [1320/1400 (94%)]\tLoss: 153.705444\n",
            "Train Epoch: 26 [1360/1400 (97%)]\tLoss: 179.121399\n",
            "====> Epoch: 26 Average loss: 166.6547\n",
            "====> Test set loss: 168.9968\n",
            "Train Epoch: 27 [0/1400 (0%)]\tLoss: 169.596115\n",
            "Train Epoch: 27 [40/1400 (3%)]\tLoss: 135.528732\n",
            "Train Epoch: 27 [80/1400 (6%)]\tLoss: 159.987854\n",
            "Train Epoch: 27 [120/1400 (9%)]\tLoss: 189.274139\n",
            "Train Epoch: 27 [160/1400 (11%)]\tLoss: 179.410980\n",
            "Train Epoch: 27 [200/1400 (14%)]\tLoss: 155.751312\n",
            "Train Epoch: 27 [240/1400 (17%)]\tLoss: 175.717499\n",
            "Train Epoch: 27 [280/1400 (20%)]\tLoss: 142.022522\n",
            "Train Epoch: 27 [320/1400 (23%)]\tLoss: 198.114136\n",
            "Train Epoch: 27 [360/1400 (26%)]\tLoss: 192.340500\n",
            "Train Epoch: 27 [400/1400 (29%)]\tLoss: 116.366989\n",
            "Train Epoch: 27 [440/1400 (31%)]\tLoss: 165.065399\n",
            "Train Epoch: 27 [480/1400 (34%)]\tLoss: 119.075722\n",
            "Train Epoch: 27 [520/1400 (37%)]\tLoss: 132.740112\n",
            "Train Epoch: 27 [560/1400 (40%)]\tLoss: 161.267990\n",
            "Train Epoch: 27 [600/1400 (43%)]\tLoss: 188.255402\n",
            "Train Epoch: 27 [640/1400 (46%)]\tLoss: 170.790970\n",
            "Train Epoch: 27 [680/1400 (49%)]\tLoss: 138.665405\n",
            "Train Epoch: 27 [720/1400 (51%)]\tLoss: 164.991730\n",
            "Train Epoch: 27 [760/1400 (54%)]\tLoss: 149.548813\n",
            "Train Epoch: 27 [800/1400 (57%)]\tLoss: 168.464676\n",
            "Train Epoch: 27 [840/1400 (60%)]\tLoss: 187.198502\n",
            "Train Epoch: 27 [880/1400 (63%)]\tLoss: 184.825485\n",
            "Train Epoch: 27 [920/1400 (66%)]\tLoss: 224.857178\n",
            "Train Epoch: 27 [960/1400 (69%)]\tLoss: 166.941406\n",
            "Train Epoch: 27 [1000/1400 (71%)]\tLoss: 144.241150\n",
            "Train Epoch: 27 [1040/1400 (74%)]\tLoss: 171.003723\n",
            "Train Epoch: 27 [1080/1400 (77%)]\tLoss: 129.450424\n",
            "Train Epoch: 27 [1120/1400 (80%)]\tLoss: 163.328674\n",
            "Train Epoch: 27 [1160/1400 (83%)]\tLoss: 165.931152\n",
            "Train Epoch: 27 [1200/1400 (86%)]\tLoss: 140.654053\n",
            "Train Epoch: 27 [1240/1400 (89%)]\tLoss: 201.542679\n",
            "Train Epoch: 27 [1280/1400 (91%)]\tLoss: 165.692276\n",
            "Train Epoch: 27 [1320/1400 (94%)]\tLoss: 187.846710\n",
            "Train Epoch: 27 [1360/1400 (97%)]\tLoss: 155.518097\n",
            "====> Epoch: 27 Average loss: 165.6882\n",
            "====> Test set loss: 167.7112\n",
            "Train Epoch: 28 [0/1400 (0%)]\tLoss: 139.889801\n",
            "Train Epoch: 28 [40/1400 (3%)]\tLoss: 138.896622\n",
            "Train Epoch: 28 [80/1400 (6%)]\tLoss: 129.868042\n",
            "Train Epoch: 28 [120/1400 (9%)]\tLoss: 145.267502\n",
            "Train Epoch: 28 [160/1400 (11%)]\tLoss: 175.900726\n",
            "Train Epoch: 28 [200/1400 (14%)]\tLoss: 204.768799\n",
            "Train Epoch: 28 [240/1400 (17%)]\tLoss: 196.774048\n",
            "Train Epoch: 28 [280/1400 (20%)]\tLoss: 222.904541\n",
            "Train Epoch: 28 [320/1400 (23%)]\tLoss: 156.820465\n",
            "Train Epoch: 28 [360/1400 (26%)]\tLoss: 161.284500\n",
            "Train Epoch: 28 [400/1400 (29%)]\tLoss: 140.339996\n",
            "Train Epoch: 28 [440/1400 (31%)]\tLoss: 161.740845\n",
            "Train Epoch: 28 [480/1400 (34%)]\tLoss: 127.983231\n",
            "Train Epoch: 28 [520/1400 (37%)]\tLoss: 149.480896\n",
            "Train Epoch: 28 [560/1400 (40%)]\tLoss: 138.381104\n",
            "Train Epoch: 28 [600/1400 (43%)]\tLoss: 170.211136\n",
            "Train Epoch: 28 [640/1400 (46%)]\tLoss: 137.088593\n",
            "Train Epoch: 28 [680/1400 (49%)]\tLoss: 157.327957\n",
            "Train Epoch: 28 [720/1400 (51%)]\tLoss: 148.663849\n",
            "Train Epoch: 28 [760/1400 (54%)]\tLoss: 159.129288\n",
            "Train Epoch: 28 [800/1400 (57%)]\tLoss: 132.860107\n",
            "Train Epoch: 28 [840/1400 (60%)]\tLoss: 165.865433\n",
            "Train Epoch: 28 [880/1400 (63%)]\tLoss: 152.713089\n",
            "Train Epoch: 28 [920/1400 (66%)]\tLoss: 150.988251\n",
            "Train Epoch: 28 [960/1400 (69%)]\tLoss: 178.459244\n",
            "Train Epoch: 28 [1000/1400 (71%)]\tLoss: 136.938202\n",
            "Train Epoch: 28 [1040/1400 (74%)]\tLoss: 168.702942\n",
            "Train Epoch: 28 [1080/1400 (77%)]\tLoss: 166.370651\n",
            "Train Epoch: 28 [1120/1400 (80%)]\tLoss: 243.860626\n",
            "Train Epoch: 28 [1160/1400 (83%)]\tLoss: 119.296318\n",
            "Train Epoch: 28 [1200/1400 (86%)]\tLoss: 178.413849\n",
            "Train Epoch: 28 [1240/1400 (89%)]\tLoss: 192.440353\n",
            "Train Epoch: 28 [1280/1400 (91%)]\tLoss: 185.718781\n",
            "Train Epoch: 28 [1320/1400 (94%)]\tLoss: 146.749054\n",
            "Train Epoch: 28 [1360/1400 (97%)]\tLoss: 150.234207\n",
            "====> Epoch: 28 Average loss: 164.5350\n",
            "====> Test set loss: 166.3681\n",
            "Train Epoch: 29 [0/1400 (0%)]\tLoss: 133.408798\n",
            "Train Epoch: 29 [40/1400 (3%)]\tLoss: 180.639404\n",
            "Train Epoch: 29 [80/1400 (6%)]\tLoss: 131.275116\n",
            "Train Epoch: 29 [120/1400 (9%)]\tLoss: 220.615372\n",
            "Train Epoch: 29 [160/1400 (11%)]\tLoss: 205.994507\n",
            "Train Epoch: 29 [200/1400 (14%)]\tLoss: 137.269409\n",
            "Train Epoch: 29 [240/1400 (17%)]\tLoss: 142.908112\n",
            "Train Epoch: 29 [280/1400 (20%)]\tLoss: 158.556671\n",
            "Train Epoch: 29 [320/1400 (23%)]\tLoss: 231.791992\n",
            "Train Epoch: 29 [360/1400 (26%)]\tLoss: 187.507751\n",
            "Train Epoch: 29 [400/1400 (29%)]\tLoss: 179.991699\n",
            "Train Epoch: 29 [440/1400 (31%)]\tLoss: 184.294189\n",
            "Train Epoch: 29 [480/1400 (34%)]\tLoss: 142.626801\n",
            "Train Epoch: 29 [520/1400 (37%)]\tLoss: 137.333450\n",
            "Train Epoch: 29 [560/1400 (40%)]\tLoss: 132.963043\n",
            "Train Epoch: 29 [600/1400 (43%)]\tLoss: 160.306442\n",
            "Train Epoch: 29 [640/1400 (46%)]\tLoss: 204.619812\n",
            "Train Epoch: 29 [680/1400 (49%)]\tLoss: 157.473923\n",
            "Train Epoch: 29 [720/1400 (51%)]\tLoss: 131.355545\n",
            "Train Epoch: 29 [760/1400 (54%)]\tLoss: 144.934616\n",
            "Train Epoch: 29 [800/1400 (57%)]\tLoss: 149.269470\n",
            "Train Epoch: 29 [840/1400 (60%)]\tLoss: 147.885834\n",
            "Train Epoch: 29 [880/1400 (63%)]\tLoss: 153.284927\n",
            "Train Epoch: 29 [920/1400 (66%)]\tLoss: 169.997772\n",
            "Train Epoch: 29 [960/1400 (69%)]\tLoss: 205.599930\n",
            "Train Epoch: 29 [1000/1400 (71%)]\tLoss: 148.388947\n",
            "Train Epoch: 29 [1040/1400 (74%)]\tLoss: 194.448273\n",
            "Train Epoch: 29 [1080/1400 (77%)]\tLoss: 150.281982\n",
            "Train Epoch: 29 [1120/1400 (80%)]\tLoss: 168.411987\n",
            "Train Epoch: 29 [1160/1400 (83%)]\tLoss: 157.316803\n",
            "Train Epoch: 29 [1200/1400 (86%)]\tLoss: 203.601379\n",
            "Train Epoch: 29 [1240/1400 (89%)]\tLoss: 185.854492\n",
            "Train Epoch: 29 [1280/1400 (91%)]\tLoss: 146.983688\n",
            "Train Epoch: 29 [1320/1400 (94%)]\tLoss: 170.447327\n",
            "Train Epoch: 29 [1360/1400 (97%)]\tLoss: 192.458969\n",
            "====> Epoch: 29 Average loss: 163.0974\n",
            "====> Test set loss: 165.0409\n",
            "Train Epoch: 30 [0/1400 (0%)]\tLoss: 142.440033\n",
            "Train Epoch: 30 [40/1400 (3%)]\tLoss: 146.947067\n",
            "Train Epoch: 30 [80/1400 (6%)]\tLoss: 154.765656\n",
            "Train Epoch: 30 [120/1400 (9%)]\tLoss: 106.031769\n",
            "Train Epoch: 30 [160/1400 (11%)]\tLoss: 173.548584\n",
            "Train Epoch: 30 [200/1400 (14%)]\tLoss: 171.434219\n",
            "Train Epoch: 30 [240/1400 (17%)]\tLoss: 152.475067\n",
            "Train Epoch: 30 [280/1400 (20%)]\tLoss: 196.466736\n",
            "Train Epoch: 30 [320/1400 (23%)]\tLoss: 137.092255\n",
            "Train Epoch: 30 [360/1400 (26%)]\tLoss: 143.098297\n",
            "Train Epoch: 30 [400/1400 (29%)]\tLoss: 156.058395\n",
            "Train Epoch: 30 [440/1400 (31%)]\tLoss: 240.690552\n",
            "Train Epoch: 30 [480/1400 (34%)]\tLoss: 141.888351\n",
            "Train Epoch: 30 [520/1400 (37%)]\tLoss: 179.485565\n",
            "Train Epoch: 30 [560/1400 (40%)]\tLoss: 137.406357\n",
            "Train Epoch: 30 [600/1400 (43%)]\tLoss: 139.067184\n",
            "Train Epoch: 30 [640/1400 (46%)]\tLoss: 135.957565\n",
            "Train Epoch: 30 [680/1400 (49%)]\tLoss: 149.130447\n",
            "Train Epoch: 30 [720/1400 (51%)]\tLoss: 166.679245\n",
            "Train Epoch: 30 [760/1400 (54%)]\tLoss: 170.407867\n",
            "Train Epoch: 30 [800/1400 (57%)]\tLoss: 159.611221\n",
            "Train Epoch: 30 [840/1400 (60%)]\tLoss: 172.033096\n",
            "Train Epoch: 30 [880/1400 (63%)]\tLoss: 139.507675\n",
            "Train Epoch: 30 [920/1400 (66%)]\tLoss: 161.340240\n",
            "Train Epoch: 30 [960/1400 (69%)]\tLoss: 133.870712\n",
            "Train Epoch: 30 [1000/1400 (71%)]\tLoss: 190.473419\n",
            "Train Epoch: 30 [1040/1400 (74%)]\tLoss: 151.169769\n",
            "Train Epoch: 30 [1080/1400 (77%)]\tLoss: 172.332504\n",
            "Train Epoch: 30 [1120/1400 (80%)]\tLoss: 175.479156\n",
            "Train Epoch: 30 [1160/1400 (83%)]\tLoss: 135.031708\n",
            "Train Epoch: 30 [1200/1400 (86%)]\tLoss: 158.534866\n",
            "Train Epoch: 30 [1240/1400 (89%)]\tLoss: 193.741333\n",
            "Train Epoch: 30 [1280/1400 (91%)]\tLoss: 175.807846\n",
            "Train Epoch: 30 [1320/1400 (94%)]\tLoss: 138.356583\n",
            "Train Epoch: 30 [1360/1400 (97%)]\tLoss: 133.698502\n",
            "====> Epoch: 30 Average loss: 161.7900\n",
            "====> Test set loss: 163.4192\n",
            "Train Epoch: 31 [0/1400 (0%)]\tLoss: 138.251450\n",
            "Train Epoch: 31 [40/1400 (3%)]\tLoss: 170.991150\n",
            "Train Epoch: 31 [80/1400 (6%)]\tLoss: 130.674606\n",
            "Train Epoch: 31 [120/1400 (9%)]\tLoss: 154.685974\n",
            "Train Epoch: 31 [160/1400 (11%)]\tLoss: 141.798996\n",
            "Train Epoch: 31 [200/1400 (14%)]\tLoss: 202.426300\n",
            "Train Epoch: 31 [240/1400 (17%)]\tLoss: 125.381195\n",
            "Train Epoch: 31 [280/1400 (20%)]\tLoss: 157.509888\n",
            "Train Epoch: 31 [320/1400 (23%)]\tLoss: 140.091309\n",
            "Train Epoch: 31 [360/1400 (26%)]\tLoss: 147.238907\n",
            "Train Epoch: 31 [400/1400 (29%)]\tLoss: 157.878967\n",
            "Train Epoch: 31 [440/1400 (31%)]\tLoss: 141.098221\n",
            "Train Epoch: 31 [480/1400 (34%)]\tLoss: 124.526062\n",
            "Train Epoch: 31 [520/1400 (37%)]\tLoss: 156.580429\n",
            "Train Epoch: 31 [560/1400 (40%)]\tLoss: 175.465622\n",
            "Train Epoch: 31 [600/1400 (43%)]\tLoss: 178.573334\n",
            "Train Epoch: 31 [640/1400 (46%)]\tLoss: 166.395264\n",
            "Train Epoch: 31 [680/1400 (49%)]\tLoss: 160.402390\n",
            "Train Epoch: 31 [720/1400 (51%)]\tLoss: 171.905975\n",
            "Train Epoch: 31 [760/1400 (54%)]\tLoss: 164.330353\n",
            "Train Epoch: 31 [800/1400 (57%)]\tLoss: 173.728271\n",
            "Train Epoch: 31 [840/1400 (60%)]\tLoss: 175.464905\n",
            "Train Epoch: 31 [880/1400 (63%)]\tLoss: 166.690308\n",
            "Train Epoch: 31 [920/1400 (66%)]\tLoss: 168.859268\n",
            "Train Epoch: 31 [960/1400 (69%)]\tLoss: 156.218994\n",
            "Train Epoch: 31 [1000/1400 (71%)]\tLoss: 193.110870\n",
            "Train Epoch: 31 [1040/1400 (74%)]\tLoss: 143.857758\n",
            "Train Epoch: 31 [1080/1400 (77%)]\tLoss: 195.932465\n",
            "Train Epoch: 31 [1120/1400 (80%)]\tLoss: 182.887054\n",
            "Train Epoch: 31 [1160/1400 (83%)]\tLoss: 130.851379\n",
            "Train Epoch: 31 [1200/1400 (86%)]\tLoss: 155.648163\n",
            "Train Epoch: 31 [1240/1400 (89%)]\tLoss: 158.962234\n",
            "Train Epoch: 31 [1280/1400 (91%)]\tLoss: 144.713287\n",
            "Train Epoch: 31 [1320/1400 (94%)]\tLoss: 145.294296\n",
            "Train Epoch: 31 [1360/1400 (97%)]\tLoss: 187.339203\n",
            "====> Epoch: 31 Average loss: 160.4507\n",
            "====> Test set loss: 162.1034\n",
            "Train Epoch: 32 [0/1400 (0%)]\tLoss: 150.797379\n",
            "Train Epoch: 32 [40/1400 (3%)]\tLoss: 150.862473\n",
            "Train Epoch: 32 [80/1400 (6%)]\tLoss: 154.293396\n",
            "Train Epoch: 32 [120/1400 (9%)]\tLoss: 174.110947\n",
            "Train Epoch: 32 [160/1400 (11%)]\tLoss: 207.900253\n",
            "Train Epoch: 32 [200/1400 (14%)]\tLoss: 124.002441\n",
            "Train Epoch: 32 [240/1400 (17%)]\tLoss: 163.503433\n",
            "Train Epoch: 32 [280/1400 (20%)]\tLoss: 187.462860\n",
            "Train Epoch: 32 [320/1400 (23%)]\tLoss: 171.031662\n",
            "Train Epoch: 32 [360/1400 (26%)]\tLoss: 159.163254\n",
            "Train Epoch: 32 [400/1400 (29%)]\tLoss: 183.277618\n",
            "Train Epoch: 32 [440/1400 (31%)]\tLoss: 142.053009\n",
            "Train Epoch: 32 [480/1400 (34%)]\tLoss: 140.319595\n",
            "Train Epoch: 32 [520/1400 (37%)]\tLoss: 183.961777\n",
            "Train Epoch: 32 [560/1400 (40%)]\tLoss: 202.035248\n",
            "Train Epoch: 32 [600/1400 (43%)]\tLoss: 193.909164\n",
            "Train Epoch: 32 [640/1400 (46%)]\tLoss: 175.524521\n",
            "Train Epoch: 32 [680/1400 (49%)]\tLoss: 139.276199\n",
            "Train Epoch: 32 [720/1400 (51%)]\tLoss: 148.023285\n",
            "Train Epoch: 32 [760/1400 (54%)]\tLoss: 162.746674\n",
            "Train Epoch: 32 [800/1400 (57%)]\tLoss: 163.820282\n",
            "Train Epoch: 32 [840/1400 (60%)]\tLoss: 170.338135\n",
            "Train Epoch: 32 [880/1400 (63%)]\tLoss: 167.433960\n",
            "Train Epoch: 32 [920/1400 (66%)]\tLoss: 176.794693\n",
            "Train Epoch: 32 [960/1400 (69%)]\tLoss: 157.715530\n",
            "Train Epoch: 32 [1000/1400 (71%)]\tLoss: 158.821625\n",
            "Train Epoch: 32 [1040/1400 (74%)]\tLoss: 162.849594\n",
            "Train Epoch: 32 [1080/1400 (77%)]\tLoss: 143.366211\n",
            "Train Epoch: 32 [1120/1400 (80%)]\tLoss: 170.550262\n",
            "Train Epoch: 32 [1160/1400 (83%)]\tLoss: 197.557800\n",
            "Train Epoch: 32 [1200/1400 (86%)]\tLoss: 137.975510\n",
            "Train Epoch: 32 [1240/1400 (89%)]\tLoss: 134.952255\n",
            "Train Epoch: 32 [1280/1400 (91%)]\tLoss: 138.746521\n",
            "Train Epoch: 32 [1320/1400 (94%)]\tLoss: 196.316254\n",
            "Train Epoch: 32 [1360/1400 (97%)]\tLoss: 141.709244\n",
            "====> Epoch: 32 Average loss: 159.2279\n",
            "====> Test set loss: 160.8955\n",
            "Train Epoch: 33 [0/1400 (0%)]\tLoss: 154.848114\n",
            "Train Epoch: 33 [40/1400 (3%)]\tLoss: 126.128250\n",
            "Train Epoch: 33 [80/1400 (6%)]\tLoss: 145.431793\n",
            "Train Epoch: 33 [120/1400 (9%)]\tLoss: 214.829681\n",
            "Train Epoch: 33 [160/1400 (11%)]\tLoss: 127.640244\n",
            "Train Epoch: 33 [200/1400 (14%)]\tLoss: 172.356567\n",
            "Train Epoch: 33 [240/1400 (17%)]\tLoss: 138.733688\n",
            "Train Epoch: 33 [280/1400 (20%)]\tLoss: 140.403397\n",
            "Train Epoch: 33 [320/1400 (23%)]\tLoss: 162.610428\n",
            "Train Epoch: 33 [360/1400 (26%)]\tLoss: 154.543762\n",
            "Train Epoch: 33 [400/1400 (29%)]\tLoss: 154.579468\n",
            "Train Epoch: 33 [440/1400 (31%)]\tLoss: 150.476669\n",
            "Train Epoch: 33 [480/1400 (34%)]\tLoss: 159.657394\n",
            "Train Epoch: 33 [520/1400 (37%)]\tLoss: 160.546204\n",
            "Train Epoch: 33 [560/1400 (40%)]\tLoss: 147.933914\n",
            "Train Epoch: 33 [600/1400 (43%)]\tLoss: 162.611160\n",
            "Train Epoch: 33 [640/1400 (46%)]\tLoss: 139.244415\n",
            "Train Epoch: 33 [680/1400 (49%)]\tLoss: 146.017609\n",
            "Train Epoch: 33 [720/1400 (51%)]\tLoss: 167.952469\n",
            "Train Epoch: 33 [760/1400 (54%)]\tLoss: 207.990601\n",
            "Train Epoch: 33 [800/1400 (57%)]\tLoss: 174.770828\n",
            "Train Epoch: 33 [840/1400 (60%)]\tLoss: 127.678879\n",
            "Train Epoch: 33 [880/1400 (63%)]\tLoss: 132.650421\n",
            "Train Epoch: 33 [920/1400 (66%)]\tLoss: 161.356598\n",
            "Train Epoch: 33 [960/1400 (69%)]\tLoss: 158.874817\n",
            "Train Epoch: 33 [1000/1400 (71%)]\tLoss: 177.879242\n",
            "Train Epoch: 33 [1040/1400 (74%)]\tLoss: 170.239960\n",
            "Train Epoch: 33 [1080/1400 (77%)]\tLoss: 122.318977\n",
            "Train Epoch: 33 [1120/1400 (80%)]\tLoss: 156.147186\n",
            "Train Epoch: 33 [1160/1400 (83%)]\tLoss: 156.861862\n",
            "Train Epoch: 33 [1200/1400 (86%)]\tLoss: 150.470947\n",
            "Train Epoch: 33 [1240/1400 (89%)]\tLoss: 156.696213\n",
            "Train Epoch: 33 [1280/1400 (91%)]\tLoss: 138.138107\n",
            "Train Epoch: 33 [1320/1400 (94%)]\tLoss: 167.571854\n",
            "Train Epoch: 33 [1360/1400 (97%)]\tLoss: 145.061584\n",
            "====> Epoch: 33 Average loss: 158.1190\n",
            "====> Test set loss: 159.7250\n",
            "Train Epoch: 34 [0/1400 (0%)]\tLoss: 139.074951\n",
            "Train Epoch: 34 [40/1400 (3%)]\tLoss: 169.570435\n",
            "Train Epoch: 34 [80/1400 (6%)]\tLoss: 160.832626\n",
            "Train Epoch: 34 [120/1400 (9%)]\tLoss: 185.273331\n",
            "Train Epoch: 34 [160/1400 (11%)]\tLoss: 133.751999\n",
            "Train Epoch: 34 [200/1400 (14%)]\tLoss: 155.098007\n",
            "Train Epoch: 34 [240/1400 (17%)]\tLoss: 146.542557\n",
            "Train Epoch: 34 [280/1400 (20%)]\tLoss: 177.607849\n",
            "Train Epoch: 34 [320/1400 (23%)]\tLoss: 169.981293\n",
            "Train Epoch: 34 [360/1400 (26%)]\tLoss: 206.298996\n",
            "Train Epoch: 34 [400/1400 (29%)]\tLoss: 158.453400\n",
            "Train Epoch: 34 [440/1400 (31%)]\tLoss: 162.143524\n",
            "Train Epoch: 34 [480/1400 (34%)]\tLoss: 185.532990\n",
            "Train Epoch: 34 [520/1400 (37%)]\tLoss: 163.881348\n",
            "Train Epoch: 34 [560/1400 (40%)]\tLoss: 154.102325\n",
            "Train Epoch: 34 [600/1400 (43%)]\tLoss: 153.061218\n",
            "Train Epoch: 34 [640/1400 (46%)]\tLoss: 143.905167\n",
            "Train Epoch: 34 [680/1400 (49%)]\tLoss: 208.655975\n",
            "Train Epoch: 34 [720/1400 (51%)]\tLoss: 201.754913\n",
            "Train Epoch: 34 [760/1400 (54%)]\tLoss: 139.455215\n",
            "Train Epoch: 34 [800/1400 (57%)]\tLoss: 130.782196\n",
            "Train Epoch: 34 [840/1400 (60%)]\tLoss: 182.360657\n",
            "Train Epoch: 34 [880/1400 (63%)]\tLoss: 175.220581\n",
            "Train Epoch: 34 [920/1400 (66%)]\tLoss: 142.211243\n",
            "Train Epoch: 34 [960/1400 (69%)]\tLoss: 170.471283\n",
            "Train Epoch: 34 [1000/1400 (71%)]\tLoss: 133.599304\n",
            "Train Epoch: 34 [1040/1400 (74%)]\tLoss: 159.888260\n",
            "Train Epoch: 34 [1080/1400 (77%)]\tLoss: 158.645996\n",
            "Train Epoch: 34 [1120/1400 (80%)]\tLoss: 163.363190\n",
            "Train Epoch: 34 [1160/1400 (83%)]\tLoss: 149.134964\n",
            "Train Epoch: 34 [1200/1400 (86%)]\tLoss: 170.449890\n",
            "Train Epoch: 34 [1240/1400 (89%)]\tLoss: 150.404648\n",
            "Train Epoch: 34 [1280/1400 (91%)]\tLoss: 201.772552\n",
            "Train Epoch: 34 [1320/1400 (94%)]\tLoss: 180.557983\n",
            "Train Epoch: 34 [1360/1400 (97%)]\tLoss: 179.333801\n",
            "====> Epoch: 34 Average loss: 157.0391\n",
            "====> Test set loss: 158.7179\n",
            "Train Epoch: 35 [0/1400 (0%)]\tLoss: 160.761887\n",
            "Train Epoch: 35 [40/1400 (3%)]\tLoss: 187.711975\n",
            "Train Epoch: 35 [80/1400 (6%)]\tLoss: 172.762375\n",
            "Train Epoch: 35 [120/1400 (9%)]\tLoss: 152.502213\n",
            "Train Epoch: 35 [160/1400 (11%)]\tLoss: 158.902603\n",
            "Train Epoch: 35 [200/1400 (14%)]\tLoss: 123.907631\n",
            "Train Epoch: 35 [240/1400 (17%)]\tLoss: 167.260849\n",
            "Train Epoch: 35 [280/1400 (20%)]\tLoss: 141.269012\n",
            "Train Epoch: 35 [320/1400 (23%)]\tLoss: 155.389771\n",
            "Train Epoch: 35 [360/1400 (26%)]\tLoss: 155.519562\n",
            "Train Epoch: 35 [400/1400 (29%)]\tLoss: 169.956146\n",
            "Train Epoch: 35 [440/1400 (31%)]\tLoss: 150.386398\n",
            "Train Epoch: 35 [480/1400 (34%)]\tLoss: 162.930145\n",
            "Train Epoch: 35 [520/1400 (37%)]\tLoss: 132.123871\n",
            "Train Epoch: 35 [560/1400 (40%)]\tLoss: 148.454575\n",
            "Train Epoch: 35 [600/1400 (43%)]\tLoss: 127.298279\n",
            "Train Epoch: 35 [640/1400 (46%)]\tLoss: 152.825775\n",
            "Train Epoch: 35 [680/1400 (49%)]\tLoss: 153.213791\n",
            "Train Epoch: 35 [720/1400 (51%)]\tLoss: 119.819534\n",
            "Train Epoch: 35 [760/1400 (54%)]\tLoss: 163.621048\n",
            "Train Epoch: 35 [800/1400 (57%)]\tLoss: 143.657211\n",
            "Train Epoch: 35 [840/1400 (60%)]\tLoss: 151.278564\n",
            "Train Epoch: 35 [880/1400 (63%)]\tLoss: 140.723434\n",
            "Train Epoch: 35 [920/1400 (66%)]\tLoss: 118.971626\n",
            "Train Epoch: 35 [960/1400 (69%)]\tLoss: 146.955063\n",
            "Train Epoch: 35 [1000/1400 (71%)]\tLoss: 146.292068\n",
            "Train Epoch: 35 [1040/1400 (74%)]\tLoss: 132.802979\n",
            "Train Epoch: 35 [1080/1400 (77%)]\tLoss: 149.599197\n",
            "Train Epoch: 35 [1120/1400 (80%)]\tLoss: 155.892349\n",
            "Train Epoch: 35 [1160/1400 (83%)]\tLoss: 184.889328\n",
            "Train Epoch: 35 [1200/1400 (86%)]\tLoss: 165.022690\n",
            "Train Epoch: 35 [1240/1400 (89%)]\tLoss: 152.149368\n",
            "Train Epoch: 35 [1280/1400 (91%)]\tLoss: 166.765671\n",
            "Train Epoch: 35 [1320/1400 (94%)]\tLoss: 142.456329\n",
            "Train Epoch: 35 [1360/1400 (97%)]\tLoss: 209.638931\n",
            "====> Epoch: 35 Average loss: 156.0806\n",
            "====> Test set loss: 157.9283\n",
            "Train Epoch: 36 [0/1400 (0%)]\tLoss: 166.345947\n",
            "Train Epoch: 36 [40/1400 (3%)]\tLoss: 146.006729\n",
            "Train Epoch: 36 [80/1400 (6%)]\tLoss: 119.699638\n",
            "Train Epoch: 36 [120/1400 (9%)]\tLoss: 147.543854\n",
            "Train Epoch: 36 [160/1400 (11%)]\tLoss: 130.946228\n",
            "Train Epoch: 36 [200/1400 (14%)]\tLoss: 142.224991\n",
            "Train Epoch: 36 [240/1400 (17%)]\tLoss: 192.967896\n",
            "Train Epoch: 36 [280/1400 (20%)]\tLoss: 126.333130\n",
            "Train Epoch: 36 [320/1400 (23%)]\tLoss: 165.704208\n",
            "Train Epoch: 36 [360/1400 (26%)]\tLoss: 156.167313\n",
            "Train Epoch: 36 [400/1400 (29%)]\tLoss: 160.962570\n",
            "Train Epoch: 36 [440/1400 (31%)]\tLoss: 126.065399\n",
            "Train Epoch: 36 [480/1400 (34%)]\tLoss: 143.836227\n",
            "Train Epoch: 36 [520/1400 (37%)]\tLoss: 155.054779\n",
            "Train Epoch: 36 [560/1400 (40%)]\tLoss: 168.833435\n",
            "Train Epoch: 36 [600/1400 (43%)]\tLoss: 144.136780\n",
            "Train Epoch: 36 [640/1400 (46%)]\tLoss: 180.606613\n",
            "Train Epoch: 36 [680/1400 (49%)]\tLoss: 181.593719\n",
            "Train Epoch: 36 [720/1400 (51%)]\tLoss: 153.064590\n",
            "Train Epoch: 36 [760/1400 (54%)]\tLoss: 160.546722\n",
            "Train Epoch: 36 [800/1400 (57%)]\tLoss: 142.625916\n",
            "Train Epoch: 36 [840/1400 (60%)]\tLoss: 159.017059\n",
            "Train Epoch: 36 [880/1400 (63%)]\tLoss: 193.202469\n",
            "Train Epoch: 36 [920/1400 (66%)]\tLoss: 132.650238\n",
            "Train Epoch: 36 [960/1400 (69%)]\tLoss: 186.067719\n",
            "Train Epoch: 36 [1000/1400 (71%)]\tLoss: 172.937836\n",
            "Train Epoch: 36 [1040/1400 (74%)]\tLoss: 137.261810\n",
            "Train Epoch: 36 [1080/1400 (77%)]\tLoss: 144.504974\n",
            "Train Epoch: 36 [1120/1400 (80%)]\tLoss: 196.569077\n",
            "Train Epoch: 36 [1160/1400 (83%)]\tLoss: 165.516693\n",
            "Train Epoch: 36 [1200/1400 (86%)]\tLoss: 202.022369\n",
            "Train Epoch: 36 [1240/1400 (89%)]\tLoss: 178.941757\n",
            "Train Epoch: 36 [1280/1400 (91%)]\tLoss: 146.707642\n",
            "Train Epoch: 36 [1320/1400 (94%)]\tLoss: 159.632767\n",
            "Train Epoch: 36 [1360/1400 (97%)]\tLoss: 141.493347\n",
            "====> Epoch: 36 Average loss: 155.1105\n",
            "====> Test set loss: 156.9650\n",
            "Train Epoch: 37 [0/1400 (0%)]\tLoss: 154.804810\n",
            "Train Epoch: 37 [40/1400 (3%)]\tLoss: 136.312531\n",
            "Train Epoch: 37 [80/1400 (6%)]\tLoss: 153.980301\n",
            "Train Epoch: 37 [120/1400 (9%)]\tLoss: 156.621338\n",
            "Train Epoch: 37 [160/1400 (11%)]\tLoss: 169.528625\n",
            "Train Epoch: 37 [200/1400 (14%)]\tLoss: 173.456848\n",
            "Train Epoch: 37 [240/1400 (17%)]\tLoss: 175.524307\n",
            "Train Epoch: 37 [280/1400 (20%)]\tLoss: 142.231216\n",
            "Train Epoch: 37 [320/1400 (23%)]\tLoss: 158.892227\n",
            "Train Epoch: 37 [360/1400 (26%)]\tLoss: 146.070755\n",
            "Train Epoch: 37 [400/1400 (29%)]\tLoss: 183.544266\n",
            "Train Epoch: 37 [440/1400 (31%)]\tLoss: 182.950928\n",
            "Train Epoch: 37 [480/1400 (34%)]\tLoss: 187.579071\n",
            "Train Epoch: 37 [520/1400 (37%)]\tLoss: 152.285767\n",
            "Train Epoch: 37 [560/1400 (40%)]\tLoss: 153.453583\n",
            "Train Epoch: 37 [600/1400 (43%)]\tLoss: 166.357071\n",
            "Train Epoch: 37 [640/1400 (46%)]\tLoss: 148.020020\n",
            "Train Epoch: 37 [680/1400 (49%)]\tLoss: 154.243454\n",
            "Train Epoch: 37 [720/1400 (51%)]\tLoss: 155.264893\n",
            "Train Epoch: 37 [760/1400 (54%)]\tLoss: 130.823959\n",
            "Train Epoch: 37 [800/1400 (57%)]\tLoss: 124.095512\n",
            "Train Epoch: 37 [840/1400 (60%)]\tLoss: 134.041183\n",
            "Train Epoch: 37 [880/1400 (63%)]\tLoss: 152.668564\n",
            "Train Epoch: 37 [920/1400 (66%)]\tLoss: 150.868408\n",
            "Train Epoch: 37 [960/1400 (69%)]\tLoss: 122.262039\n",
            "Train Epoch: 37 [1000/1400 (71%)]\tLoss: 144.589096\n",
            "Train Epoch: 37 [1040/1400 (74%)]\tLoss: 167.342361\n",
            "Train Epoch: 37 [1080/1400 (77%)]\tLoss: 153.393829\n",
            "Train Epoch: 37 [1120/1400 (80%)]\tLoss: 151.108292\n",
            "Train Epoch: 37 [1160/1400 (83%)]\tLoss: 180.352661\n",
            "Train Epoch: 37 [1200/1400 (86%)]\tLoss: 132.096008\n",
            "Train Epoch: 37 [1240/1400 (89%)]\tLoss: 163.463821\n",
            "Train Epoch: 37 [1280/1400 (91%)]\tLoss: 146.061447\n",
            "Train Epoch: 37 [1320/1400 (94%)]\tLoss: 152.887283\n",
            "Train Epoch: 37 [1360/1400 (97%)]\tLoss: 144.533295\n",
            "====> Epoch: 37 Average loss: 154.1055\n",
            "====> Test set loss: 156.2023\n",
            "Train Epoch: 38 [0/1400 (0%)]\tLoss: 134.872818\n",
            "Train Epoch: 38 [40/1400 (3%)]\tLoss: 153.808594\n",
            "Train Epoch: 38 [80/1400 (6%)]\tLoss: 133.867874\n",
            "Train Epoch: 38 [120/1400 (9%)]\tLoss: 143.234070\n",
            "Train Epoch: 38 [160/1400 (11%)]\tLoss: 141.990479\n",
            "Train Epoch: 38 [200/1400 (14%)]\tLoss: 183.165359\n",
            "Train Epoch: 38 [240/1400 (17%)]\tLoss: 193.697800\n",
            "Train Epoch: 38 [280/1400 (20%)]\tLoss: 181.095062\n",
            "Train Epoch: 38 [320/1400 (23%)]\tLoss: 132.044220\n",
            "Train Epoch: 38 [360/1400 (26%)]\tLoss: 196.847656\n",
            "Train Epoch: 38 [400/1400 (29%)]\tLoss: 183.983856\n",
            "Train Epoch: 38 [440/1400 (31%)]\tLoss: 141.323212\n",
            "Train Epoch: 38 [480/1400 (34%)]\tLoss: 154.243713\n",
            "Train Epoch: 38 [520/1400 (37%)]\tLoss: 126.057533\n",
            "Train Epoch: 38 [560/1400 (40%)]\tLoss: 138.041107\n",
            "Train Epoch: 38 [600/1400 (43%)]\tLoss: 146.615341\n",
            "Train Epoch: 38 [640/1400 (46%)]\tLoss: 136.949799\n",
            "Train Epoch: 38 [680/1400 (49%)]\tLoss: 137.919693\n",
            "Train Epoch: 38 [720/1400 (51%)]\tLoss: 143.792130\n",
            "Train Epoch: 38 [760/1400 (54%)]\tLoss: 143.667282\n",
            "Train Epoch: 38 [800/1400 (57%)]\tLoss: 150.800934\n",
            "Train Epoch: 38 [840/1400 (60%)]\tLoss: 150.222794\n",
            "Train Epoch: 38 [880/1400 (63%)]\tLoss: 163.683121\n",
            "Train Epoch: 38 [920/1400 (66%)]\tLoss: 148.457245\n",
            "Train Epoch: 38 [960/1400 (69%)]\tLoss: 139.379349\n",
            "Train Epoch: 38 [1000/1400 (71%)]\tLoss: 176.557175\n",
            "Train Epoch: 38 [1040/1400 (74%)]\tLoss: 135.341522\n",
            "Train Epoch: 38 [1080/1400 (77%)]\tLoss: 149.737930\n",
            "Train Epoch: 38 [1120/1400 (80%)]\tLoss: 184.550018\n",
            "Train Epoch: 38 [1160/1400 (83%)]\tLoss: 164.189529\n",
            "Train Epoch: 38 [1200/1400 (86%)]\tLoss: 158.279617\n",
            "Train Epoch: 38 [1240/1400 (89%)]\tLoss: 127.913620\n",
            "Train Epoch: 38 [1280/1400 (91%)]\tLoss: 120.464836\n",
            "Train Epoch: 38 [1320/1400 (94%)]\tLoss: 177.708389\n",
            "Train Epoch: 38 [1360/1400 (97%)]\tLoss: 153.636566\n",
            "====> Epoch: 38 Average loss: 153.1890\n",
            "====> Test set loss: 155.4174\n",
            "Train Epoch: 39 [0/1400 (0%)]\tLoss: 123.517258\n",
            "Train Epoch: 39 [40/1400 (3%)]\tLoss: 141.052063\n",
            "Train Epoch: 39 [80/1400 (6%)]\tLoss: 171.089096\n",
            "Train Epoch: 39 [120/1400 (9%)]\tLoss: 137.594925\n",
            "Train Epoch: 39 [160/1400 (11%)]\tLoss: 164.485062\n",
            "Train Epoch: 39 [200/1400 (14%)]\tLoss: 148.991684\n",
            "Train Epoch: 39 [240/1400 (17%)]\tLoss: 135.000748\n",
            "Train Epoch: 39 [280/1400 (20%)]\tLoss: 149.379623\n",
            "Train Epoch: 39 [320/1400 (23%)]\tLoss: 154.282333\n",
            "Train Epoch: 39 [360/1400 (26%)]\tLoss: 145.913773\n",
            "Train Epoch: 39 [400/1400 (29%)]\tLoss: 131.175613\n",
            "Train Epoch: 39 [440/1400 (31%)]\tLoss: 172.751175\n",
            "Train Epoch: 39 [480/1400 (34%)]\tLoss: 139.768555\n",
            "Train Epoch: 39 [520/1400 (37%)]\tLoss: 127.620216\n",
            "Train Epoch: 39 [560/1400 (40%)]\tLoss: 120.938202\n",
            "Train Epoch: 39 [600/1400 (43%)]\tLoss: 145.621506\n",
            "Train Epoch: 39 [640/1400 (46%)]\tLoss: 173.259979\n",
            "Train Epoch: 39 [680/1400 (49%)]\tLoss: 140.659927\n",
            "Train Epoch: 39 [720/1400 (51%)]\tLoss: 155.507126\n",
            "Train Epoch: 39 [760/1400 (54%)]\tLoss: 158.610992\n",
            "Train Epoch: 39 [800/1400 (57%)]\tLoss: 121.859756\n",
            "Train Epoch: 39 [840/1400 (60%)]\tLoss: 124.166580\n",
            "Train Epoch: 39 [880/1400 (63%)]\tLoss: 113.553032\n",
            "Train Epoch: 39 [920/1400 (66%)]\tLoss: 121.088463\n",
            "Train Epoch: 39 [960/1400 (69%)]\tLoss: 160.521942\n",
            "Train Epoch: 39 [1000/1400 (71%)]\tLoss: 169.552902\n",
            "Train Epoch: 39 [1040/1400 (74%)]\tLoss: 143.902466\n",
            "Train Epoch: 39 [1080/1400 (77%)]\tLoss: 150.463196\n",
            "Train Epoch: 39 [1120/1400 (80%)]\tLoss: 126.866455\n",
            "Train Epoch: 39 [1160/1400 (83%)]\tLoss: 118.663811\n",
            "Train Epoch: 39 [1200/1400 (86%)]\tLoss: 138.727097\n",
            "Train Epoch: 39 [1240/1400 (89%)]\tLoss: 161.575729\n",
            "Train Epoch: 39 [1280/1400 (91%)]\tLoss: 159.324341\n",
            "Train Epoch: 39 [1320/1400 (94%)]\tLoss: 122.075180\n",
            "Train Epoch: 39 [1360/1400 (97%)]\tLoss: 163.786407\n",
            "====> Epoch: 39 Average loss: 152.2058\n",
            "====> Test set loss: 154.6944\n",
            "Train Epoch: 40 [0/1400 (0%)]\tLoss: 148.420609\n",
            "Train Epoch: 40 [40/1400 (3%)]\tLoss: 182.478882\n",
            "Train Epoch: 40 [80/1400 (6%)]\tLoss: 130.158066\n",
            "Train Epoch: 40 [120/1400 (9%)]\tLoss: 174.684113\n",
            "Train Epoch: 40 [160/1400 (11%)]\tLoss: 172.375885\n",
            "Train Epoch: 40 [200/1400 (14%)]\tLoss: 150.380463\n",
            "Train Epoch: 40 [240/1400 (17%)]\tLoss: 146.255600\n",
            "Train Epoch: 40 [280/1400 (20%)]\tLoss: 132.703827\n",
            "Train Epoch: 40 [320/1400 (23%)]\tLoss: 150.882767\n",
            "Train Epoch: 40 [360/1400 (26%)]\tLoss: 144.454910\n",
            "Train Epoch: 40 [400/1400 (29%)]\tLoss: 161.224060\n",
            "Train Epoch: 40 [440/1400 (31%)]\tLoss: 146.489410\n",
            "Train Epoch: 40 [480/1400 (34%)]\tLoss: 144.667694\n",
            "Train Epoch: 40 [520/1400 (37%)]\tLoss: 151.363281\n",
            "Train Epoch: 40 [560/1400 (40%)]\tLoss: 174.144257\n",
            "Train Epoch: 40 [600/1400 (43%)]\tLoss: 144.507202\n",
            "Train Epoch: 40 [640/1400 (46%)]\tLoss: 140.214066\n",
            "Train Epoch: 40 [680/1400 (49%)]\tLoss: 152.463348\n",
            "Train Epoch: 40 [720/1400 (51%)]\tLoss: 158.264008\n",
            "Train Epoch: 40 [760/1400 (54%)]\tLoss: 153.762039\n",
            "Train Epoch: 40 [800/1400 (57%)]\tLoss: 160.780350\n",
            "Train Epoch: 40 [840/1400 (60%)]\tLoss: 152.599915\n",
            "Train Epoch: 40 [880/1400 (63%)]\tLoss: 129.990829\n",
            "Train Epoch: 40 [920/1400 (66%)]\tLoss: 146.437256\n",
            "Train Epoch: 40 [960/1400 (69%)]\tLoss: 137.695297\n",
            "Train Epoch: 40 [1000/1400 (71%)]\tLoss: 174.886185\n",
            "Train Epoch: 40 [1040/1400 (74%)]\tLoss: 172.556778\n",
            "Train Epoch: 40 [1080/1400 (77%)]\tLoss: 155.381638\n",
            "Train Epoch: 40 [1120/1400 (80%)]\tLoss: 152.928329\n",
            "Train Epoch: 40 [1160/1400 (83%)]\tLoss: 141.835266\n",
            "Train Epoch: 40 [1200/1400 (86%)]\tLoss: 120.672462\n",
            "Train Epoch: 40 [1240/1400 (89%)]\tLoss: 140.884872\n",
            "Train Epoch: 40 [1280/1400 (91%)]\tLoss: 172.869751\n",
            "Train Epoch: 40 [1320/1400 (94%)]\tLoss: 140.480530\n",
            "Train Epoch: 40 [1360/1400 (97%)]\tLoss: 175.161224\n",
            "====> Epoch: 40 Average loss: 151.2606\n",
            "====> Test set loss: 153.9808\n",
            "Train Epoch: 41 [0/1400 (0%)]\tLoss: 150.742065\n",
            "Train Epoch: 41 [40/1400 (3%)]\tLoss: 136.214630\n",
            "Train Epoch: 41 [80/1400 (6%)]\tLoss: 143.169937\n",
            "Train Epoch: 41 [120/1400 (9%)]\tLoss: 134.535965\n",
            "Train Epoch: 41 [160/1400 (11%)]\tLoss: 151.327499\n",
            "Train Epoch: 41 [200/1400 (14%)]\tLoss: 150.602509\n",
            "Train Epoch: 41 [240/1400 (17%)]\tLoss: 172.546173\n",
            "Train Epoch: 41 [280/1400 (20%)]\tLoss: 195.285782\n",
            "Train Epoch: 41 [320/1400 (23%)]\tLoss: 139.602997\n",
            "Train Epoch: 41 [360/1400 (26%)]\tLoss: 174.087143\n",
            "Train Epoch: 41 [400/1400 (29%)]\tLoss: 167.783798\n",
            "Train Epoch: 41 [440/1400 (31%)]\tLoss: 128.502945\n",
            "Train Epoch: 41 [480/1400 (34%)]\tLoss: 131.378159\n",
            "Train Epoch: 41 [520/1400 (37%)]\tLoss: 156.871948\n",
            "Train Epoch: 41 [560/1400 (40%)]\tLoss: 127.937553\n",
            "Train Epoch: 41 [600/1400 (43%)]\tLoss: 130.389465\n",
            "Train Epoch: 41 [640/1400 (46%)]\tLoss: 144.096054\n",
            "Train Epoch: 41 [680/1400 (49%)]\tLoss: 136.530106\n",
            "Train Epoch: 41 [720/1400 (51%)]\tLoss: 147.887177\n",
            "Train Epoch: 41 [760/1400 (54%)]\tLoss: 145.335938\n",
            "Train Epoch: 41 [800/1400 (57%)]\tLoss: 185.484467\n",
            "Train Epoch: 41 [840/1400 (60%)]\tLoss: 135.088150\n",
            "Train Epoch: 41 [880/1400 (63%)]\tLoss: 151.329254\n",
            "Train Epoch: 41 [920/1400 (66%)]\tLoss: 163.500092\n",
            "Train Epoch: 41 [960/1400 (69%)]\tLoss: 157.951981\n",
            "Train Epoch: 41 [1000/1400 (71%)]\tLoss: 190.343140\n",
            "Train Epoch: 41 [1040/1400 (74%)]\tLoss: 185.109772\n",
            "Train Epoch: 41 [1080/1400 (77%)]\tLoss: 170.083389\n",
            "Train Epoch: 41 [1120/1400 (80%)]\tLoss: 165.445038\n",
            "Train Epoch: 41 [1160/1400 (83%)]\tLoss: 171.948761\n",
            "Train Epoch: 41 [1200/1400 (86%)]\tLoss: 168.960907\n",
            "Train Epoch: 41 [1240/1400 (89%)]\tLoss: 137.062775\n",
            "Train Epoch: 41 [1280/1400 (91%)]\tLoss: 160.549683\n",
            "Train Epoch: 41 [1320/1400 (94%)]\tLoss: 149.894379\n",
            "Train Epoch: 41 [1360/1400 (97%)]\tLoss: 132.172363\n",
            "====> Epoch: 41 Average loss: 150.4677\n",
            "====> Test set loss: 153.2803\n",
            "Train Epoch: 42 [0/1400 (0%)]\tLoss: 144.445831\n",
            "Train Epoch: 42 [40/1400 (3%)]\tLoss: 149.611649\n",
            "Train Epoch: 42 [80/1400 (6%)]\tLoss: 140.562927\n",
            "Train Epoch: 42 [120/1400 (9%)]\tLoss: 174.214722\n",
            "Train Epoch: 42 [160/1400 (11%)]\tLoss: 144.314728\n",
            "Train Epoch: 42 [200/1400 (14%)]\tLoss: 135.592316\n",
            "Train Epoch: 42 [240/1400 (17%)]\tLoss: 153.891876\n",
            "Train Epoch: 42 [280/1400 (20%)]\tLoss: 135.845886\n",
            "Train Epoch: 42 [320/1400 (23%)]\tLoss: 133.518677\n",
            "Train Epoch: 42 [360/1400 (26%)]\tLoss: 130.612747\n",
            "Train Epoch: 42 [400/1400 (29%)]\tLoss: 128.706253\n",
            "Train Epoch: 42 [440/1400 (31%)]\tLoss: 132.360687\n",
            "Train Epoch: 42 [480/1400 (34%)]\tLoss: 147.517044\n",
            "Train Epoch: 42 [520/1400 (37%)]\tLoss: 125.368423\n",
            "Train Epoch: 42 [560/1400 (40%)]\tLoss: 148.983154\n",
            "Train Epoch: 42 [600/1400 (43%)]\tLoss: 140.734253\n",
            "Train Epoch: 42 [640/1400 (46%)]\tLoss: 154.030350\n",
            "Train Epoch: 42 [680/1400 (49%)]\tLoss: 152.074051\n",
            "Train Epoch: 42 [720/1400 (51%)]\tLoss: 139.386749\n",
            "Train Epoch: 42 [760/1400 (54%)]\tLoss: 156.599655\n",
            "Train Epoch: 42 [800/1400 (57%)]\tLoss: 139.297974\n",
            "Train Epoch: 42 [840/1400 (60%)]\tLoss: 134.663818\n",
            "Train Epoch: 42 [880/1400 (63%)]\tLoss: 126.120590\n",
            "Train Epoch: 42 [920/1400 (66%)]\tLoss: 137.185226\n",
            "Train Epoch: 42 [960/1400 (69%)]\tLoss: 153.837326\n",
            "Train Epoch: 42 [1000/1400 (71%)]\tLoss: 164.407944\n",
            "Train Epoch: 42 [1040/1400 (74%)]\tLoss: 121.859055\n",
            "Train Epoch: 42 [1080/1400 (77%)]\tLoss: 167.879654\n",
            "Train Epoch: 42 [1120/1400 (80%)]\tLoss: 166.109222\n",
            "Train Epoch: 42 [1160/1400 (83%)]\tLoss: 143.309250\n",
            "Train Epoch: 42 [1200/1400 (86%)]\tLoss: 143.274994\n",
            "Train Epoch: 42 [1240/1400 (89%)]\tLoss: 129.419556\n",
            "Train Epoch: 42 [1280/1400 (91%)]\tLoss: 120.910599\n",
            "Train Epoch: 42 [1320/1400 (94%)]\tLoss: 163.889923\n",
            "Train Epoch: 42 [1360/1400 (97%)]\tLoss: 153.768753\n",
            "====> Epoch: 42 Average loss: 149.7139\n",
            "====> Test set loss: 152.7324\n",
            "Train Epoch: 43 [0/1400 (0%)]\tLoss: 155.651199\n",
            "Train Epoch: 43 [40/1400 (3%)]\tLoss: 133.912613\n",
            "Train Epoch: 43 [80/1400 (6%)]\tLoss: 142.392868\n",
            "Train Epoch: 43 [120/1400 (9%)]\tLoss: 125.159500\n",
            "Train Epoch: 43 [160/1400 (11%)]\tLoss: 145.727646\n",
            "Train Epoch: 43 [200/1400 (14%)]\tLoss: 128.391830\n",
            "Train Epoch: 43 [240/1400 (17%)]\tLoss: 164.381348\n",
            "Train Epoch: 43 [280/1400 (20%)]\tLoss: 146.911942\n",
            "Train Epoch: 43 [320/1400 (23%)]\tLoss: 120.010063\n",
            "Train Epoch: 43 [360/1400 (26%)]\tLoss: 145.533859\n",
            "Train Epoch: 43 [400/1400 (29%)]\tLoss: 129.638199\n",
            "Train Epoch: 43 [440/1400 (31%)]\tLoss: 124.815117\n",
            "Train Epoch: 43 [480/1400 (34%)]\tLoss: 167.327332\n",
            "Train Epoch: 43 [520/1400 (37%)]\tLoss: 136.971100\n",
            "Train Epoch: 43 [560/1400 (40%)]\tLoss: 152.005829\n",
            "Train Epoch: 43 [600/1400 (43%)]\tLoss: 151.268127\n",
            "Train Epoch: 43 [640/1400 (46%)]\tLoss: 149.476837\n",
            "Train Epoch: 43 [680/1400 (49%)]\tLoss: 150.486450\n",
            "Train Epoch: 43 [720/1400 (51%)]\tLoss: 140.557037\n",
            "Train Epoch: 43 [760/1400 (54%)]\tLoss: 132.689133\n",
            "Train Epoch: 43 [800/1400 (57%)]\tLoss: 112.129257\n",
            "Train Epoch: 43 [840/1400 (60%)]\tLoss: 175.179779\n",
            "Train Epoch: 43 [880/1400 (63%)]\tLoss: 132.446838\n",
            "Train Epoch: 43 [920/1400 (66%)]\tLoss: 163.960754\n",
            "Train Epoch: 43 [960/1400 (69%)]\tLoss: 120.247536\n",
            "Train Epoch: 43 [1000/1400 (71%)]\tLoss: 154.871689\n",
            "Train Epoch: 43 [1040/1400 (74%)]\tLoss: 143.438446\n",
            "Train Epoch: 43 [1080/1400 (77%)]\tLoss: 141.164734\n",
            "Train Epoch: 43 [1120/1400 (80%)]\tLoss: 195.655716\n",
            "Train Epoch: 43 [1160/1400 (83%)]\tLoss: 140.360245\n",
            "Train Epoch: 43 [1200/1400 (86%)]\tLoss: 124.718872\n",
            "Train Epoch: 43 [1240/1400 (89%)]\tLoss: 145.768875\n",
            "Train Epoch: 43 [1280/1400 (91%)]\tLoss: 168.207275\n",
            "Train Epoch: 43 [1320/1400 (94%)]\tLoss: 142.374588\n",
            "Train Epoch: 43 [1360/1400 (97%)]\tLoss: 156.461136\n",
            "====> Epoch: 43 Average loss: 149.0461\n",
            "====> Test set loss: 152.0948\n",
            "Train Epoch: 44 [0/1400 (0%)]\tLoss: 151.884674\n",
            "Train Epoch: 44 [40/1400 (3%)]\tLoss: 148.282791\n",
            "Train Epoch: 44 [80/1400 (6%)]\tLoss: 124.826744\n",
            "Train Epoch: 44 [120/1400 (9%)]\tLoss: 204.912369\n",
            "Train Epoch: 44 [160/1400 (11%)]\tLoss: 172.326187\n",
            "Train Epoch: 44 [200/1400 (14%)]\tLoss: 160.719452\n",
            "Train Epoch: 44 [240/1400 (17%)]\tLoss: 145.830246\n",
            "Train Epoch: 44 [280/1400 (20%)]\tLoss: 170.219727\n",
            "Train Epoch: 44 [320/1400 (23%)]\tLoss: 126.439835\n",
            "Train Epoch: 44 [360/1400 (26%)]\tLoss: 150.388992\n",
            "Train Epoch: 44 [400/1400 (29%)]\tLoss: 172.689240\n",
            "Train Epoch: 44 [440/1400 (31%)]\tLoss: 157.538391\n",
            "Train Epoch: 44 [480/1400 (34%)]\tLoss: 171.373520\n",
            "Train Epoch: 44 [520/1400 (37%)]\tLoss: 183.229218\n",
            "Train Epoch: 44 [560/1400 (40%)]\tLoss: 126.453293\n",
            "Train Epoch: 44 [600/1400 (43%)]\tLoss: 165.795349\n",
            "Train Epoch: 44 [640/1400 (46%)]\tLoss: 120.704498\n",
            "Train Epoch: 44 [680/1400 (49%)]\tLoss: 163.475098\n",
            "Train Epoch: 44 [720/1400 (51%)]\tLoss: 137.136734\n",
            "Train Epoch: 44 [760/1400 (54%)]\tLoss: 129.322937\n",
            "Train Epoch: 44 [800/1400 (57%)]\tLoss: 129.212585\n",
            "Train Epoch: 44 [840/1400 (60%)]\tLoss: 171.287354\n",
            "Train Epoch: 44 [880/1400 (63%)]\tLoss: 144.999802\n",
            "Train Epoch: 44 [920/1400 (66%)]\tLoss: 164.308121\n",
            "Train Epoch: 44 [960/1400 (69%)]\tLoss: 148.586838\n",
            "Train Epoch: 44 [1000/1400 (71%)]\tLoss: 157.380157\n",
            "Train Epoch: 44 [1040/1400 (74%)]\tLoss: 139.475418\n",
            "Train Epoch: 44 [1080/1400 (77%)]\tLoss: 173.897644\n",
            "Train Epoch: 44 [1120/1400 (80%)]\tLoss: 135.325867\n",
            "Train Epoch: 44 [1160/1400 (83%)]\tLoss: 154.061172\n",
            "Train Epoch: 44 [1200/1400 (86%)]\tLoss: 151.931686\n",
            "Train Epoch: 44 [1240/1400 (89%)]\tLoss: 142.327240\n",
            "Train Epoch: 44 [1280/1400 (91%)]\tLoss: 155.336304\n",
            "Train Epoch: 44 [1320/1400 (94%)]\tLoss: 131.557800\n",
            "Train Epoch: 44 [1360/1400 (97%)]\tLoss: 167.295853\n",
            "====> Epoch: 44 Average loss: 148.4324\n",
            "====> Test set loss: 151.7055\n",
            "Train Epoch: 45 [0/1400 (0%)]\tLoss: 129.365280\n",
            "Train Epoch: 45 [40/1400 (3%)]\tLoss: 142.983398\n",
            "Train Epoch: 45 [80/1400 (6%)]\tLoss: 148.149246\n",
            "Train Epoch: 45 [120/1400 (9%)]\tLoss: 154.982300\n",
            "Train Epoch: 45 [160/1400 (11%)]\tLoss: 160.404358\n",
            "Train Epoch: 45 [200/1400 (14%)]\tLoss: 141.924377\n",
            "Train Epoch: 45 [240/1400 (17%)]\tLoss: 140.708191\n",
            "Train Epoch: 45 [280/1400 (20%)]\tLoss: 109.903885\n",
            "Train Epoch: 45 [320/1400 (23%)]\tLoss: 139.391541\n",
            "Train Epoch: 45 [360/1400 (26%)]\tLoss: 139.937958\n",
            "Train Epoch: 45 [400/1400 (29%)]\tLoss: 166.945282\n",
            "Train Epoch: 45 [440/1400 (31%)]\tLoss: 161.478561\n",
            "Train Epoch: 45 [480/1400 (34%)]\tLoss: 162.882919\n",
            "Train Epoch: 45 [520/1400 (37%)]\tLoss: 171.174438\n",
            "Train Epoch: 45 [560/1400 (40%)]\tLoss: 122.157333\n",
            "Train Epoch: 45 [600/1400 (43%)]\tLoss: 168.026840\n",
            "Train Epoch: 45 [640/1400 (46%)]\tLoss: 158.393387\n",
            "Train Epoch: 45 [680/1400 (49%)]\tLoss: 154.230789\n",
            "Train Epoch: 45 [720/1400 (51%)]\tLoss: 146.315308\n",
            "Train Epoch: 45 [760/1400 (54%)]\tLoss: 158.481415\n",
            "Train Epoch: 45 [800/1400 (57%)]\tLoss: 149.629456\n",
            "Train Epoch: 45 [840/1400 (60%)]\tLoss: 139.269440\n",
            "Train Epoch: 45 [880/1400 (63%)]\tLoss: 154.831238\n",
            "Train Epoch: 45 [920/1400 (66%)]\tLoss: 160.635086\n",
            "Train Epoch: 45 [960/1400 (69%)]\tLoss: 147.873322\n",
            "Train Epoch: 45 [1000/1400 (71%)]\tLoss: 120.368599\n",
            "Train Epoch: 45 [1040/1400 (74%)]\tLoss: 123.837006\n",
            "Train Epoch: 45 [1080/1400 (77%)]\tLoss: 146.435150\n",
            "Train Epoch: 45 [1120/1400 (80%)]\tLoss: 157.300598\n",
            "Train Epoch: 45 [1160/1400 (83%)]\tLoss: 155.849243\n",
            "Train Epoch: 45 [1200/1400 (86%)]\tLoss: 129.954803\n",
            "Train Epoch: 45 [1240/1400 (89%)]\tLoss: 135.866730\n",
            "Train Epoch: 45 [1280/1400 (91%)]\tLoss: 149.505493\n",
            "Train Epoch: 45 [1320/1400 (94%)]\tLoss: 152.538132\n",
            "Train Epoch: 45 [1360/1400 (97%)]\tLoss: 132.817596\n",
            "====> Epoch: 45 Average loss: 147.8487\n",
            "====> Test set loss: 151.1650\n",
            "Train Epoch: 46 [0/1400 (0%)]\tLoss: 129.812149\n",
            "Train Epoch: 46 [40/1400 (3%)]\tLoss: 138.914825\n",
            "Train Epoch: 46 [80/1400 (6%)]\tLoss: 133.615692\n",
            "Train Epoch: 46 [120/1400 (9%)]\tLoss: 139.884735\n",
            "Train Epoch: 46 [160/1400 (11%)]\tLoss: 165.142029\n",
            "Train Epoch: 46 [200/1400 (14%)]\tLoss: 167.067352\n",
            "Train Epoch: 46 [240/1400 (17%)]\tLoss: 152.099380\n",
            "Train Epoch: 46 [280/1400 (20%)]\tLoss: 129.336823\n",
            "Train Epoch: 46 [320/1400 (23%)]\tLoss: 144.090408\n",
            "Train Epoch: 46 [360/1400 (26%)]\tLoss: 140.629425\n",
            "Train Epoch: 46 [400/1400 (29%)]\tLoss: 150.997421\n",
            "Train Epoch: 46 [440/1400 (31%)]\tLoss: 135.252487\n",
            "Train Epoch: 46 [480/1400 (34%)]\tLoss: 120.250656\n",
            "Train Epoch: 46 [520/1400 (37%)]\tLoss: 165.241455\n",
            "Train Epoch: 46 [560/1400 (40%)]\tLoss: 147.400574\n",
            "Train Epoch: 46 [600/1400 (43%)]\tLoss: 148.071747\n",
            "Train Epoch: 46 [640/1400 (46%)]\tLoss: 146.430740\n",
            "Train Epoch: 46 [680/1400 (49%)]\tLoss: 145.810471\n",
            "Train Epoch: 46 [720/1400 (51%)]\tLoss: 168.699203\n",
            "Train Epoch: 46 [760/1400 (54%)]\tLoss: 138.565598\n",
            "Train Epoch: 46 [800/1400 (57%)]\tLoss: 147.174957\n",
            "Train Epoch: 46 [840/1400 (60%)]\tLoss: 127.380402\n",
            "Train Epoch: 46 [880/1400 (63%)]\tLoss: 145.587616\n",
            "Train Epoch: 46 [920/1400 (66%)]\tLoss: 139.363342\n",
            "Train Epoch: 46 [960/1400 (69%)]\tLoss: 145.791565\n",
            "Train Epoch: 46 [1000/1400 (71%)]\tLoss: 132.935059\n",
            "Train Epoch: 46 [1040/1400 (74%)]\tLoss: 143.007385\n",
            "Train Epoch: 46 [1080/1400 (77%)]\tLoss: 135.997910\n",
            "Train Epoch: 46 [1120/1400 (80%)]\tLoss: 133.866898\n",
            "Train Epoch: 46 [1160/1400 (83%)]\tLoss: 148.751129\n",
            "Train Epoch: 46 [1200/1400 (86%)]\tLoss: 162.550827\n",
            "Train Epoch: 46 [1240/1400 (89%)]\tLoss: 134.877640\n",
            "Train Epoch: 46 [1280/1400 (91%)]\tLoss: 132.611984\n",
            "Train Epoch: 46 [1320/1400 (94%)]\tLoss: 138.983002\n",
            "Train Epoch: 46 [1360/1400 (97%)]\tLoss: 132.563080\n",
            "====> Epoch: 46 Average loss: 147.3799\n",
            "====> Test set loss: 150.6338\n",
            "Train Epoch: 47 [0/1400 (0%)]\tLoss: 149.606842\n",
            "Train Epoch: 47 [40/1400 (3%)]\tLoss: 147.459000\n",
            "Train Epoch: 47 [80/1400 (6%)]\tLoss: 162.316116\n",
            "Train Epoch: 47 [120/1400 (9%)]\tLoss: 132.131134\n",
            "Train Epoch: 47 [160/1400 (11%)]\tLoss: 130.759918\n",
            "Train Epoch: 47 [200/1400 (14%)]\tLoss: 121.265236\n",
            "Train Epoch: 47 [240/1400 (17%)]\tLoss: 167.049301\n",
            "Train Epoch: 47 [280/1400 (20%)]\tLoss: 141.776077\n",
            "Train Epoch: 47 [320/1400 (23%)]\tLoss: 173.153625\n",
            "Train Epoch: 47 [360/1400 (26%)]\tLoss: 146.719727\n",
            "Train Epoch: 47 [400/1400 (29%)]\tLoss: 153.088409\n",
            "Train Epoch: 47 [440/1400 (31%)]\tLoss: 129.210388\n",
            "Train Epoch: 47 [480/1400 (34%)]\tLoss: 141.906448\n",
            "Train Epoch: 47 [520/1400 (37%)]\tLoss: 162.140121\n",
            "Train Epoch: 47 [560/1400 (40%)]\tLoss: 146.731018\n",
            "Train Epoch: 47 [600/1400 (43%)]\tLoss: 135.844727\n",
            "Train Epoch: 47 [640/1400 (46%)]\tLoss: 136.391083\n",
            "Train Epoch: 47 [680/1400 (49%)]\tLoss: 143.446777\n",
            "Train Epoch: 47 [720/1400 (51%)]\tLoss: 147.708496\n",
            "Train Epoch: 47 [760/1400 (54%)]\tLoss: 127.498245\n",
            "Train Epoch: 47 [800/1400 (57%)]\tLoss: 135.925598\n",
            "Train Epoch: 47 [840/1400 (60%)]\tLoss: 132.674057\n",
            "Train Epoch: 47 [880/1400 (63%)]\tLoss: 165.836838\n",
            "Train Epoch: 47 [920/1400 (66%)]\tLoss: 132.058304\n",
            "Train Epoch: 47 [960/1400 (69%)]\tLoss: 121.604111\n",
            "Train Epoch: 47 [1000/1400 (71%)]\tLoss: 144.519211\n",
            "Train Epoch: 47 [1040/1400 (74%)]\tLoss: 135.971329\n",
            "Train Epoch: 47 [1080/1400 (77%)]\tLoss: 200.561005\n",
            "Train Epoch: 47 [1120/1400 (80%)]\tLoss: 139.450409\n",
            "Train Epoch: 47 [1160/1400 (83%)]\tLoss: 168.927383\n",
            "Train Epoch: 47 [1200/1400 (86%)]\tLoss: 134.858521\n",
            "Train Epoch: 47 [1240/1400 (89%)]\tLoss: 136.895386\n",
            "Train Epoch: 47 [1280/1400 (91%)]\tLoss: 151.111084\n",
            "Train Epoch: 47 [1320/1400 (94%)]\tLoss: 158.684753\n",
            "Train Epoch: 47 [1360/1400 (97%)]\tLoss: 145.278473\n",
            "====> Epoch: 47 Average loss: 146.9016\n",
            "====> Test set loss: 150.2897\n",
            "Train Epoch: 48 [0/1400 (0%)]\tLoss: 129.769730\n",
            "Train Epoch: 48 [40/1400 (3%)]\tLoss: 165.485992\n",
            "Train Epoch: 48 [80/1400 (6%)]\tLoss: 132.786285\n",
            "Train Epoch: 48 [120/1400 (9%)]\tLoss: 118.927757\n",
            "Train Epoch: 48 [160/1400 (11%)]\tLoss: 148.053177\n",
            "Train Epoch: 48 [200/1400 (14%)]\tLoss: 138.593826\n",
            "Train Epoch: 48 [240/1400 (17%)]\tLoss: 164.654800\n",
            "Train Epoch: 48 [280/1400 (20%)]\tLoss: 143.079559\n",
            "Train Epoch: 48 [320/1400 (23%)]\tLoss: 137.222260\n",
            "Train Epoch: 48 [360/1400 (26%)]\tLoss: 134.298813\n",
            "Train Epoch: 48 [400/1400 (29%)]\tLoss: 173.397476\n",
            "Train Epoch: 48 [440/1400 (31%)]\tLoss: 132.374603\n",
            "Train Epoch: 48 [480/1400 (34%)]\tLoss: 138.918869\n",
            "Train Epoch: 48 [520/1400 (37%)]\tLoss: 167.715057\n",
            "Train Epoch: 48 [560/1400 (40%)]\tLoss: 142.172226\n",
            "Train Epoch: 48 [600/1400 (43%)]\tLoss: 137.437943\n",
            "Train Epoch: 48 [640/1400 (46%)]\tLoss: 120.068489\n",
            "Train Epoch: 48 [680/1400 (49%)]\tLoss: 140.409378\n",
            "Train Epoch: 48 [720/1400 (51%)]\tLoss: 151.187881\n",
            "Train Epoch: 48 [760/1400 (54%)]\tLoss: 153.009827\n",
            "Train Epoch: 48 [800/1400 (57%)]\tLoss: 167.844055\n",
            "Train Epoch: 48 [840/1400 (60%)]\tLoss: 140.567444\n",
            "Train Epoch: 48 [880/1400 (63%)]\tLoss: 165.458694\n",
            "Train Epoch: 48 [920/1400 (66%)]\tLoss: 155.036774\n",
            "Train Epoch: 48 [960/1400 (69%)]\tLoss: 110.512558\n",
            "Train Epoch: 48 [1000/1400 (71%)]\tLoss: 137.095337\n",
            "Train Epoch: 48 [1040/1400 (74%)]\tLoss: 139.241104\n",
            "Train Epoch: 48 [1080/1400 (77%)]\tLoss: 153.867233\n",
            "Train Epoch: 48 [1120/1400 (80%)]\tLoss: 160.497253\n",
            "Train Epoch: 48 [1160/1400 (83%)]\tLoss: 131.628815\n",
            "Train Epoch: 48 [1200/1400 (86%)]\tLoss: 136.872971\n",
            "Train Epoch: 48 [1240/1400 (89%)]\tLoss: 160.663330\n",
            "Train Epoch: 48 [1280/1400 (91%)]\tLoss: 132.697739\n",
            "Train Epoch: 48 [1320/1400 (94%)]\tLoss: 152.779984\n",
            "Train Epoch: 48 [1360/1400 (97%)]\tLoss: 153.597473\n",
            "====> Epoch: 48 Average loss: 146.4869\n",
            "====> Test set loss: 149.8303\n",
            "Train Epoch: 49 [0/1400 (0%)]\tLoss: 144.939392\n",
            "Train Epoch: 49 [40/1400 (3%)]\tLoss: 172.411163\n",
            "Train Epoch: 49 [80/1400 (6%)]\tLoss: 147.347794\n",
            "Train Epoch: 49 [120/1400 (9%)]\tLoss: 141.426376\n",
            "Train Epoch: 49 [160/1400 (11%)]\tLoss: 155.649078\n",
            "Train Epoch: 49 [200/1400 (14%)]\tLoss: 142.002792\n",
            "Train Epoch: 49 [240/1400 (17%)]\tLoss: 153.832367\n",
            "Train Epoch: 49 [280/1400 (20%)]\tLoss: 150.963516\n",
            "Train Epoch: 49 [320/1400 (23%)]\tLoss: 153.785736\n",
            "Train Epoch: 49 [360/1400 (26%)]\tLoss: 172.389587\n",
            "Train Epoch: 49 [400/1400 (29%)]\tLoss: 127.425629\n",
            "Train Epoch: 49 [440/1400 (31%)]\tLoss: 151.749008\n",
            "Train Epoch: 49 [480/1400 (34%)]\tLoss: 134.749390\n",
            "Train Epoch: 49 [520/1400 (37%)]\tLoss: 142.560089\n",
            "Train Epoch: 49 [560/1400 (40%)]\tLoss: 145.347458\n",
            "Train Epoch: 49 [600/1400 (43%)]\tLoss: 135.920761\n",
            "Train Epoch: 49 [640/1400 (46%)]\tLoss: 147.660950\n",
            "Train Epoch: 49 [680/1400 (49%)]\tLoss: 154.938721\n",
            "Train Epoch: 49 [720/1400 (51%)]\tLoss: 148.422440\n",
            "Train Epoch: 49 [760/1400 (54%)]\tLoss: 172.046280\n",
            "Train Epoch: 49 [800/1400 (57%)]\tLoss: 142.295746\n",
            "Train Epoch: 49 [840/1400 (60%)]\tLoss: 144.360184\n",
            "Train Epoch: 49 [880/1400 (63%)]\tLoss: 132.294617\n",
            "Train Epoch: 49 [920/1400 (66%)]\tLoss: 132.397079\n",
            "Train Epoch: 49 [960/1400 (69%)]\tLoss: 124.949783\n",
            "Train Epoch: 49 [1000/1400 (71%)]\tLoss: 190.150085\n",
            "Train Epoch: 49 [1040/1400 (74%)]\tLoss: 140.857834\n",
            "Train Epoch: 49 [1080/1400 (77%)]\tLoss: 173.764816\n",
            "Train Epoch: 49 [1120/1400 (80%)]\tLoss: 162.339951\n",
            "Train Epoch: 49 [1160/1400 (83%)]\tLoss: 158.307327\n",
            "Train Epoch: 49 [1200/1400 (86%)]\tLoss: 153.331497\n",
            "Train Epoch: 49 [1240/1400 (89%)]\tLoss: 146.827393\n",
            "Train Epoch: 49 [1280/1400 (91%)]\tLoss: 123.910179\n",
            "Train Epoch: 49 [1320/1400 (94%)]\tLoss: 161.344543\n",
            "Train Epoch: 49 [1360/1400 (97%)]\tLoss: 132.792648\n",
            "====> Epoch: 49 Average loss: 146.0457\n",
            "====> Test set loss: 149.5329\n",
            "Train Epoch: 50 [0/1400 (0%)]\tLoss: 126.578094\n",
            "Train Epoch: 50 [40/1400 (3%)]\tLoss: 127.316391\n",
            "Train Epoch: 50 [80/1400 (6%)]\tLoss: 149.419128\n",
            "Train Epoch: 50 [120/1400 (9%)]\tLoss: 162.705185\n",
            "Train Epoch: 50 [160/1400 (11%)]\tLoss: 141.095734\n",
            "Train Epoch: 50 [200/1400 (14%)]\tLoss: 141.367203\n",
            "Train Epoch: 50 [240/1400 (17%)]\tLoss: 131.378418\n",
            "Train Epoch: 50 [280/1400 (20%)]\tLoss: 150.991028\n",
            "Train Epoch: 50 [320/1400 (23%)]\tLoss: 148.285080\n",
            "Train Epoch: 50 [360/1400 (26%)]\tLoss: 144.725861\n",
            "Train Epoch: 50 [400/1400 (29%)]\tLoss: 172.968536\n",
            "Train Epoch: 50 [440/1400 (31%)]\tLoss: 133.838257\n",
            "Train Epoch: 50 [480/1400 (34%)]\tLoss: 146.449951\n",
            "Train Epoch: 50 [520/1400 (37%)]\tLoss: 135.363693\n",
            "Train Epoch: 50 [560/1400 (40%)]\tLoss: 147.607056\n",
            "Train Epoch: 50 [600/1400 (43%)]\tLoss: 141.889969\n",
            "Train Epoch: 50 [640/1400 (46%)]\tLoss: 148.167160\n",
            "Train Epoch: 50 [680/1400 (49%)]\tLoss: 136.074051\n",
            "Train Epoch: 50 [720/1400 (51%)]\tLoss: 153.014313\n",
            "Train Epoch: 50 [760/1400 (54%)]\tLoss: 133.878189\n",
            "Train Epoch: 50 [800/1400 (57%)]\tLoss: 139.071075\n",
            "Train Epoch: 50 [840/1400 (60%)]\tLoss: 151.239410\n",
            "Train Epoch: 50 [880/1400 (63%)]\tLoss: 175.138519\n",
            "Train Epoch: 50 [920/1400 (66%)]\tLoss: 152.278824\n",
            "Train Epoch: 50 [960/1400 (69%)]\tLoss: 137.718094\n",
            "Train Epoch: 50 [1000/1400 (71%)]\tLoss: 142.503983\n",
            "Train Epoch: 50 [1040/1400 (74%)]\tLoss: 126.569221\n",
            "Train Epoch: 50 [1080/1400 (77%)]\tLoss: 131.338211\n",
            "Train Epoch: 50 [1120/1400 (80%)]\tLoss: 149.514221\n",
            "Train Epoch: 50 [1160/1400 (83%)]\tLoss: 189.959396\n",
            "Train Epoch: 50 [1200/1400 (86%)]\tLoss: 139.565811\n",
            "Train Epoch: 50 [1240/1400 (89%)]\tLoss: 142.043030\n",
            "Train Epoch: 50 [1280/1400 (91%)]\tLoss: 152.613419\n",
            "Train Epoch: 50 [1320/1400 (94%)]\tLoss: 169.557404\n",
            "Train Epoch: 50 [1360/1400 (97%)]\tLoss: 140.721817\n",
            "====> Epoch: 50 Average loss: 145.7278\n",
            "====> Test set loss: 149.1763\n",
            "Train Epoch: 51 [0/1400 (0%)]\tLoss: 139.915283\n",
            "Train Epoch: 51 [40/1400 (3%)]\tLoss: 139.576889\n",
            "Train Epoch: 51 [80/1400 (6%)]\tLoss: 141.009216\n",
            "Train Epoch: 51 [120/1400 (9%)]\tLoss: 125.550423\n",
            "Train Epoch: 51 [160/1400 (11%)]\tLoss: 128.000122\n",
            "Train Epoch: 51 [200/1400 (14%)]\tLoss: 150.853577\n",
            "Train Epoch: 51 [240/1400 (17%)]\tLoss: 158.952621\n",
            "Train Epoch: 51 [280/1400 (20%)]\tLoss: 168.783096\n",
            "Train Epoch: 51 [320/1400 (23%)]\tLoss: 134.378113\n",
            "Train Epoch: 51 [360/1400 (26%)]\tLoss: 135.944702\n",
            "Train Epoch: 51 [400/1400 (29%)]\tLoss: 146.552963\n",
            "Train Epoch: 51 [440/1400 (31%)]\tLoss: 144.195526\n",
            "Train Epoch: 51 [480/1400 (34%)]\tLoss: 144.060455\n",
            "Train Epoch: 51 [520/1400 (37%)]\tLoss: 153.923691\n",
            "Train Epoch: 51 [560/1400 (40%)]\tLoss: 144.347839\n",
            "Train Epoch: 51 [600/1400 (43%)]\tLoss: 163.408737\n",
            "Train Epoch: 51 [640/1400 (46%)]\tLoss: 134.739716\n",
            "Train Epoch: 51 [680/1400 (49%)]\tLoss: 163.081512\n",
            "Train Epoch: 51 [720/1400 (51%)]\tLoss: 145.094711\n",
            "Train Epoch: 51 [760/1400 (54%)]\tLoss: 139.676392\n",
            "Train Epoch: 51 [800/1400 (57%)]\tLoss: 143.573074\n",
            "Train Epoch: 51 [840/1400 (60%)]\tLoss: 149.153885\n",
            "Train Epoch: 51 [880/1400 (63%)]\tLoss: 145.650803\n",
            "Train Epoch: 51 [920/1400 (66%)]\tLoss: 146.006104\n",
            "Train Epoch: 51 [960/1400 (69%)]\tLoss: 134.145721\n",
            "Train Epoch: 51 [1000/1400 (71%)]\tLoss: 117.741066\n",
            "Train Epoch: 51 [1040/1400 (74%)]\tLoss: 123.976624\n",
            "Train Epoch: 51 [1080/1400 (77%)]\tLoss: 137.273102\n",
            "Train Epoch: 51 [1120/1400 (80%)]\tLoss: 141.873795\n",
            "Train Epoch: 51 [1160/1400 (83%)]\tLoss: 145.074921\n",
            "Train Epoch: 51 [1200/1400 (86%)]\tLoss: 139.688568\n",
            "Train Epoch: 51 [1240/1400 (89%)]\tLoss: 187.035065\n",
            "Train Epoch: 51 [1280/1400 (91%)]\tLoss: 163.308487\n",
            "Train Epoch: 51 [1320/1400 (94%)]\tLoss: 146.982483\n",
            "Train Epoch: 51 [1360/1400 (97%)]\tLoss: 130.353439\n",
            "====> Epoch: 51 Average loss: 145.3528\n",
            "====> Test set loss: 148.8943\n",
            "Train Epoch: 52 [0/1400 (0%)]\tLoss: 147.230392\n",
            "Train Epoch: 52 [40/1400 (3%)]\tLoss: 150.814331\n",
            "Train Epoch: 52 [80/1400 (6%)]\tLoss: 168.671692\n",
            "Train Epoch: 52 [120/1400 (9%)]\tLoss: 138.999832\n",
            "Train Epoch: 52 [160/1400 (11%)]\tLoss: 128.222382\n",
            "Train Epoch: 52 [200/1400 (14%)]\tLoss: 147.637787\n",
            "Train Epoch: 52 [240/1400 (17%)]\tLoss: 143.762131\n",
            "Train Epoch: 52 [280/1400 (20%)]\tLoss: 164.963516\n",
            "Train Epoch: 52 [320/1400 (23%)]\tLoss: 137.953735\n",
            "Train Epoch: 52 [360/1400 (26%)]\tLoss: 136.909103\n",
            "Train Epoch: 52 [400/1400 (29%)]\tLoss: 126.961456\n",
            "Train Epoch: 52 [440/1400 (31%)]\tLoss: 131.268341\n",
            "Train Epoch: 52 [480/1400 (34%)]\tLoss: 134.010040\n",
            "Train Epoch: 52 [520/1400 (37%)]\tLoss: 147.725937\n",
            "Train Epoch: 52 [560/1400 (40%)]\tLoss: 150.966354\n",
            "Train Epoch: 52 [600/1400 (43%)]\tLoss: 128.572021\n",
            "Train Epoch: 52 [640/1400 (46%)]\tLoss: 139.302963\n",
            "Train Epoch: 52 [680/1400 (49%)]\tLoss: 120.043900\n",
            "Train Epoch: 52 [720/1400 (51%)]\tLoss: 153.463165\n",
            "Train Epoch: 52 [760/1400 (54%)]\tLoss: 138.307877\n",
            "Train Epoch: 52 [800/1400 (57%)]\tLoss: 145.432236\n",
            "Train Epoch: 52 [840/1400 (60%)]\tLoss: 159.619781\n",
            "Train Epoch: 52 [880/1400 (63%)]\tLoss: 141.293671\n",
            "Train Epoch: 52 [920/1400 (66%)]\tLoss: 157.239288\n",
            "Train Epoch: 52 [960/1400 (69%)]\tLoss: 149.675568\n",
            "Train Epoch: 52 [1000/1400 (71%)]\tLoss: 141.093857\n",
            "Train Epoch: 52 [1040/1400 (74%)]\tLoss: 155.586380\n",
            "Train Epoch: 52 [1080/1400 (77%)]\tLoss: 140.077759\n",
            "Train Epoch: 52 [1120/1400 (80%)]\tLoss: 141.280807\n",
            "Train Epoch: 52 [1160/1400 (83%)]\tLoss: 132.190674\n",
            "Train Epoch: 52 [1200/1400 (86%)]\tLoss: 135.503845\n",
            "Train Epoch: 52 [1240/1400 (89%)]\tLoss: 118.375214\n",
            "Train Epoch: 52 [1280/1400 (91%)]\tLoss: 118.071587\n",
            "Train Epoch: 52 [1320/1400 (94%)]\tLoss: 131.303497\n",
            "Train Epoch: 52 [1360/1400 (97%)]\tLoss: 194.756073\n",
            "====> Epoch: 52 Average loss: 145.0384\n",
            "====> Test set loss: 148.5472\n",
            "Train Epoch: 53 [0/1400 (0%)]\tLoss: 180.259979\n",
            "Train Epoch: 53 [40/1400 (3%)]\tLoss: 120.826599\n",
            "Train Epoch: 53 [80/1400 (6%)]\tLoss: 137.163910\n",
            "Train Epoch: 53 [120/1400 (9%)]\tLoss: 139.356293\n",
            "Train Epoch: 53 [160/1400 (11%)]\tLoss: 138.352936\n",
            "Train Epoch: 53 [200/1400 (14%)]\tLoss: 136.214447\n",
            "Train Epoch: 53 [240/1400 (17%)]\tLoss: 173.711182\n",
            "Train Epoch: 53 [280/1400 (20%)]\tLoss: 117.721924\n",
            "Train Epoch: 53 [320/1400 (23%)]\tLoss: 139.999420\n",
            "Train Epoch: 53 [360/1400 (26%)]\tLoss: 134.329498\n",
            "Train Epoch: 53 [400/1400 (29%)]\tLoss: 167.656296\n",
            "Train Epoch: 53 [440/1400 (31%)]\tLoss: 138.516312\n",
            "Train Epoch: 53 [480/1400 (34%)]\tLoss: 129.117493\n",
            "Train Epoch: 53 [520/1400 (37%)]\tLoss: 155.460129\n",
            "Train Epoch: 53 [560/1400 (40%)]\tLoss: 131.799591\n",
            "Train Epoch: 53 [600/1400 (43%)]\tLoss: 136.146149\n",
            "Train Epoch: 53 [640/1400 (46%)]\tLoss: 116.551414\n",
            "Train Epoch: 53 [680/1400 (49%)]\tLoss: 128.024872\n",
            "Train Epoch: 53 [720/1400 (51%)]\tLoss: 156.522720\n",
            "Train Epoch: 53 [760/1400 (54%)]\tLoss: 145.252609\n",
            "Train Epoch: 53 [800/1400 (57%)]\tLoss: 154.600754\n",
            "Train Epoch: 53 [840/1400 (60%)]\tLoss: 160.089142\n",
            "Train Epoch: 53 [880/1400 (63%)]\tLoss: 134.299606\n",
            "Train Epoch: 53 [920/1400 (66%)]\tLoss: 151.646362\n",
            "Train Epoch: 53 [960/1400 (69%)]\tLoss: 156.531952\n",
            "Train Epoch: 53 [1000/1400 (71%)]\tLoss: 113.462273\n",
            "Train Epoch: 53 [1040/1400 (74%)]\tLoss: 124.658516\n",
            "Train Epoch: 53 [1080/1400 (77%)]\tLoss: 151.504150\n",
            "Train Epoch: 53 [1120/1400 (80%)]\tLoss: 138.801071\n",
            "Train Epoch: 53 [1160/1400 (83%)]\tLoss: 128.749161\n",
            "Train Epoch: 53 [1200/1400 (86%)]\tLoss: 149.448853\n",
            "Train Epoch: 53 [1240/1400 (89%)]\tLoss: 140.653091\n",
            "Train Epoch: 53 [1280/1400 (91%)]\tLoss: 144.053406\n",
            "Train Epoch: 53 [1320/1400 (94%)]\tLoss: 135.049057\n",
            "Train Epoch: 53 [1360/1400 (97%)]\tLoss: 168.950577\n",
            "====> Epoch: 53 Average loss: 144.7471\n",
            "====> Test set loss: 148.2577\n",
            "Train Epoch: 54 [0/1400 (0%)]\tLoss: 152.135803\n",
            "Train Epoch: 54 [40/1400 (3%)]\tLoss: 134.181961\n",
            "Train Epoch: 54 [80/1400 (6%)]\tLoss: 140.137970\n",
            "Train Epoch: 54 [120/1400 (9%)]\tLoss: 169.276596\n",
            "Train Epoch: 54 [160/1400 (11%)]\tLoss: 136.641708\n",
            "Train Epoch: 54 [200/1400 (14%)]\tLoss: 149.292648\n",
            "Train Epoch: 54 [240/1400 (17%)]\tLoss: 118.746765\n",
            "Train Epoch: 54 [280/1400 (20%)]\tLoss: 133.944702\n",
            "Train Epoch: 54 [320/1400 (23%)]\tLoss: 118.665741\n",
            "Train Epoch: 54 [360/1400 (26%)]\tLoss: 138.875381\n",
            "Train Epoch: 54 [400/1400 (29%)]\tLoss: 107.492096\n",
            "Train Epoch: 54 [440/1400 (31%)]\tLoss: 139.224854\n",
            "Train Epoch: 54 [480/1400 (34%)]\tLoss: 163.301468\n",
            "Train Epoch: 54 [520/1400 (37%)]\tLoss: 132.131531\n",
            "Train Epoch: 54 [560/1400 (40%)]\tLoss: 151.464050\n",
            "Train Epoch: 54 [600/1400 (43%)]\tLoss: 155.678360\n",
            "Train Epoch: 54 [640/1400 (46%)]\tLoss: 147.588211\n",
            "Train Epoch: 54 [680/1400 (49%)]\tLoss: 138.021912\n",
            "Train Epoch: 54 [720/1400 (51%)]\tLoss: 141.671890\n",
            "Train Epoch: 54 [760/1400 (54%)]\tLoss: 145.860672\n",
            "Train Epoch: 54 [800/1400 (57%)]\tLoss: 147.676086\n",
            "Train Epoch: 54 [840/1400 (60%)]\tLoss: 155.662552\n",
            "Train Epoch: 54 [880/1400 (63%)]\tLoss: 134.014526\n",
            "Train Epoch: 54 [920/1400 (66%)]\tLoss: 144.685898\n",
            "Train Epoch: 54 [960/1400 (69%)]\tLoss: 120.792519\n",
            "Train Epoch: 54 [1000/1400 (71%)]\tLoss: 137.006699\n",
            "Train Epoch: 54 [1040/1400 (74%)]\tLoss: 139.293350\n",
            "Train Epoch: 54 [1080/1400 (77%)]\tLoss: 153.837921\n",
            "Train Epoch: 54 [1120/1400 (80%)]\tLoss: 132.470047\n",
            "Train Epoch: 54 [1160/1400 (83%)]\tLoss: 142.673645\n",
            "Train Epoch: 54 [1200/1400 (86%)]\tLoss: 149.158783\n",
            "Train Epoch: 54 [1240/1400 (89%)]\tLoss: 161.841995\n",
            "Train Epoch: 54 [1280/1400 (91%)]\tLoss: 130.161987\n",
            "Train Epoch: 54 [1320/1400 (94%)]\tLoss: 153.293610\n",
            "Train Epoch: 54 [1360/1400 (97%)]\tLoss: 137.430450\n",
            "====> Epoch: 54 Average loss: 144.4186\n",
            "====> Test set loss: 148.0514\n",
            "Train Epoch: 55 [0/1400 (0%)]\tLoss: 136.708862\n",
            "Train Epoch: 55 [40/1400 (3%)]\tLoss: 142.744797\n",
            "Train Epoch: 55 [80/1400 (6%)]\tLoss: 138.310837\n",
            "Train Epoch: 55 [120/1400 (9%)]\tLoss: 163.843445\n",
            "Train Epoch: 55 [160/1400 (11%)]\tLoss: 161.336609\n",
            "Train Epoch: 55 [200/1400 (14%)]\tLoss: 141.724030\n",
            "Train Epoch: 55 [240/1400 (17%)]\tLoss: 137.270523\n",
            "Train Epoch: 55 [280/1400 (20%)]\tLoss: 164.410828\n",
            "Train Epoch: 55 [320/1400 (23%)]\tLoss: 165.851288\n",
            "Train Epoch: 55 [360/1400 (26%)]\tLoss: 160.902466\n",
            "Train Epoch: 55 [400/1400 (29%)]\tLoss: 141.317444\n",
            "Train Epoch: 55 [440/1400 (31%)]\tLoss: 145.581039\n",
            "Train Epoch: 55 [480/1400 (34%)]\tLoss: 138.022949\n",
            "Train Epoch: 55 [520/1400 (37%)]\tLoss: 163.720367\n",
            "Train Epoch: 55 [560/1400 (40%)]\tLoss: 139.365219\n",
            "Train Epoch: 55 [600/1400 (43%)]\tLoss: 138.435745\n",
            "Train Epoch: 55 [640/1400 (46%)]\tLoss: 137.744415\n",
            "Train Epoch: 55 [680/1400 (49%)]\tLoss: 147.131897\n",
            "Train Epoch: 55 [720/1400 (51%)]\tLoss: 143.300339\n",
            "Train Epoch: 55 [760/1400 (54%)]\tLoss: 129.484406\n",
            "Train Epoch: 55 [800/1400 (57%)]\tLoss: 146.662430\n",
            "Train Epoch: 55 [840/1400 (60%)]\tLoss: 150.329910\n",
            "Train Epoch: 55 [880/1400 (63%)]\tLoss: 136.294189\n",
            "Train Epoch: 55 [920/1400 (66%)]\tLoss: 134.575134\n",
            "Train Epoch: 55 [960/1400 (69%)]\tLoss: 154.666443\n",
            "Train Epoch: 55 [1000/1400 (71%)]\tLoss: 130.781693\n",
            "Train Epoch: 55 [1040/1400 (74%)]\tLoss: 132.678131\n",
            "Train Epoch: 55 [1080/1400 (77%)]\tLoss: 140.477371\n",
            "Train Epoch: 55 [1120/1400 (80%)]\tLoss: 148.249298\n",
            "Train Epoch: 55 [1160/1400 (83%)]\tLoss: 137.725510\n",
            "Train Epoch: 55 [1200/1400 (86%)]\tLoss: 161.490631\n",
            "Train Epoch: 55 [1240/1400 (89%)]\tLoss: 142.816452\n",
            "Train Epoch: 55 [1280/1400 (91%)]\tLoss: 160.570328\n",
            "Train Epoch: 55 [1320/1400 (94%)]\tLoss: 142.305573\n",
            "Train Epoch: 55 [1360/1400 (97%)]\tLoss: 161.742142\n",
            "====> Epoch: 55 Average loss: 144.2027\n",
            "====> Test set loss: 147.9021\n",
            "Train Epoch: 56 [0/1400 (0%)]\tLoss: 173.342850\n",
            "Train Epoch: 56 [40/1400 (3%)]\tLoss: 143.250320\n",
            "Train Epoch: 56 [80/1400 (6%)]\tLoss: 140.772812\n",
            "Train Epoch: 56 [120/1400 (9%)]\tLoss: 118.353027\n",
            "Train Epoch: 56 [160/1400 (11%)]\tLoss: 140.436020\n",
            "Train Epoch: 56 [200/1400 (14%)]\tLoss: 132.964645\n",
            "Train Epoch: 56 [240/1400 (17%)]\tLoss: 155.456665\n",
            "Train Epoch: 56 [280/1400 (20%)]\tLoss: 142.168732\n",
            "Train Epoch: 56 [320/1400 (23%)]\tLoss: 154.986984\n",
            "Train Epoch: 56 [360/1400 (26%)]\tLoss: 142.687180\n",
            "Train Epoch: 56 [400/1400 (29%)]\tLoss: 154.288925\n",
            "Train Epoch: 56 [440/1400 (31%)]\tLoss: 142.521530\n",
            "Train Epoch: 56 [480/1400 (34%)]\tLoss: 137.301682\n",
            "Train Epoch: 56 [520/1400 (37%)]\tLoss: 155.690231\n",
            "Train Epoch: 56 [560/1400 (40%)]\tLoss: 140.991898\n",
            "Train Epoch: 56 [600/1400 (43%)]\tLoss: 153.714142\n",
            "Train Epoch: 56 [640/1400 (46%)]\tLoss: 136.877457\n",
            "Train Epoch: 56 [680/1400 (49%)]\tLoss: 106.922180\n",
            "Train Epoch: 56 [720/1400 (51%)]\tLoss: 147.470581\n",
            "Train Epoch: 56 [760/1400 (54%)]\tLoss: 137.294952\n",
            "Train Epoch: 56 [800/1400 (57%)]\tLoss: 130.644760\n",
            "Train Epoch: 56 [840/1400 (60%)]\tLoss: 157.048447\n",
            "Train Epoch: 56 [880/1400 (63%)]\tLoss: 148.159210\n",
            "Train Epoch: 56 [920/1400 (66%)]\tLoss: 133.732407\n",
            "Train Epoch: 56 [960/1400 (69%)]\tLoss: 149.890320\n",
            "Train Epoch: 56 [1000/1400 (71%)]\tLoss: 174.082062\n",
            "Train Epoch: 56 [1040/1400 (74%)]\tLoss: 160.783234\n",
            "Train Epoch: 56 [1080/1400 (77%)]\tLoss: 134.863419\n",
            "Train Epoch: 56 [1120/1400 (80%)]\tLoss: 128.951050\n",
            "Train Epoch: 56 [1160/1400 (83%)]\tLoss: 117.089317\n",
            "Train Epoch: 56 [1200/1400 (86%)]\tLoss: 158.985382\n",
            "Train Epoch: 56 [1240/1400 (89%)]\tLoss: 117.697723\n",
            "Train Epoch: 56 [1280/1400 (91%)]\tLoss: 145.421646\n",
            "Train Epoch: 56 [1320/1400 (94%)]\tLoss: 143.610168\n",
            "Train Epoch: 56 [1360/1400 (97%)]\tLoss: 151.247818\n",
            "====> Epoch: 56 Average loss: 143.9085\n",
            "====> Test set loss: 147.5745\n",
            "Train Epoch: 57 [0/1400 (0%)]\tLoss: 139.193863\n",
            "Train Epoch: 57 [40/1400 (3%)]\tLoss: 163.982452\n",
            "Train Epoch: 57 [80/1400 (6%)]\tLoss: 143.274796\n",
            "Train Epoch: 57 [120/1400 (9%)]\tLoss: 150.162552\n",
            "Train Epoch: 57 [160/1400 (11%)]\tLoss: 130.882355\n",
            "Train Epoch: 57 [200/1400 (14%)]\tLoss: 139.364731\n",
            "Train Epoch: 57 [240/1400 (17%)]\tLoss: 140.268356\n",
            "Train Epoch: 57 [280/1400 (20%)]\tLoss: 139.489807\n",
            "Train Epoch: 57 [320/1400 (23%)]\tLoss: 145.586060\n",
            "Train Epoch: 57 [360/1400 (26%)]\tLoss: 137.066589\n",
            "Train Epoch: 57 [400/1400 (29%)]\tLoss: 129.223526\n",
            "Train Epoch: 57 [440/1400 (31%)]\tLoss: 137.058884\n",
            "Train Epoch: 57 [480/1400 (34%)]\tLoss: 146.373795\n",
            "Train Epoch: 57 [520/1400 (37%)]\tLoss: 134.764343\n",
            "Train Epoch: 57 [560/1400 (40%)]\tLoss: 158.029419\n",
            "Train Epoch: 57 [600/1400 (43%)]\tLoss: 139.507843\n",
            "Train Epoch: 57 [640/1400 (46%)]\tLoss: 144.468857\n",
            "Train Epoch: 57 [680/1400 (49%)]\tLoss: 117.579880\n",
            "Train Epoch: 57 [720/1400 (51%)]\tLoss: 120.734154\n",
            "Train Epoch: 57 [760/1400 (54%)]\tLoss: 157.231110\n",
            "Train Epoch: 57 [800/1400 (57%)]\tLoss: 145.713593\n",
            "Train Epoch: 57 [840/1400 (60%)]\tLoss: 159.796494\n",
            "Train Epoch: 57 [880/1400 (63%)]\tLoss: 143.957886\n",
            "Train Epoch: 57 [920/1400 (66%)]\tLoss: 137.008118\n",
            "Train Epoch: 57 [960/1400 (69%)]\tLoss: 143.412872\n",
            "Train Epoch: 57 [1000/1400 (71%)]\tLoss: 156.113708\n",
            "Train Epoch: 57 [1040/1400 (74%)]\tLoss: 111.656822\n",
            "Train Epoch: 57 [1080/1400 (77%)]\tLoss: 154.002029\n",
            "Train Epoch: 57 [1120/1400 (80%)]\tLoss: 166.514221\n",
            "Train Epoch: 57 [1160/1400 (83%)]\tLoss: 139.605179\n",
            "Train Epoch: 57 [1200/1400 (86%)]\tLoss: 149.785675\n",
            "Train Epoch: 57 [1240/1400 (89%)]\tLoss: 145.448685\n",
            "Train Epoch: 57 [1280/1400 (91%)]\tLoss: 144.099548\n",
            "Train Epoch: 57 [1320/1400 (94%)]\tLoss: 152.007584\n",
            "Train Epoch: 57 [1360/1400 (97%)]\tLoss: 135.717667\n",
            "====> Epoch: 57 Average loss: 143.6707\n",
            "====> Test set loss: 147.2734\n",
            "Train Epoch: 58 [0/1400 (0%)]\tLoss: 133.760101\n",
            "Train Epoch: 58 [40/1400 (3%)]\tLoss: 136.061920\n",
            "Train Epoch: 58 [80/1400 (6%)]\tLoss: 146.050888\n",
            "Train Epoch: 58 [120/1400 (9%)]\tLoss: 142.624435\n",
            "Train Epoch: 58 [160/1400 (11%)]\tLoss: 142.115646\n",
            "Train Epoch: 58 [200/1400 (14%)]\tLoss: 162.394852\n",
            "Train Epoch: 58 [240/1400 (17%)]\tLoss: 166.087097\n",
            "Train Epoch: 58 [280/1400 (20%)]\tLoss: 154.550018\n",
            "Train Epoch: 58 [320/1400 (23%)]\tLoss: 137.780594\n",
            "Train Epoch: 58 [360/1400 (26%)]\tLoss: 131.057663\n",
            "Train Epoch: 58 [400/1400 (29%)]\tLoss: 138.393890\n",
            "Train Epoch: 58 [440/1400 (31%)]\tLoss: 152.768478\n",
            "Train Epoch: 58 [480/1400 (34%)]\tLoss: 148.870667\n",
            "Train Epoch: 58 [520/1400 (37%)]\tLoss: 144.532318\n",
            "Train Epoch: 58 [560/1400 (40%)]\tLoss: 148.289383\n",
            "Train Epoch: 58 [600/1400 (43%)]\tLoss: 137.573456\n",
            "Train Epoch: 58 [640/1400 (46%)]\tLoss: 138.001907\n",
            "Train Epoch: 58 [680/1400 (49%)]\tLoss: 129.048828\n",
            "Train Epoch: 58 [720/1400 (51%)]\tLoss: 159.764481\n",
            "Train Epoch: 58 [760/1400 (54%)]\tLoss: 123.716721\n",
            "Train Epoch: 58 [800/1400 (57%)]\tLoss: 134.127518\n",
            "Train Epoch: 58 [840/1400 (60%)]\tLoss: 132.992920\n",
            "Train Epoch: 58 [880/1400 (63%)]\tLoss: 146.083649\n",
            "Train Epoch: 58 [920/1400 (66%)]\tLoss: 142.661224\n",
            "Train Epoch: 58 [960/1400 (69%)]\tLoss: 131.284973\n",
            "Train Epoch: 58 [1000/1400 (71%)]\tLoss: 129.708267\n",
            "Train Epoch: 58 [1040/1400 (74%)]\tLoss: 152.272049\n",
            "Train Epoch: 58 [1080/1400 (77%)]\tLoss: 152.568558\n",
            "Train Epoch: 58 [1120/1400 (80%)]\tLoss: 117.813324\n",
            "Train Epoch: 58 [1160/1400 (83%)]\tLoss: 130.389236\n",
            "Train Epoch: 58 [1200/1400 (86%)]\tLoss: 154.413834\n",
            "Train Epoch: 58 [1240/1400 (89%)]\tLoss: 148.063751\n",
            "Train Epoch: 58 [1280/1400 (91%)]\tLoss: 128.028885\n",
            "Train Epoch: 58 [1320/1400 (94%)]\tLoss: 157.583984\n",
            "Train Epoch: 58 [1360/1400 (97%)]\tLoss: 127.956863\n",
            "====> Epoch: 58 Average loss: 143.5026\n",
            "====> Test set loss: 147.1463\n",
            "Train Epoch: 59 [0/1400 (0%)]\tLoss: 113.812683\n",
            "Train Epoch: 59 [40/1400 (3%)]\tLoss: 132.265594\n",
            "Train Epoch: 59 [80/1400 (6%)]\tLoss: 158.167847\n",
            "Train Epoch: 59 [120/1400 (9%)]\tLoss: 129.473114\n",
            "Train Epoch: 59 [160/1400 (11%)]\tLoss: 157.348221\n",
            "Train Epoch: 59 [200/1400 (14%)]\tLoss: 123.932365\n",
            "Train Epoch: 59 [240/1400 (17%)]\tLoss: 176.012512\n",
            "Train Epoch: 59 [280/1400 (20%)]\tLoss: 145.577774\n",
            "Train Epoch: 59 [320/1400 (23%)]\tLoss: 132.300369\n",
            "Train Epoch: 59 [360/1400 (26%)]\tLoss: 161.617462\n",
            "Train Epoch: 59 [400/1400 (29%)]\tLoss: 151.638672\n",
            "Train Epoch: 59 [440/1400 (31%)]\tLoss: 149.457367\n",
            "Train Epoch: 59 [480/1400 (34%)]\tLoss: 131.893646\n",
            "Train Epoch: 59 [520/1400 (37%)]\tLoss: 145.030334\n",
            "Train Epoch: 59 [560/1400 (40%)]\tLoss: 156.412369\n",
            "Train Epoch: 59 [600/1400 (43%)]\tLoss: 121.990822\n",
            "Train Epoch: 59 [640/1400 (46%)]\tLoss: 156.510315\n",
            "Train Epoch: 59 [680/1400 (49%)]\tLoss: 156.412338\n",
            "Train Epoch: 59 [720/1400 (51%)]\tLoss: 140.074387\n",
            "Train Epoch: 59 [760/1400 (54%)]\tLoss: 129.404587\n",
            "Train Epoch: 59 [800/1400 (57%)]\tLoss: 146.185852\n",
            "Train Epoch: 59 [840/1400 (60%)]\tLoss: 164.978119\n",
            "Train Epoch: 59 [880/1400 (63%)]\tLoss: 137.397430\n",
            "Train Epoch: 59 [920/1400 (66%)]\tLoss: 131.115463\n",
            "Train Epoch: 59 [960/1400 (69%)]\tLoss: 131.347244\n",
            "Train Epoch: 59 [1000/1400 (71%)]\tLoss: 162.044113\n",
            "Train Epoch: 59 [1040/1400 (74%)]\tLoss: 145.552277\n",
            "Train Epoch: 59 [1080/1400 (77%)]\tLoss: 155.492096\n",
            "Train Epoch: 59 [1120/1400 (80%)]\tLoss: 148.149918\n",
            "Train Epoch: 59 [1160/1400 (83%)]\tLoss: 130.509125\n",
            "Train Epoch: 59 [1200/1400 (86%)]\tLoss: 150.336395\n",
            "Train Epoch: 59 [1240/1400 (89%)]\tLoss: 131.846603\n",
            "Train Epoch: 59 [1280/1400 (91%)]\tLoss: 157.352646\n",
            "Train Epoch: 59 [1320/1400 (94%)]\tLoss: 133.862335\n",
            "Train Epoch: 59 [1360/1400 (97%)]\tLoss: 137.216827\n",
            "====> Epoch: 59 Average loss: 143.3064\n",
            "====> Test set loss: 147.1772\n",
            "Train Epoch: 60 [0/1400 (0%)]\tLoss: 152.392868\n",
            "Train Epoch: 60 [40/1400 (3%)]\tLoss: 137.317459\n",
            "Train Epoch: 60 [80/1400 (6%)]\tLoss: 146.636292\n",
            "Train Epoch: 60 [120/1400 (9%)]\tLoss: 164.821487\n",
            "Train Epoch: 60 [160/1400 (11%)]\tLoss: 147.786026\n",
            "Train Epoch: 60 [200/1400 (14%)]\tLoss: 144.313278\n",
            "Train Epoch: 60 [240/1400 (17%)]\tLoss: 132.025314\n",
            "Train Epoch: 60 [280/1400 (20%)]\tLoss: 143.168106\n",
            "Train Epoch: 60 [320/1400 (23%)]\tLoss: 171.213699\n",
            "Train Epoch: 60 [360/1400 (26%)]\tLoss: 146.281631\n",
            "Train Epoch: 60 [400/1400 (29%)]\tLoss: 132.907730\n",
            "Train Epoch: 60 [440/1400 (31%)]\tLoss: 133.870148\n",
            "Train Epoch: 60 [480/1400 (34%)]\tLoss: 123.815559\n",
            "Train Epoch: 60 [520/1400 (37%)]\tLoss: 136.868179\n",
            "Train Epoch: 60 [560/1400 (40%)]\tLoss: 140.829361\n",
            "Train Epoch: 60 [600/1400 (43%)]\tLoss: 155.500336\n",
            "Train Epoch: 60 [640/1400 (46%)]\tLoss: 135.020325\n",
            "Train Epoch: 60 [680/1400 (49%)]\tLoss: 159.630203\n",
            "Train Epoch: 60 [720/1400 (51%)]\tLoss: 151.390228\n",
            "Train Epoch: 60 [760/1400 (54%)]\tLoss: 130.535660\n",
            "Train Epoch: 60 [800/1400 (57%)]\tLoss: 151.706863\n",
            "Train Epoch: 60 [840/1400 (60%)]\tLoss: 121.425629\n",
            "Train Epoch: 60 [880/1400 (63%)]\tLoss: 129.384766\n",
            "Train Epoch: 60 [920/1400 (66%)]\tLoss: 140.727615\n",
            "Train Epoch: 60 [960/1400 (69%)]\tLoss: 150.785919\n",
            "Train Epoch: 60 [1000/1400 (71%)]\tLoss: 154.390350\n",
            "Train Epoch: 60 [1040/1400 (74%)]\tLoss: 147.989227\n",
            "Train Epoch: 60 [1080/1400 (77%)]\tLoss: 160.591995\n",
            "Train Epoch: 60 [1120/1400 (80%)]\tLoss: 156.467026\n",
            "Train Epoch: 60 [1160/1400 (83%)]\tLoss: 133.612000\n",
            "Train Epoch: 60 [1200/1400 (86%)]\tLoss: 129.711044\n",
            "Train Epoch: 60 [1240/1400 (89%)]\tLoss: 137.382721\n",
            "Train Epoch: 60 [1280/1400 (91%)]\tLoss: 150.212677\n",
            "Train Epoch: 60 [1320/1400 (94%)]\tLoss: 136.508575\n",
            "Train Epoch: 60 [1360/1400 (97%)]\tLoss: 126.310486\n",
            "====> Epoch: 60 Average loss: 143.1394\n",
            "====> Test set loss: 146.8327\n",
            "Train Epoch: 61 [0/1400 (0%)]\tLoss: 141.550034\n",
            "Train Epoch: 61 [40/1400 (3%)]\tLoss: 136.095947\n",
            "Train Epoch: 61 [80/1400 (6%)]\tLoss: 138.929047\n",
            "Train Epoch: 61 [120/1400 (9%)]\tLoss: 133.473160\n",
            "Train Epoch: 61 [160/1400 (11%)]\tLoss: 132.929596\n",
            "Train Epoch: 61 [200/1400 (14%)]\tLoss: 133.807190\n",
            "Train Epoch: 61 [240/1400 (17%)]\tLoss: 148.936035\n",
            "Train Epoch: 61 [280/1400 (20%)]\tLoss: 130.297684\n",
            "Train Epoch: 61 [320/1400 (23%)]\tLoss: 113.573692\n",
            "Train Epoch: 61 [360/1400 (26%)]\tLoss: 146.769150\n",
            "Train Epoch: 61 [400/1400 (29%)]\tLoss: 149.374390\n",
            "Train Epoch: 61 [440/1400 (31%)]\tLoss: 161.188004\n",
            "Train Epoch: 61 [480/1400 (34%)]\tLoss: 149.501114\n",
            "Train Epoch: 61 [520/1400 (37%)]\tLoss: 164.399445\n",
            "Train Epoch: 61 [560/1400 (40%)]\tLoss: 152.066803\n",
            "Train Epoch: 61 [600/1400 (43%)]\tLoss: 140.019302\n",
            "Train Epoch: 61 [640/1400 (46%)]\tLoss: 161.285492\n",
            "Train Epoch: 61 [680/1400 (49%)]\tLoss: 147.946854\n",
            "Train Epoch: 61 [720/1400 (51%)]\tLoss: 165.993362\n",
            "Train Epoch: 61 [760/1400 (54%)]\tLoss: 146.659698\n",
            "Train Epoch: 61 [800/1400 (57%)]\tLoss: 143.409012\n",
            "Train Epoch: 61 [840/1400 (60%)]\tLoss: 139.833740\n",
            "Train Epoch: 61 [880/1400 (63%)]\tLoss: 124.887383\n",
            "Train Epoch: 61 [920/1400 (66%)]\tLoss: 150.569763\n",
            "Train Epoch: 61 [960/1400 (69%)]\tLoss: 133.269455\n",
            "Train Epoch: 61 [1000/1400 (71%)]\tLoss: 146.406921\n",
            "Train Epoch: 61 [1040/1400 (74%)]\tLoss: 153.198502\n",
            "Train Epoch: 61 [1080/1400 (77%)]\tLoss: 143.817078\n",
            "Train Epoch: 61 [1120/1400 (80%)]\tLoss: 145.931000\n",
            "Train Epoch: 61 [1160/1400 (83%)]\tLoss: 120.540825\n",
            "Train Epoch: 61 [1200/1400 (86%)]\tLoss: 117.678879\n",
            "Train Epoch: 61 [1240/1400 (89%)]\tLoss: 155.302811\n",
            "Train Epoch: 61 [1280/1400 (91%)]\tLoss: 125.129425\n",
            "Train Epoch: 61 [1320/1400 (94%)]\tLoss: 161.348999\n",
            "Train Epoch: 61 [1360/1400 (97%)]\tLoss: 133.861908\n",
            "====> Epoch: 61 Average loss: 142.9253\n",
            "====> Test set loss: 146.7985\n",
            "Train Epoch: 62 [0/1400 (0%)]\tLoss: 148.925827\n",
            "Train Epoch: 62 [40/1400 (3%)]\tLoss: 132.674362\n",
            "Train Epoch: 62 [80/1400 (6%)]\tLoss: 134.049667\n",
            "Train Epoch: 62 [120/1400 (9%)]\tLoss: 151.487823\n",
            "Train Epoch: 62 [160/1400 (11%)]\tLoss: 150.442703\n",
            "Train Epoch: 62 [200/1400 (14%)]\tLoss: 151.720963\n",
            "Train Epoch: 62 [240/1400 (17%)]\tLoss: 145.388748\n",
            "Train Epoch: 62 [280/1400 (20%)]\tLoss: 127.556107\n",
            "Train Epoch: 62 [320/1400 (23%)]\tLoss: 144.090424\n",
            "Train Epoch: 62 [360/1400 (26%)]\tLoss: 146.004074\n",
            "Train Epoch: 62 [400/1400 (29%)]\tLoss: 147.448822\n",
            "Train Epoch: 62 [440/1400 (31%)]\tLoss: 124.913025\n",
            "Train Epoch: 62 [480/1400 (34%)]\tLoss: 143.938171\n",
            "Train Epoch: 62 [520/1400 (37%)]\tLoss: 138.414230\n",
            "Train Epoch: 62 [560/1400 (40%)]\tLoss: 132.427979\n",
            "Train Epoch: 62 [600/1400 (43%)]\tLoss: 131.367615\n",
            "Train Epoch: 62 [640/1400 (46%)]\tLoss: 172.825806\n",
            "Train Epoch: 62 [680/1400 (49%)]\tLoss: 159.486603\n",
            "Train Epoch: 62 [720/1400 (51%)]\tLoss: 167.134155\n",
            "Train Epoch: 62 [760/1400 (54%)]\tLoss: 141.562622\n",
            "Train Epoch: 62 [800/1400 (57%)]\tLoss: 128.099213\n",
            "Train Epoch: 62 [840/1400 (60%)]\tLoss: 149.666931\n",
            "Train Epoch: 62 [880/1400 (63%)]\tLoss: 138.167404\n",
            "Train Epoch: 62 [920/1400 (66%)]\tLoss: 135.210648\n",
            "Train Epoch: 62 [960/1400 (69%)]\tLoss: 132.477448\n",
            "Train Epoch: 62 [1000/1400 (71%)]\tLoss: 161.586136\n",
            "Train Epoch: 62 [1040/1400 (74%)]\tLoss: 144.200119\n",
            "Train Epoch: 62 [1080/1400 (77%)]\tLoss: 139.925476\n",
            "Train Epoch: 62 [1120/1400 (80%)]\tLoss: 146.319214\n",
            "Train Epoch: 62 [1160/1400 (83%)]\tLoss: 156.037231\n",
            "Train Epoch: 62 [1200/1400 (86%)]\tLoss: 123.332092\n",
            "Train Epoch: 62 [1240/1400 (89%)]\tLoss: 153.892014\n",
            "Train Epoch: 62 [1280/1400 (91%)]\tLoss: 145.998489\n",
            "Train Epoch: 62 [1320/1400 (94%)]\tLoss: 145.865051\n",
            "Train Epoch: 62 [1360/1400 (97%)]\tLoss: 156.348434\n",
            "====> Epoch: 62 Average loss: 142.7443\n",
            "====> Test set loss: 146.5954\n",
            "Train Epoch: 63 [0/1400 (0%)]\tLoss: 128.677307\n",
            "Train Epoch: 63 [40/1400 (3%)]\tLoss: 130.984955\n",
            "Train Epoch: 63 [80/1400 (6%)]\tLoss: 149.612457\n",
            "Train Epoch: 63 [120/1400 (9%)]\tLoss: 128.078598\n",
            "Train Epoch: 63 [160/1400 (11%)]\tLoss: 131.503494\n",
            "Train Epoch: 63 [200/1400 (14%)]\tLoss: 142.372803\n",
            "Train Epoch: 63 [240/1400 (17%)]\tLoss: 126.901680\n",
            "Train Epoch: 63 [280/1400 (20%)]\tLoss: 146.349289\n",
            "Train Epoch: 63 [320/1400 (23%)]\tLoss: 161.140778\n",
            "Train Epoch: 63 [360/1400 (26%)]\tLoss: 127.355614\n",
            "Train Epoch: 63 [400/1400 (29%)]\tLoss: 160.959930\n",
            "Train Epoch: 63 [440/1400 (31%)]\tLoss: 156.864883\n",
            "Train Epoch: 63 [480/1400 (34%)]\tLoss: 133.582809\n",
            "Train Epoch: 63 [520/1400 (37%)]\tLoss: 132.447617\n",
            "Train Epoch: 63 [560/1400 (40%)]\tLoss: 149.370331\n",
            "Train Epoch: 63 [600/1400 (43%)]\tLoss: 137.505676\n",
            "Train Epoch: 63 [640/1400 (46%)]\tLoss: 163.169952\n",
            "Train Epoch: 63 [680/1400 (49%)]\tLoss: 152.860489\n",
            "Train Epoch: 63 [720/1400 (51%)]\tLoss: 141.533539\n",
            "Train Epoch: 63 [760/1400 (54%)]\tLoss: 150.705002\n",
            "Train Epoch: 63 [800/1400 (57%)]\tLoss: 165.159103\n",
            "Train Epoch: 63 [840/1400 (60%)]\tLoss: 168.277267\n",
            "Train Epoch: 63 [880/1400 (63%)]\tLoss: 131.693115\n",
            "Train Epoch: 63 [920/1400 (66%)]\tLoss: 139.845108\n",
            "Train Epoch: 63 [960/1400 (69%)]\tLoss: 122.539589\n",
            "Train Epoch: 63 [1000/1400 (71%)]\tLoss: 129.482391\n",
            "Train Epoch: 63 [1040/1400 (74%)]\tLoss: 124.242126\n",
            "Train Epoch: 63 [1080/1400 (77%)]\tLoss: 139.494766\n",
            "Train Epoch: 63 [1120/1400 (80%)]\tLoss: 167.522766\n",
            "Train Epoch: 63 [1160/1400 (83%)]\tLoss: 145.847778\n",
            "Train Epoch: 63 [1200/1400 (86%)]\tLoss: 151.342941\n",
            "Train Epoch: 63 [1240/1400 (89%)]\tLoss: 140.230499\n",
            "Train Epoch: 63 [1280/1400 (91%)]\tLoss: 149.175781\n",
            "Train Epoch: 63 [1320/1400 (94%)]\tLoss: 154.494095\n",
            "Train Epoch: 63 [1360/1400 (97%)]\tLoss: 163.992233\n",
            "====> Epoch: 63 Average loss: 142.6048\n",
            "====> Test set loss: 146.4960\n",
            "Train Epoch: 64 [0/1400 (0%)]\tLoss: 134.401642\n",
            "Train Epoch: 64 [40/1400 (3%)]\tLoss: 145.362564\n",
            "Train Epoch: 64 [80/1400 (6%)]\tLoss: 156.869766\n",
            "Train Epoch: 64 [120/1400 (9%)]\tLoss: 144.479950\n",
            "Train Epoch: 64 [160/1400 (11%)]\tLoss: 131.359848\n",
            "Train Epoch: 64 [200/1400 (14%)]\tLoss: 152.417877\n",
            "Train Epoch: 64 [240/1400 (17%)]\tLoss: 143.507202\n",
            "Train Epoch: 64 [280/1400 (20%)]\tLoss: 128.641754\n",
            "Train Epoch: 64 [320/1400 (23%)]\tLoss: 146.556229\n",
            "Train Epoch: 64 [360/1400 (26%)]\tLoss: 137.905563\n",
            "Train Epoch: 64 [400/1400 (29%)]\tLoss: 138.668839\n",
            "Train Epoch: 64 [440/1400 (31%)]\tLoss: 154.256088\n",
            "Train Epoch: 64 [480/1400 (34%)]\tLoss: 142.619751\n",
            "Train Epoch: 64 [520/1400 (37%)]\tLoss: 144.639984\n",
            "Train Epoch: 64 [560/1400 (40%)]\tLoss: 134.105042\n",
            "Train Epoch: 64 [600/1400 (43%)]\tLoss: 161.769287\n",
            "Train Epoch: 64 [640/1400 (46%)]\tLoss: 149.065018\n",
            "Train Epoch: 64 [680/1400 (49%)]\tLoss: 124.620628\n",
            "Train Epoch: 64 [720/1400 (51%)]\tLoss: 163.106506\n",
            "Train Epoch: 64 [760/1400 (54%)]\tLoss: 140.854721\n",
            "Train Epoch: 64 [800/1400 (57%)]\tLoss: 116.333878\n",
            "Train Epoch: 64 [840/1400 (60%)]\tLoss: 129.175247\n",
            "Train Epoch: 64 [880/1400 (63%)]\tLoss: 139.240540\n",
            "Train Epoch: 64 [920/1400 (66%)]\tLoss: 130.533386\n",
            "Train Epoch: 64 [960/1400 (69%)]\tLoss: 152.885681\n",
            "Train Epoch: 64 [1000/1400 (71%)]\tLoss: 128.487213\n",
            "Train Epoch: 64 [1040/1400 (74%)]\tLoss: 142.953644\n",
            "Train Epoch: 64 [1080/1400 (77%)]\tLoss: 131.628357\n",
            "Train Epoch: 64 [1120/1400 (80%)]\tLoss: 141.605362\n",
            "Train Epoch: 64 [1160/1400 (83%)]\tLoss: 158.087662\n",
            "Train Epoch: 64 [1200/1400 (86%)]\tLoss: 139.441925\n",
            "Train Epoch: 64 [1240/1400 (89%)]\tLoss: 163.188965\n",
            "Train Epoch: 64 [1280/1400 (91%)]\tLoss: 116.888008\n",
            "Train Epoch: 64 [1320/1400 (94%)]\tLoss: 125.135429\n",
            "Train Epoch: 64 [1360/1400 (97%)]\tLoss: 147.397980\n",
            "====> Epoch: 64 Average loss: 142.5205\n",
            "====> Test set loss: 146.3892\n",
            "Train Epoch: 65 [0/1400 (0%)]\tLoss: 132.402359\n",
            "Train Epoch: 65 [40/1400 (3%)]\tLoss: 128.103699\n",
            "Train Epoch: 65 [80/1400 (6%)]\tLoss: 122.195496\n",
            "Train Epoch: 65 [120/1400 (9%)]\tLoss: 133.250565\n",
            "Train Epoch: 65 [160/1400 (11%)]\tLoss: 149.460281\n",
            "Train Epoch: 65 [200/1400 (14%)]\tLoss: 138.024399\n",
            "Train Epoch: 65 [240/1400 (17%)]\tLoss: 136.942795\n",
            "Train Epoch: 65 [280/1400 (20%)]\tLoss: 143.643845\n",
            "Train Epoch: 65 [320/1400 (23%)]\tLoss: 151.697784\n",
            "Train Epoch: 65 [360/1400 (26%)]\tLoss: 119.076790\n",
            "Train Epoch: 65 [400/1400 (29%)]\tLoss: 134.263290\n",
            "Train Epoch: 65 [440/1400 (31%)]\tLoss: 127.195244\n",
            "Train Epoch: 65 [480/1400 (34%)]\tLoss: 143.182434\n",
            "Train Epoch: 65 [520/1400 (37%)]\tLoss: 140.846497\n",
            "Train Epoch: 65 [560/1400 (40%)]\tLoss: 127.599014\n",
            "Train Epoch: 65 [600/1400 (43%)]\tLoss: 167.498383\n",
            "Train Epoch: 65 [640/1400 (46%)]\tLoss: 155.066269\n",
            "Train Epoch: 65 [680/1400 (49%)]\tLoss: 137.250031\n",
            "Train Epoch: 65 [720/1400 (51%)]\tLoss: 129.652786\n",
            "Train Epoch: 65 [760/1400 (54%)]\tLoss: 128.241760\n",
            "Train Epoch: 65 [800/1400 (57%)]\tLoss: 166.031860\n",
            "Train Epoch: 65 [840/1400 (60%)]\tLoss: 142.824524\n",
            "Train Epoch: 65 [880/1400 (63%)]\tLoss: 125.024834\n",
            "Train Epoch: 65 [920/1400 (66%)]\tLoss: 154.718460\n",
            "Train Epoch: 65 [960/1400 (69%)]\tLoss: 147.781036\n",
            "Train Epoch: 65 [1000/1400 (71%)]\tLoss: 143.735703\n",
            "Train Epoch: 65 [1040/1400 (74%)]\tLoss: 145.274979\n",
            "Train Epoch: 65 [1080/1400 (77%)]\tLoss: 134.929977\n",
            "Train Epoch: 65 [1120/1400 (80%)]\tLoss: 142.439072\n",
            "Train Epoch: 65 [1160/1400 (83%)]\tLoss: 152.389328\n",
            "Train Epoch: 65 [1200/1400 (86%)]\tLoss: 172.826355\n",
            "Train Epoch: 65 [1240/1400 (89%)]\tLoss: 139.400513\n",
            "Train Epoch: 65 [1280/1400 (91%)]\tLoss: 150.715881\n",
            "Train Epoch: 65 [1320/1400 (94%)]\tLoss: 148.611465\n",
            "Train Epoch: 65 [1360/1400 (97%)]\tLoss: 148.421555\n",
            "====> Epoch: 65 Average loss: 142.3651\n",
            "====> Test set loss: 146.3005\n",
            "Train Epoch: 66 [0/1400 (0%)]\tLoss: 143.122070\n",
            "Train Epoch: 66 [40/1400 (3%)]\tLoss: 142.752121\n",
            "Train Epoch: 66 [80/1400 (6%)]\tLoss: 156.460617\n",
            "Train Epoch: 66 [120/1400 (9%)]\tLoss: 148.170349\n",
            "Train Epoch: 66 [160/1400 (11%)]\tLoss: 128.754608\n",
            "Train Epoch: 66 [200/1400 (14%)]\tLoss: 158.002045\n",
            "Train Epoch: 66 [240/1400 (17%)]\tLoss: 124.067551\n",
            "Train Epoch: 66 [280/1400 (20%)]\tLoss: 132.393600\n",
            "Train Epoch: 66 [320/1400 (23%)]\tLoss: 152.170502\n",
            "Train Epoch: 66 [360/1400 (26%)]\tLoss: 154.116592\n",
            "Train Epoch: 66 [400/1400 (29%)]\tLoss: 140.823395\n",
            "Train Epoch: 66 [440/1400 (31%)]\tLoss: 133.524536\n",
            "Train Epoch: 66 [480/1400 (34%)]\tLoss: 133.776047\n",
            "Train Epoch: 66 [520/1400 (37%)]\tLoss: 156.672897\n",
            "Train Epoch: 66 [560/1400 (40%)]\tLoss: 156.739349\n",
            "Train Epoch: 66 [600/1400 (43%)]\tLoss: 122.555412\n",
            "Train Epoch: 66 [640/1400 (46%)]\tLoss: 157.883194\n",
            "Train Epoch: 66 [680/1400 (49%)]\tLoss: 145.008591\n",
            "Train Epoch: 66 [720/1400 (51%)]\tLoss: 151.201187\n",
            "Train Epoch: 66 [760/1400 (54%)]\tLoss: 160.333023\n",
            "Train Epoch: 66 [800/1400 (57%)]\tLoss: 137.334717\n",
            "Train Epoch: 66 [840/1400 (60%)]\tLoss: 158.108337\n",
            "Train Epoch: 66 [880/1400 (63%)]\tLoss: 155.583771\n",
            "Train Epoch: 66 [920/1400 (66%)]\tLoss: 126.288239\n",
            "Train Epoch: 66 [960/1400 (69%)]\tLoss: 153.793976\n",
            "Train Epoch: 66 [1000/1400 (71%)]\tLoss: 151.643723\n",
            "Train Epoch: 66 [1040/1400 (74%)]\tLoss: 137.416901\n",
            "Train Epoch: 66 [1080/1400 (77%)]\tLoss: 129.980103\n",
            "Train Epoch: 66 [1120/1400 (80%)]\tLoss: 139.719681\n",
            "Train Epoch: 66 [1160/1400 (83%)]\tLoss: 123.132439\n",
            "Train Epoch: 66 [1200/1400 (86%)]\tLoss: 154.805893\n",
            "Train Epoch: 66 [1240/1400 (89%)]\tLoss: 128.552826\n",
            "Train Epoch: 66 [1280/1400 (91%)]\tLoss: 148.403793\n",
            "Train Epoch: 66 [1320/1400 (94%)]\tLoss: 126.523811\n",
            "Train Epoch: 66 [1360/1400 (97%)]\tLoss: 138.207855\n",
            "====> Epoch: 66 Average loss: 142.2745\n",
            "====> Test set loss: 146.2483\n",
            "Train Epoch: 67 [0/1400 (0%)]\tLoss: 149.805878\n",
            "Train Epoch: 67 [40/1400 (3%)]\tLoss: 137.668457\n",
            "Train Epoch: 67 [80/1400 (6%)]\tLoss: 147.676041\n",
            "Train Epoch: 67 [120/1400 (9%)]\tLoss: 149.071259\n",
            "Train Epoch: 67 [160/1400 (11%)]\tLoss: 113.727905\n",
            "Train Epoch: 67 [200/1400 (14%)]\tLoss: 119.251526\n",
            "Train Epoch: 67 [240/1400 (17%)]\tLoss: 137.212814\n",
            "Train Epoch: 67 [280/1400 (20%)]\tLoss: 131.944809\n",
            "Train Epoch: 67 [320/1400 (23%)]\tLoss: 146.895966\n",
            "Train Epoch: 67 [360/1400 (26%)]\tLoss: 144.000107\n",
            "Train Epoch: 67 [400/1400 (29%)]\tLoss: 122.804749\n",
            "Train Epoch: 67 [440/1400 (31%)]\tLoss: 140.808167\n",
            "Train Epoch: 67 [480/1400 (34%)]\tLoss: 163.025055\n",
            "Train Epoch: 67 [520/1400 (37%)]\tLoss: 168.543732\n",
            "Train Epoch: 67 [560/1400 (40%)]\tLoss: 118.506294\n",
            "Train Epoch: 67 [600/1400 (43%)]\tLoss: 141.892853\n",
            "Train Epoch: 67 [640/1400 (46%)]\tLoss: 139.661819\n",
            "Train Epoch: 67 [680/1400 (49%)]\tLoss: 133.654083\n",
            "Train Epoch: 67 [720/1400 (51%)]\tLoss: 143.434128\n",
            "Train Epoch: 67 [760/1400 (54%)]\tLoss: 132.797958\n",
            "Train Epoch: 67 [800/1400 (57%)]\tLoss: 151.410889\n",
            "Train Epoch: 67 [840/1400 (60%)]\tLoss: 144.743652\n",
            "Train Epoch: 67 [880/1400 (63%)]\tLoss: 141.260727\n",
            "Train Epoch: 67 [920/1400 (66%)]\tLoss: 127.917252\n",
            "Train Epoch: 67 [960/1400 (69%)]\tLoss: 114.972878\n",
            "Train Epoch: 67 [1000/1400 (71%)]\tLoss: 141.014771\n",
            "Train Epoch: 67 [1040/1400 (74%)]\tLoss: 121.217018\n",
            "Train Epoch: 67 [1080/1400 (77%)]\tLoss: 154.946472\n",
            "Train Epoch: 67 [1120/1400 (80%)]\tLoss: 128.879852\n",
            "Train Epoch: 67 [1160/1400 (83%)]\tLoss: 144.215042\n",
            "Train Epoch: 67 [1200/1400 (86%)]\tLoss: 136.983749\n",
            "Train Epoch: 67 [1240/1400 (89%)]\tLoss: 122.388542\n",
            "Train Epoch: 67 [1280/1400 (91%)]\tLoss: 139.290070\n",
            "Train Epoch: 67 [1320/1400 (94%)]\tLoss: 163.613968\n",
            "Train Epoch: 67 [1360/1400 (97%)]\tLoss: 133.823837\n",
            "====> Epoch: 67 Average loss: 142.1967\n",
            "====> Test set loss: 146.0109\n",
            "Train Epoch: 68 [0/1400 (0%)]\tLoss: 159.253586\n",
            "Train Epoch: 68 [40/1400 (3%)]\tLoss: 127.085464\n",
            "Train Epoch: 68 [80/1400 (6%)]\tLoss: 171.921432\n",
            "Train Epoch: 68 [120/1400 (9%)]\tLoss: 151.964233\n",
            "Train Epoch: 68 [160/1400 (11%)]\tLoss: 132.809967\n",
            "Train Epoch: 68 [200/1400 (14%)]\tLoss: 127.174011\n",
            "Train Epoch: 68 [240/1400 (17%)]\tLoss: 148.907349\n",
            "Train Epoch: 68 [280/1400 (20%)]\tLoss: 159.215164\n",
            "Train Epoch: 68 [320/1400 (23%)]\tLoss: 147.077164\n",
            "Train Epoch: 68 [360/1400 (26%)]\tLoss: 160.800003\n",
            "Train Epoch: 68 [400/1400 (29%)]\tLoss: 149.185883\n",
            "Train Epoch: 68 [440/1400 (31%)]\tLoss: 149.507263\n",
            "Train Epoch: 68 [480/1400 (34%)]\tLoss: 159.815628\n",
            "Train Epoch: 68 [520/1400 (37%)]\tLoss: 153.478363\n",
            "Train Epoch: 68 [560/1400 (40%)]\tLoss: 139.827255\n",
            "Train Epoch: 68 [600/1400 (43%)]\tLoss: 135.143372\n",
            "Train Epoch: 68 [640/1400 (46%)]\tLoss: 140.943680\n",
            "Train Epoch: 68 [680/1400 (49%)]\tLoss: 138.973434\n",
            "Train Epoch: 68 [720/1400 (51%)]\tLoss: 140.781662\n",
            "Train Epoch: 68 [760/1400 (54%)]\tLoss: 150.598724\n",
            "Train Epoch: 68 [800/1400 (57%)]\tLoss: 137.971558\n",
            "Train Epoch: 68 [840/1400 (60%)]\tLoss: 142.530151\n",
            "Train Epoch: 68 [880/1400 (63%)]\tLoss: 148.062073\n",
            "Train Epoch: 68 [920/1400 (66%)]\tLoss: 139.454285\n",
            "Train Epoch: 68 [960/1400 (69%)]\tLoss: 144.043396\n",
            "Train Epoch: 68 [1000/1400 (71%)]\tLoss: 170.519394\n",
            "Train Epoch: 68 [1040/1400 (74%)]\tLoss: 142.294434\n",
            "Train Epoch: 68 [1080/1400 (77%)]\tLoss: 158.587692\n",
            "Train Epoch: 68 [1120/1400 (80%)]\tLoss: 140.325089\n",
            "Train Epoch: 68 [1160/1400 (83%)]\tLoss: 148.972076\n",
            "Train Epoch: 68 [1200/1400 (86%)]\tLoss: 165.082062\n",
            "Train Epoch: 68 [1240/1400 (89%)]\tLoss: 119.248253\n",
            "Train Epoch: 68 [1280/1400 (91%)]\tLoss: 137.997955\n",
            "Train Epoch: 68 [1320/1400 (94%)]\tLoss: 136.654449\n",
            "Train Epoch: 68 [1360/1400 (97%)]\tLoss: 157.359375\n",
            "====> Epoch: 68 Average loss: 142.0774\n",
            "====> Test set loss: 146.0795\n",
            "Train Epoch: 69 [0/1400 (0%)]\tLoss: 128.288071\n",
            "Train Epoch: 69 [40/1400 (3%)]\tLoss: 136.808044\n",
            "Train Epoch: 69 [80/1400 (6%)]\tLoss: 130.669144\n",
            "Train Epoch: 69 [120/1400 (9%)]\tLoss: 175.528137\n",
            "Train Epoch: 69 [160/1400 (11%)]\tLoss: 140.093002\n",
            "Train Epoch: 69 [200/1400 (14%)]\tLoss: 125.631798\n",
            "Train Epoch: 69 [240/1400 (17%)]\tLoss: 129.815125\n",
            "Train Epoch: 69 [280/1400 (20%)]\tLoss: 125.813522\n",
            "Train Epoch: 69 [320/1400 (23%)]\tLoss: 156.429031\n",
            "Train Epoch: 69 [360/1400 (26%)]\tLoss: 153.757935\n",
            "Train Epoch: 69 [400/1400 (29%)]\tLoss: 136.611923\n",
            "Train Epoch: 69 [440/1400 (31%)]\tLoss: 136.749527\n",
            "Train Epoch: 69 [480/1400 (34%)]\tLoss: 156.418930\n",
            "Train Epoch: 69 [520/1400 (37%)]\tLoss: 157.605820\n",
            "Train Epoch: 69 [560/1400 (40%)]\tLoss: 140.395538\n",
            "Train Epoch: 69 [600/1400 (43%)]\tLoss: 150.635742\n",
            "Train Epoch: 69 [640/1400 (46%)]\tLoss: 144.172607\n",
            "Train Epoch: 69 [680/1400 (49%)]\tLoss: 132.957855\n",
            "Train Epoch: 69 [720/1400 (51%)]\tLoss: 148.702911\n",
            "Train Epoch: 69 [760/1400 (54%)]\tLoss: 118.081360\n",
            "Train Epoch: 69 [800/1400 (57%)]\tLoss: 128.838531\n",
            "Train Epoch: 69 [840/1400 (60%)]\tLoss: 136.709381\n",
            "Train Epoch: 69 [880/1400 (63%)]\tLoss: 146.989441\n",
            "Train Epoch: 69 [920/1400 (66%)]\tLoss: 150.626541\n",
            "Train Epoch: 69 [960/1400 (69%)]\tLoss: 142.524078\n",
            "Train Epoch: 69 [1000/1400 (71%)]\tLoss: 133.759155\n",
            "Train Epoch: 69 [1040/1400 (74%)]\tLoss: 142.967728\n",
            "Train Epoch: 69 [1080/1400 (77%)]\tLoss: 146.591415\n",
            "Train Epoch: 69 [1120/1400 (80%)]\tLoss: 134.253677\n",
            "Train Epoch: 69 [1160/1400 (83%)]\tLoss: 127.977837\n",
            "Train Epoch: 69 [1200/1400 (86%)]\tLoss: 146.791138\n",
            "Train Epoch: 69 [1240/1400 (89%)]\tLoss: 143.519745\n",
            "Train Epoch: 69 [1280/1400 (91%)]\tLoss: 126.852470\n",
            "Train Epoch: 69 [1320/1400 (94%)]\tLoss: 130.710846\n",
            "Train Epoch: 69 [1360/1400 (97%)]\tLoss: 150.684372\n",
            "====> Epoch: 69 Average loss: 142.0484\n",
            "====> Test set loss: 145.9615\n",
            "Train Epoch: 70 [0/1400 (0%)]\tLoss: 125.493774\n",
            "Train Epoch: 70 [40/1400 (3%)]\tLoss: 157.076920\n",
            "Train Epoch: 70 [80/1400 (6%)]\tLoss: 141.010056\n",
            "Train Epoch: 70 [120/1400 (9%)]\tLoss: 128.402405\n",
            "Train Epoch: 70 [160/1400 (11%)]\tLoss: 140.470154\n",
            "Train Epoch: 70 [200/1400 (14%)]\tLoss: 137.247055\n",
            "Train Epoch: 70 [240/1400 (17%)]\tLoss: 138.533615\n",
            "Train Epoch: 70 [280/1400 (20%)]\tLoss: 131.994400\n",
            "Train Epoch: 70 [320/1400 (23%)]\tLoss: 130.529251\n",
            "Train Epoch: 70 [360/1400 (26%)]\tLoss: 139.329483\n",
            "Train Epoch: 70 [400/1400 (29%)]\tLoss: 147.887100\n",
            "Train Epoch: 70 [440/1400 (31%)]\tLoss: 167.227234\n",
            "Train Epoch: 70 [480/1400 (34%)]\tLoss: 142.016846\n",
            "Train Epoch: 70 [520/1400 (37%)]\tLoss: 145.485931\n",
            "Train Epoch: 70 [560/1400 (40%)]\tLoss: 113.900940\n",
            "Train Epoch: 70 [600/1400 (43%)]\tLoss: 136.448822\n",
            "Train Epoch: 70 [640/1400 (46%)]\tLoss: 131.066986\n",
            "Train Epoch: 70 [680/1400 (49%)]\tLoss: 121.529243\n",
            "Train Epoch: 70 [720/1400 (51%)]\tLoss: 142.984222\n",
            "Train Epoch: 70 [760/1400 (54%)]\tLoss: 146.847153\n",
            "Train Epoch: 70 [800/1400 (57%)]\tLoss: 128.080688\n",
            "Train Epoch: 70 [840/1400 (60%)]\tLoss: 119.070267\n",
            "Train Epoch: 70 [880/1400 (63%)]\tLoss: 145.007233\n",
            "Train Epoch: 70 [920/1400 (66%)]\tLoss: 128.538712\n",
            "Train Epoch: 70 [960/1400 (69%)]\tLoss: 154.613373\n",
            "Train Epoch: 70 [1000/1400 (71%)]\tLoss: 145.602554\n",
            "Train Epoch: 70 [1040/1400 (74%)]\tLoss: 156.604599\n",
            "Train Epoch: 70 [1080/1400 (77%)]\tLoss: 156.852097\n",
            "Train Epoch: 70 [1120/1400 (80%)]\tLoss: 140.147964\n",
            "Train Epoch: 70 [1160/1400 (83%)]\tLoss: 154.414841\n",
            "Train Epoch: 70 [1200/1400 (86%)]\tLoss: 139.120712\n",
            "Train Epoch: 70 [1240/1400 (89%)]\tLoss: 123.040184\n",
            "Train Epoch: 70 [1280/1400 (91%)]\tLoss: 143.173813\n",
            "Train Epoch: 70 [1320/1400 (94%)]\tLoss: 146.153015\n",
            "Train Epoch: 70 [1360/1400 (97%)]\tLoss: 153.915604\n",
            "====> Epoch: 70 Average loss: 141.9831\n",
            "====> Test set loss: 145.8568\n",
            "Train Epoch: 71 [0/1400 (0%)]\tLoss: 150.025146\n",
            "Train Epoch: 71 [40/1400 (3%)]\tLoss: 123.504814\n",
            "Train Epoch: 71 [80/1400 (6%)]\tLoss: 145.198883\n",
            "Train Epoch: 71 [120/1400 (9%)]\tLoss: 167.419281\n",
            "Train Epoch: 71 [160/1400 (11%)]\tLoss: 177.891846\n",
            "Train Epoch: 71 [200/1400 (14%)]\tLoss: 135.916626\n",
            "Train Epoch: 71 [240/1400 (17%)]\tLoss: 126.050209\n",
            "Train Epoch: 71 [280/1400 (20%)]\tLoss: 174.791794\n",
            "Train Epoch: 71 [320/1400 (23%)]\tLoss: 129.159439\n",
            "Train Epoch: 71 [360/1400 (26%)]\tLoss: 159.970734\n",
            "Train Epoch: 71 [400/1400 (29%)]\tLoss: 127.950844\n",
            "Train Epoch: 71 [440/1400 (31%)]\tLoss: 146.945450\n",
            "Train Epoch: 71 [480/1400 (34%)]\tLoss: 147.490692\n",
            "Train Epoch: 71 [520/1400 (37%)]\tLoss: 137.462326\n",
            "Train Epoch: 71 [560/1400 (40%)]\tLoss: 111.319221\n",
            "Train Epoch: 71 [600/1400 (43%)]\tLoss: 136.967697\n",
            "Train Epoch: 71 [640/1400 (46%)]\tLoss: 132.434326\n",
            "Train Epoch: 71 [680/1400 (49%)]\tLoss: 137.261154\n",
            "Train Epoch: 71 [720/1400 (51%)]\tLoss: 157.990768\n",
            "Train Epoch: 71 [760/1400 (54%)]\tLoss: 135.198837\n",
            "Train Epoch: 71 [800/1400 (57%)]\tLoss: 154.108063\n",
            "Train Epoch: 71 [840/1400 (60%)]\tLoss: 124.750107\n",
            "Train Epoch: 71 [880/1400 (63%)]\tLoss: 145.124496\n",
            "Train Epoch: 71 [920/1400 (66%)]\tLoss: 132.691742\n",
            "Train Epoch: 71 [960/1400 (69%)]\tLoss: 118.491188\n",
            "Train Epoch: 71 [1000/1400 (71%)]\tLoss: 149.269577\n",
            "Train Epoch: 71 [1040/1400 (74%)]\tLoss: 144.690872\n",
            "Train Epoch: 71 [1080/1400 (77%)]\tLoss: 160.997772\n",
            "Train Epoch: 71 [1120/1400 (80%)]\tLoss: 149.045502\n",
            "Train Epoch: 71 [1160/1400 (83%)]\tLoss: 138.347672\n",
            "Train Epoch: 71 [1200/1400 (86%)]\tLoss: 145.477417\n",
            "Train Epoch: 71 [1240/1400 (89%)]\tLoss: 132.099152\n",
            "Train Epoch: 71 [1280/1400 (91%)]\tLoss: 136.137695\n",
            "Train Epoch: 71 [1320/1400 (94%)]\tLoss: 154.979324\n",
            "Train Epoch: 71 [1360/1400 (97%)]\tLoss: 143.624649\n",
            "====> Epoch: 71 Average loss: 141.8852\n",
            "====> Test set loss: 145.9328\n",
            "Train Epoch: 72 [0/1400 (0%)]\tLoss: 145.681473\n",
            "Train Epoch: 72 [40/1400 (3%)]\tLoss: 144.199112\n",
            "Train Epoch: 72 [80/1400 (6%)]\tLoss: 128.475571\n",
            "Train Epoch: 72 [120/1400 (9%)]\tLoss: 157.641541\n",
            "Train Epoch: 72 [160/1400 (11%)]\tLoss: 148.400497\n",
            "Train Epoch: 72 [200/1400 (14%)]\tLoss: 159.922211\n",
            "Train Epoch: 72 [240/1400 (17%)]\tLoss: 148.002930\n",
            "Train Epoch: 72 [280/1400 (20%)]\tLoss: 143.230194\n",
            "Train Epoch: 72 [320/1400 (23%)]\tLoss: 144.886169\n",
            "Train Epoch: 72 [360/1400 (26%)]\tLoss: 138.751892\n",
            "Train Epoch: 72 [400/1400 (29%)]\tLoss: 163.667328\n",
            "Train Epoch: 72 [440/1400 (31%)]\tLoss: 137.679672\n",
            "Train Epoch: 72 [480/1400 (34%)]\tLoss: 144.293335\n",
            "Train Epoch: 72 [520/1400 (37%)]\tLoss: 141.961594\n",
            "Train Epoch: 72 [560/1400 (40%)]\tLoss: 149.604172\n",
            "Train Epoch: 72 [600/1400 (43%)]\tLoss: 135.469193\n",
            "Train Epoch: 72 [640/1400 (46%)]\tLoss: 115.511230\n",
            "Train Epoch: 72 [680/1400 (49%)]\tLoss: 157.166962\n",
            "Train Epoch: 72 [720/1400 (51%)]\tLoss: 126.510559\n",
            "Train Epoch: 72 [760/1400 (54%)]\tLoss: 147.463608\n",
            "Train Epoch: 72 [800/1400 (57%)]\tLoss: 141.605377\n",
            "Train Epoch: 72 [840/1400 (60%)]\tLoss: 143.517212\n",
            "Train Epoch: 72 [880/1400 (63%)]\tLoss: 133.091064\n",
            "Train Epoch: 72 [920/1400 (66%)]\tLoss: 117.197464\n",
            "Train Epoch: 72 [960/1400 (69%)]\tLoss: 127.156052\n",
            "Train Epoch: 72 [1000/1400 (71%)]\tLoss: 152.287048\n",
            "Train Epoch: 72 [1040/1400 (74%)]\tLoss: 150.757050\n",
            "Train Epoch: 72 [1080/1400 (77%)]\tLoss: 124.610626\n",
            "Train Epoch: 72 [1120/1400 (80%)]\tLoss: 142.532547\n",
            "Train Epoch: 72 [1160/1400 (83%)]\tLoss: 168.733826\n",
            "Train Epoch: 72 [1200/1400 (86%)]\tLoss: 150.652618\n",
            "Train Epoch: 72 [1240/1400 (89%)]\tLoss: 127.315987\n",
            "Train Epoch: 72 [1280/1400 (91%)]\tLoss: 129.046692\n",
            "Train Epoch: 72 [1320/1400 (94%)]\tLoss: 138.776611\n",
            "Train Epoch: 72 [1360/1400 (97%)]\tLoss: 165.465088\n",
            "====> Epoch: 72 Average loss: 141.8592\n",
            "====> Test set loss: 145.8483\n",
            "Train Epoch: 73 [0/1400 (0%)]\tLoss: 122.031677\n",
            "Train Epoch: 73 [40/1400 (3%)]\tLoss: 137.456741\n",
            "Train Epoch: 73 [80/1400 (6%)]\tLoss: 135.479553\n",
            "Train Epoch: 73 [120/1400 (9%)]\tLoss: 129.282761\n",
            "Train Epoch: 73 [160/1400 (11%)]\tLoss: 139.402557\n",
            "Train Epoch: 73 [200/1400 (14%)]\tLoss: 124.164978\n",
            "Train Epoch: 73 [240/1400 (17%)]\tLoss: 155.118149\n",
            "Train Epoch: 73 [280/1400 (20%)]\tLoss: 142.935944\n",
            "Train Epoch: 73 [320/1400 (23%)]\tLoss: 156.077194\n",
            "Train Epoch: 73 [360/1400 (26%)]\tLoss: 150.794418\n",
            "Train Epoch: 73 [400/1400 (29%)]\tLoss: 141.380890\n",
            "Train Epoch: 73 [440/1400 (31%)]\tLoss: 135.485947\n",
            "Train Epoch: 73 [480/1400 (34%)]\tLoss: 161.972107\n",
            "Train Epoch: 73 [520/1400 (37%)]\tLoss: 162.161942\n",
            "Train Epoch: 73 [560/1400 (40%)]\tLoss: 150.470245\n",
            "Train Epoch: 73 [600/1400 (43%)]\tLoss: 109.759735\n",
            "Train Epoch: 73 [640/1400 (46%)]\tLoss: 163.674744\n",
            "Train Epoch: 73 [680/1400 (49%)]\tLoss: 139.018021\n",
            "Train Epoch: 73 [720/1400 (51%)]\tLoss: 120.011185\n",
            "Train Epoch: 73 [760/1400 (54%)]\tLoss: 150.226959\n",
            "Train Epoch: 73 [800/1400 (57%)]\tLoss: 124.651588\n",
            "Train Epoch: 73 [840/1400 (60%)]\tLoss: 153.303406\n",
            "Train Epoch: 73 [880/1400 (63%)]\tLoss: 156.908630\n",
            "Train Epoch: 73 [920/1400 (66%)]\tLoss: 127.640152\n",
            "Train Epoch: 73 [960/1400 (69%)]\tLoss: 149.291672\n",
            "Train Epoch: 73 [1000/1400 (71%)]\tLoss: 160.630417\n",
            "Train Epoch: 73 [1040/1400 (74%)]\tLoss: 178.408691\n",
            "Train Epoch: 73 [1080/1400 (77%)]\tLoss: 142.895966\n",
            "Train Epoch: 73 [1120/1400 (80%)]\tLoss: 122.450287\n",
            "Train Epoch: 73 [1160/1400 (83%)]\tLoss: 151.940018\n",
            "Train Epoch: 73 [1200/1400 (86%)]\tLoss: 124.073662\n",
            "Train Epoch: 73 [1240/1400 (89%)]\tLoss: 128.131134\n",
            "Train Epoch: 73 [1280/1400 (91%)]\tLoss: 133.101700\n",
            "Train Epoch: 73 [1320/1400 (94%)]\tLoss: 143.179977\n",
            "Train Epoch: 73 [1360/1400 (97%)]\tLoss: 125.049858\n",
            "====> Epoch: 73 Average loss: 141.7763\n",
            "====> Test set loss: 145.8242\n",
            "Train Epoch: 74 [0/1400 (0%)]\tLoss: 125.065247\n",
            "Train Epoch: 74 [40/1400 (3%)]\tLoss: 141.266891\n",
            "Train Epoch: 74 [80/1400 (6%)]\tLoss: 141.664719\n",
            "Train Epoch: 74 [120/1400 (9%)]\tLoss: 135.358231\n",
            "Train Epoch: 74 [160/1400 (11%)]\tLoss: 127.372635\n",
            "Train Epoch: 74 [200/1400 (14%)]\tLoss: 145.277786\n",
            "Train Epoch: 74 [240/1400 (17%)]\tLoss: 142.214035\n",
            "Train Epoch: 74 [280/1400 (20%)]\tLoss: 138.575546\n",
            "Train Epoch: 74 [320/1400 (23%)]\tLoss: 138.062134\n",
            "Train Epoch: 74 [360/1400 (26%)]\tLoss: 144.397018\n",
            "Train Epoch: 74 [400/1400 (29%)]\tLoss: 121.614838\n",
            "Train Epoch: 74 [440/1400 (31%)]\tLoss: 159.617554\n",
            "Train Epoch: 74 [480/1400 (34%)]\tLoss: 154.563553\n",
            "Train Epoch: 74 [520/1400 (37%)]\tLoss: 139.303070\n",
            "Train Epoch: 74 [560/1400 (40%)]\tLoss: 134.752533\n",
            "Train Epoch: 74 [600/1400 (43%)]\tLoss: 141.090942\n",
            "Train Epoch: 74 [640/1400 (46%)]\tLoss: 151.732285\n",
            "Train Epoch: 74 [680/1400 (49%)]\tLoss: 141.478455\n",
            "Train Epoch: 74 [720/1400 (51%)]\tLoss: 156.221848\n",
            "Train Epoch: 74 [760/1400 (54%)]\tLoss: 152.230652\n",
            "Train Epoch: 74 [800/1400 (57%)]\tLoss: 136.454010\n",
            "Train Epoch: 74 [840/1400 (60%)]\tLoss: 148.882416\n",
            "Train Epoch: 74 [880/1400 (63%)]\tLoss: 153.617310\n",
            "Train Epoch: 74 [920/1400 (66%)]\tLoss: 131.225128\n",
            "Train Epoch: 74 [960/1400 (69%)]\tLoss: 141.543777\n",
            "Train Epoch: 74 [1000/1400 (71%)]\tLoss: 132.164093\n",
            "Train Epoch: 74 [1040/1400 (74%)]\tLoss: 155.003799\n",
            "Train Epoch: 74 [1080/1400 (77%)]\tLoss: 148.570099\n",
            "Train Epoch: 74 [1120/1400 (80%)]\tLoss: 140.643570\n",
            "Train Epoch: 74 [1160/1400 (83%)]\tLoss: 155.286469\n",
            "Train Epoch: 74 [1200/1400 (86%)]\tLoss: 140.081284\n",
            "Train Epoch: 74 [1240/1400 (89%)]\tLoss: 183.711655\n",
            "Train Epoch: 74 [1280/1400 (91%)]\tLoss: 160.933426\n",
            "Train Epoch: 74 [1320/1400 (94%)]\tLoss: 124.094223\n",
            "Train Epoch: 74 [1360/1400 (97%)]\tLoss: 137.877380\n",
            "====> Epoch: 74 Average loss: 141.7734\n",
            "====> Test set loss: 145.7063\n",
            "Train Epoch: 75 [0/1400 (0%)]\tLoss: 130.817642\n",
            "Train Epoch: 75 [40/1400 (3%)]\tLoss: 132.825867\n",
            "Train Epoch: 75 [80/1400 (6%)]\tLoss: 146.950577\n",
            "Train Epoch: 75 [120/1400 (9%)]\tLoss: 127.931023\n",
            "Train Epoch: 75 [160/1400 (11%)]\tLoss: 125.120522\n",
            "Train Epoch: 75 [200/1400 (14%)]\tLoss: 152.734009\n",
            "Train Epoch: 75 [240/1400 (17%)]\tLoss: 131.214172\n",
            "Train Epoch: 75 [280/1400 (20%)]\tLoss: 156.163773\n",
            "Train Epoch: 75 [320/1400 (23%)]\tLoss: 152.292862\n",
            "Train Epoch: 75 [360/1400 (26%)]\tLoss: 143.974548\n",
            "Train Epoch: 75 [400/1400 (29%)]\tLoss: 142.153442\n",
            "Train Epoch: 75 [440/1400 (31%)]\tLoss: 162.534851\n",
            "Train Epoch: 75 [480/1400 (34%)]\tLoss: 130.078796\n",
            "Train Epoch: 75 [520/1400 (37%)]\tLoss: 137.328384\n",
            "Train Epoch: 75 [560/1400 (40%)]\tLoss: 128.439056\n",
            "Train Epoch: 75 [600/1400 (43%)]\tLoss: 139.352066\n",
            "Train Epoch: 75 [640/1400 (46%)]\tLoss: 147.199570\n",
            "Train Epoch: 75 [680/1400 (49%)]\tLoss: 139.296646\n",
            "Train Epoch: 75 [720/1400 (51%)]\tLoss: 153.383133\n",
            "Train Epoch: 75 [760/1400 (54%)]\tLoss: 158.308563\n",
            "Train Epoch: 75 [800/1400 (57%)]\tLoss: 121.216400\n",
            "Train Epoch: 75 [840/1400 (60%)]\tLoss: 125.964340\n",
            "Train Epoch: 75 [880/1400 (63%)]\tLoss: 125.853691\n",
            "Train Epoch: 75 [920/1400 (66%)]\tLoss: 162.089645\n",
            "Train Epoch: 75 [960/1400 (69%)]\tLoss: 128.934021\n",
            "Train Epoch: 75 [1000/1400 (71%)]\tLoss: 129.149567\n",
            "Train Epoch: 75 [1040/1400 (74%)]\tLoss: 151.849060\n",
            "Train Epoch: 75 [1080/1400 (77%)]\tLoss: 133.717789\n",
            "Train Epoch: 75 [1120/1400 (80%)]\tLoss: 160.419525\n",
            "Train Epoch: 75 [1160/1400 (83%)]\tLoss: 131.247177\n",
            "Train Epoch: 75 [1200/1400 (86%)]\tLoss: 121.489258\n",
            "Train Epoch: 75 [1240/1400 (89%)]\tLoss: 162.130600\n",
            "Train Epoch: 75 [1280/1400 (91%)]\tLoss: 155.689499\n",
            "Train Epoch: 75 [1320/1400 (94%)]\tLoss: 147.221222\n",
            "Train Epoch: 75 [1360/1400 (97%)]\tLoss: 169.646545\n",
            "====> Epoch: 75 Average loss: 141.7230\n",
            "====> Test set loss: 145.7832\n",
            "Train Epoch: 76 [0/1400 (0%)]\tLoss: 154.353714\n",
            "Train Epoch: 76 [40/1400 (3%)]\tLoss: 137.261002\n",
            "Train Epoch: 76 [80/1400 (6%)]\tLoss: 150.071838\n",
            "Train Epoch: 76 [120/1400 (9%)]\tLoss: 144.124908\n",
            "Train Epoch: 76 [160/1400 (11%)]\tLoss: 152.621719\n",
            "Train Epoch: 76 [200/1400 (14%)]\tLoss: 137.645157\n",
            "Train Epoch: 76 [240/1400 (17%)]\tLoss: 136.794968\n",
            "Train Epoch: 76 [280/1400 (20%)]\tLoss: 153.386444\n",
            "Train Epoch: 76 [320/1400 (23%)]\tLoss: 151.131348\n",
            "Train Epoch: 76 [360/1400 (26%)]\tLoss: 151.722931\n",
            "Train Epoch: 76 [400/1400 (29%)]\tLoss: 118.957382\n",
            "Train Epoch: 76 [440/1400 (31%)]\tLoss: 153.990463\n",
            "Train Epoch: 76 [480/1400 (34%)]\tLoss: 161.980240\n",
            "Train Epoch: 76 [520/1400 (37%)]\tLoss: 136.535568\n",
            "Train Epoch: 76 [560/1400 (40%)]\tLoss: 135.355118\n",
            "Train Epoch: 76 [600/1400 (43%)]\tLoss: 148.299515\n",
            "Train Epoch: 76 [640/1400 (46%)]\tLoss: 135.618607\n",
            "Train Epoch: 76 [680/1400 (49%)]\tLoss: 122.936974\n",
            "Train Epoch: 76 [720/1400 (51%)]\tLoss: 167.852386\n",
            "Train Epoch: 76 [760/1400 (54%)]\tLoss: 162.726105\n",
            "Train Epoch: 76 [800/1400 (57%)]\tLoss: 109.296021\n",
            "Train Epoch: 76 [840/1400 (60%)]\tLoss: 145.440567\n",
            "Train Epoch: 76 [880/1400 (63%)]\tLoss: 130.298660\n",
            "Train Epoch: 76 [920/1400 (66%)]\tLoss: 145.070709\n",
            "Train Epoch: 76 [960/1400 (69%)]\tLoss: 137.780258\n",
            "Train Epoch: 76 [1000/1400 (71%)]\tLoss: 126.294212\n",
            "Train Epoch: 76 [1040/1400 (74%)]\tLoss: 137.590897\n",
            "Train Epoch: 76 [1080/1400 (77%)]\tLoss: 130.863358\n",
            "Train Epoch: 76 [1120/1400 (80%)]\tLoss: 146.831558\n",
            "Train Epoch: 76 [1160/1400 (83%)]\tLoss: 134.406326\n",
            "Train Epoch: 76 [1200/1400 (86%)]\tLoss: 133.558838\n",
            "Train Epoch: 76 [1240/1400 (89%)]\tLoss: 156.004532\n",
            "Train Epoch: 76 [1280/1400 (91%)]\tLoss: 156.694870\n",
            "Train Epoch: 76 [1320/1400 (94%)]\tLoss: 149.794266\n",
            "Train Epoch: 76 [1360/1400 (97%)]\tLoss: 150.865479\n",
            "====> Epoch: 76 Average loss: 141.6887\n",
            "====> Test set loss: 145.8196\n",
            "Train Epoch: 77 [0/1400 (0%)]\tLoss: 138.473145\n",
            "Train Epoch: 77 [40/1400 (3%)]\tLoss: 131.275833\n",
            "Train Epoch: 77 [80/1400 (6%)]\tLoss: 157.911407\n",
            "Train Epoch: 77 [120/1400 (9%)]\tLoss: 138.908493\n",
            "Train Epoch: 77 [160/1400 (11%)]\tLoss: 144.762695\n",
            "Train Epoch: 77 [200/1400 (14%)]\tLoss: 154.094238\n",
            "Train Epoch: 77 [240/1400 (17%)]\tLoss: 127.948586\n",
            "Train Epoch: 77 [280/1400 (20%)]\tLoss: 151.248367\n",
            "Train Epoch: 77 [320/1400 (23%)]\tLoss: 129.132477\n",
            "Train Epoch: 77 [360/1400 (26%)]\tLoss: 131.377197\n",
            "Train Epoch: 77 [400/1400 (29%)]\tLoss: 135.666977\n",
            "Train Epoch: 77 [440/1400 (31%)]\tLoss: 136.513428\n",
            "Train Epoch: 77 [480/1400 (34%)]\tLoss: 141.654800\n",
            "Train Epoch: 77 [520/1400 (37%)]\tLoss: 142.725250\n",
            "Train Epoch: 77 [560/1400 (40%)]\tLoss: 128.763855\n",
            "Train Epoch: 77 [600/1400 (43%)]\tLoss: 152.024887\n",
            "Train Epoch: 77 [640/1400 (46%)]\tLoss: 160.976776\n",
            "Train Epoch: 77 [680/1400 (49%)]\tLoss: 143.768036\n",
            "Train Epoch: 77 [720/1400 (51%)]\tLoss: 127.084824\n",
            "Train Epoch: 77 [760/1400 (54%)]\tLoss: 143.404175\n",
            "Train Epoch: 77 [800/1400 (57%)]\tLoss: 145.948395\n",
            "Train Epoch: 77 [840/1400 (60%)]\tLoss: 161.230103\n",
            "Train Epoch: 77 [880/1400 (63%)]\tLoss: 134.777176\n",
            "Train Epoch: 77 [920/1400 (66%)]\tLoss: 142.437302\n",
            "Train Epoch: 77 [960/1400 (69%)]\tLoss: 142.832535\n",
            "Train Epoch: 77 [1000/1400 (71%)]\tLoss: 151.679901\n",
            "Train Epoch: 77 [1040/1400 (74%)]\tLoss: 143.646332\n",
            "Train Epoch: 77 [1080/1400 (77%)]\tLoss: 145.519104\n",
            "Train Epoch: 77 [1120/1400 (80%)]\tLoss: 135.270630\n",
            "Train Epoch: 77 [1160/1400 (83%)]\tLoss: 145.301651\n",
            "Train Epoch: 77 [1200/1400 (86%)]\tLoss: 127.937164\n",
            "Train Epoch: 77 [1240/1400 (89%)]\tLoss: 157.242569\n",
            "Train Epoch: 77 [1280/1400 (91%)]\tLoss: 126.634071\n",
            "Train Epoch: 77 [1320/1400 (94%)]\tLoss: 137.947479\n",
            "Train Epoch: 77 [1360/1400 (97%)]\tLoss: 131.714844\n",
            "====> Epoch: 77 Average loss: 141.6442\n",
            "====> Test set loss: 145.7555\n",
            "Train Epoch: 78 [0/1400 (0%)]\tLoss: 129.876648\n",
            "Train Epoch: 78 [40/1400 (3%)]\tLoss: 130.375870\n",
            "Train Epoch: 78 [80/1400 (6%)]\tLoss: 168.262207\n",
            "Train Epoch: 78 [120/1400 (9%)]\tLoss: 121.119652\n",
            "Train Epoch: 78 [160/1400 (11%)]\tLoss: 139.544067\n",
            "Train Epoch: 78 [200/1400 (14%)]\tLoss: 139.076843\n",
            "Train Epoch: 78 [240/1400 (17%)]\tLoss: 147.803589\n",
            "Train Epoch: 78 [280/1400 (20%)]\tLoss: 159.930344\n",
            "Train Epoch: 78 [320/1400 (23%)]\tLoss: 130.764618\n",
            "Train Epoch: 78 [360/1400 (26%)]\tLoss: 166.358780\n",
            "Train Epoch: 78 [400/1400 (29%)]\tLoss: 125.904396\n",
            "Train Epoch: 78 [440/1400 (31%)]\tLoss: 157.668915\n",
            "Train Epoch: 78 [480/1400 (34%)]\tLoss: 141.821503\n",
            "Train Epoch: 78 [520/1400 (37%)]\tLoss: 148.743652\n",
            "Train Epoch: 78 [560/1400 (40%)]\tLoss: 144.518936\n",
            "Train Epoch: 78 [600/1400 (43%)]\tLoss: 124.146919\n",
            "Train Epoch: 78 [640/1400 (46%)]\tLoss: 127.375648\n",
            "Train Epoch: 78 [680/1400 (49%)]\tLoss: 139.854324\n",
            "Train Epoch: 78 [720/1400 (51%)]\tLoss: 114.306320\n",
            "Train Epoch: 78 [760/1400 (54%)]\tLoss: 143.334381\n",
            "Train Epoch: 78 [800/1400 (57%)]\tLoss: 162.828903\n",
            "Train Epoch: 78 [840/1400 (60%)]\tLoss: 136.665405\n",
            "Train Epoch: 78 [880/1400 (63%)]\tLoss: 140.318848\n",
            "Train Epoch: 78 [920/1400 (66%)]\tLoss: 127.820984\n",
            "Train Epoch: 78 [960/1400 (69%)]\tLoss: 132.159653\n",
            "Train Epoch: 78 [1000/1400 (71%)]\tLoss: 149.368256\n",
            "Train Epoch: 78 [1040/1400 (74%)]\tLoss: 113.811440\n",
            "Train Epoch: 78 [1080/1400 (77%)]\tLoss: 152.780991\n",
            "Train Epoch: 78 [1120/1400 (80%)]\tLoss: 135.156006\n",
            "Train Epoch: 78 [1160/1400 (83%)]\tLoss: 146.083755\n",
            "Train Epoch: 78 [1200/1400 (86%)]\tLoss: 151.474747\n",
            "Train Epoch: 78 [1240/1400 (89%)]\tLoss: 125.945419\n",
            "Train Epoch: 78 [1280/1400 (91%)]\tLoss: 127.127731\n",
            "Train Epoch: 78 [1320/1400 (94%)]\tLoss: 129.590363\n",
            "Train Epoch: 78 [1360/1400 (97%)]\tLoss: 119.401222\n",
            "====> Epoch: 78 Average loss: 141.6735\n",
            "====> Test set loss: 145.6964\n",
            "Train Epoch: 79 [0/1400 (0%)]\tLoss: 137.078781\n",
            "Train Epoch: 79 [40/1400 (3%)]\tLoss: 147.599182\n",
            "Train Epoch: 79 [80/1400 (6%)]\tLoss: 148.978470\n",
            "Train Epoch: 79 [120/1400 (9%)]\tLoss: 146.859009\n",
            "Train Epoch: 79 [160/1400 (11%)]\tLoss: 134.321518\n",
            "Train Epoch: 79 [200/1400 (14%)]\tLoss: 137.340958\n",
            "Train Epoch: 79 [240/1400 (17%)]\tLoss: 139.924072\n",
            "Train Epoch: 79 [280/1400 (20%)]\tLoss: 138.438110\n",
            "Train Epoch: 79 [320/1400 (23%)]\tLoss: 144.002869\n",
            "Train Epoch: 79 [360/1400 (26%)]\tLoss: 166.828857\n",
            "Train Epoch: 79 [400/1400 (29%)]\tLoss: 158.330048\n",
            "Train Epoch: 79 [440/1400 (31%)]\tLoss: 147.603302\n",
            "Train Epoch: 79 [480/1400 (34%)]\tLoss: 137.338730\n",
            "Train Epoch: 79 [520/1400 (37%)]\tLoss: 127.311859\n",
            "Train Epoch: 79 [560/1400 (40%)]\tLoss: 148.171570\n",
            "Train Epoch: 79 [600/1400 (43%)]\tLoss: 122.250610\n",
            "Train Epoch: 79 [640/1400 (46%)]\tLoss: 163.481842\n",
            "Train Epoch: 79 [680/1400 (49%)]\tLoss: 132.093094\n",
            "Train Epoch: 79 [720/1400 (51%)]\tLoss: 135.861267\n",
            "Train Epoch: 79 [760/1400 (54%)]\tLoss: 157.451279\n",
            "Train Epoch: 79 [800/1400 (57%)]\tLoss: 140.423111\n",
            "Train Epoch: 79 [840/1400 (60%)]\tLoss: 147.432541\n",
            "Train Epoch: 79 [880/1400 (63%)]\tLoss: 127.060661\n",
            "Train Epoch: 79 [920/1400 (66%)]\tLoss: 134.438919\n",
            "Train Epoch: 79 [960/1400 (69%)]\tLoss: 132.614517\n",
            "Train Epoch: 79 [1000/1400 (71%)]\tLoss: 154.442383\n",
            "Train Epoch: 79 [1040/1400 (74%)]\tLoss: 147.083374\n",
            "Train Epoch: 79 [1080/1400 (77%)]\tLoss: 139.909195\n",
            "Train Epoch: 79 [1120/1400 (80%)]\tLoss: 160.693405\n",
            "Train Epoch: 79 [1160/1400 (83%)]\tLoss: 138.118698\n",
            "Train Epoch: 79 [1200/1400 (86%)]\tLoss: 148.446609\n",
            "Train Epoch: 79 [1240/1400 (89%)]\tLoss: 163.184586\n",
            "Train Epoch: 79 [1280/1400 (91%)]\tLoss: 138.318604\n",
            "Train Epoch: 79 [1320/1400 (94%)]\tLoss: 127.430252\n",
            "Train Epoch: 79 [1360/1400 (97%)]\tLoss: 140.549881\n",
            "====> Epoch: 79 Average loss: 141.6267\n",
            "====> Test set loss: 145.7239\n",
            "Train Epoch: 80 [0/1400 (0%)]\tLoss: 155.794418\n",
            "Train Epoch: 80 [40/1400 (3%)]\tLoss: 149.935822\n",
            "Train Epoch: 80 [80/1400 (6%)]\tLoss: 121.185295\n",
            "Train Epoch: 80 [120/1400 (9%)]\tLoss: 114.372452\n",
            "Train Epoch: 80 [160/1400 (11%)]\tLoss: 130.810806\n",
            "Train Epoch: 80 [200/1400 (14%)]\tLoss: 109.113083\n",
            "Train Epoch: 80 [240/1400 (17%)]\tLoss: 129.903381\n",
            "Train Epoch: 80 [280/1400 (20%)]\tLoss: 138.811264\n",
            "Train Epoch: 80 [320/1400 (23%)]\tLoss: 165.408173\n",
            "Train Epoch: 80 [360/1400 (26%)]\tLoss: 137.445999\n",
            "Train Epoch: 80 [400/1400 (29%)]\tLoss: 160.414307\n",
            "Train Epoch: 80 [440/1400 (31%)]\tLoss: 139.318863\n",
            "Train Epoch: 80 [480/1400 (34%)]\tLoss: 155.494583\n",
            "Train Epoch: 80 [520/1400 (37%)]\tLoss: 137.353119\n",
            "Train Epoch: 80 [560/1400 (40%)]\tLoss: 136.902283\n",
            "Train Epoch: 80 [600/1400 (43%)]\tLoss: 136.300674\n",
            "Train Epoch: 80 [640/1400 (46%)]\tLoss: 137.069092\n",
            "Train Epoch: 80 [680/1400 (49%)]\tLoss: 160.922638\n",
            "Train Epoch: 80 [720/1400 (51%)]\tLoss: 146.492126\n",
            "Train Epoch: 80 [760/1400 (54%)]\tLoss: 141.646866\n",
            "Train Epoch: 80 [800/1400 (57%)]\tLoss: 130.872040\n",
            "Train Epoch: 80 [840/1400 (60%)]\tLoss: 141.036682\n",
            "Train Epoch: 80 [880/1400 (63%)]\tLoss: 146.748642\n",
            "Train Epoch: 80 [920/1400 (66%)]\tLoss: 149.748978\n",
            "Train Epoch: 80 [960/1400 (69%)]\tLoss: 126.234734\n",
            "Train Epoch: 80 [1000/1400 (71%)]\tLoss: 127.714027\n",
            "Train Epoch: 80 [1040/1400 (74%)]\tLoss: 153.003983\n",
            "Train Epoch: 80 [1080/1400 (77%)]\tLoss: 131.436813\n",
            "Train Epoch: 80 [1120/1400 (80%)]\tLoss: 142.268204\n",
            "Train Epoch: 80 [1160/1400 (83%)]\tLoss: 145.714157\n",
            "Train Epoch: 80 [1200/1400 (86%)]\tLoss: 136.727280\n",
            "Train Epoch: 80 [1240/1400 (89%)]\tLoss: 146.508911\n",
            "Train Epoch: 80 [1280/1400 (91%)]\tLoss: 116.815315\n",
            "Train Epoch: 80 [1320/1400 (94%)]\tLoss: 121.167580\n",
            "Train Epoch: 80 [1360/1400 (97%)]\tLoss: 118.242691\n",
            "====> Epoch: 80 Average loss: 141.6055\n",
            "====> Test set loss: 145.8243\n",
            "Train Epoch: 81 [0/1400 (0%)]\tLoss: 160.358643\n",
            "Train Epoch: 81 [40/1400 (3%)]\tLoss: 134.225647\n",
            "Train Epoch: 81 [80/1400 (6%)]\tLoss: 155.565231\n",
            "Train Epoch: 81 [120/1400 (9%)]\tLoss: 136.551849\n",
            "Train Epoch: 81 [160/1400 (11%)]\tLoss: 162.144714\n",
            "Train Epoch: 81 [200/1400 (14%)]\tLoss: 129.265442\n",
            "Train Epoch: 81 [240/1400 (17%)]\tLoss: 134.951157\n",
            "Train Epoch: 81 [280/1400 (20%)]\tLoss: 130.870895\n",
            "Train Epoch: 81 [320/1400 (23%)]\tLoss: 127.413307\n",
            "Train Epoch: 81 [360/1400 (26%)]\tLoss: 143.437103\n",
            "Train Epoch: 81 [400/1400 (29%)]\tLoss: 131.401764\n",
            "Train Epoch: 81 [440/1400 (31%)]\tLoss: 136.802628\n",
            "Train Epoch: 81 [480/1400 (34%)]\tLoss: 136.024918\n",
            "Train Epoch: 81 [520/1400 (37%)]\tLoss: 142.237991\n",
            "Train Epoch: 81 [560/1400 (40%)]\tLoss: 132.632812\n",
            "Train Epoch: 81 [600/1400 (43%)]\tLoss: 128.220856\n",
            "Train Epoch: 81 [640/1400 (46%)]\tLoss: 145.022827\n",
            "Train Epoch: 81 [680/1400 (49%)]\tLoss: 136.195709\n",
            "Train Epoch: 81 [720/1400 (51%)]\tLoss: 139.322906\n",
            "Train Epoch: 81 [760/1400 (54%)]\tLoss: 150.987366\n",
            "Train Epoch: 81 [800/1400 (57%)]\tLoss: 155.725021\n",
            "Train Epoch: 81 [840/1400 (60%)]\tLoss: 134.786942\n",
            "Train Epoch: 81 [880/1400 (63%)]\tLoss: 155.182343\n",
            "Train Epoch: 81 [920/1400 (66%)]\tLoss: 134.906784\n",
            "Train Epoch: 81 [960/1400 (69%)]\tLoss: 130.669342\n",
            "Train Epoch: 81 [1000/1400 (71%)]\tLoss: 136.999680\n",
            "Train Epoch: 81 [1040/1400 (74%)]\tLoss: 132.291534\n",
            "Train Epoch: 81 [1080/1400 (77%)]\tLoss: 129.897308\n",
            "Train Epoch: 81 [1120/1400 (80%)]\tLoss: 151.173294\n",
            "Train Epoch: 81 [1160/1400 (83%)]\tLoss: 149.986374\n",
            "Train Epoch: 81 [1200/1400 (86%)]\tLoss: 163.511520\n",
            "Train Epoch: 81 [1240/1400 (89%)]\tLoss: 122.280998\n",
            "Train Epoch: 81 [1280/1400 (91%)]\tLoss: 142.590057\n",
            "Train Epoch: 81 [1320/1400 (94%)]\tLoss: 149.283951\n",
            "Train Epoch: 81 [1360/1400 (97%)]\tLoss: 141.899307\n",
            "====> Epoch: 81 Average loss: 141.5678\n",
            "====> Test set loss: 145.7896\n",
            "Train Epoch: 82 [0/1400 (0%)]\tLoss: 133.824005\n",
            "Train Epoch: 82 [40/1400 (3%)]\tLoss: 162.832809\n",
            "Train Epoch: 82 [80/1400 (6%)]\tLoss: 137.482529\n",
            "Train Epoch: 82 [120/1400 (9%)]\tLoss: 136.437271\n",
            "Train Epoch: 82 [160/1400 (11%)]\tLoss: 139.018555\n",
            "Train Epoch: 82 [200/1400 (14%)]\tLoss: 142.877167\n",
            "Train Epoch: 82 [240/1400 (17%)]\tLoss: 127.655708\n",
            "Train Epoch: 82 [280/1400 (20%)]\tLoss: 133.876053\n",
            "Train Epoch: 82 [320/1400 (23%)]\tLoss: 168.744568\n",
            "Train Epoch: 82 [360/1400 (26%)]\tLoss: 121.255302\n",
            "Train Epoch: 82 [400/1400 (29%)]\tLoss: 124.215576\n",
            "Train Epoch: 82 [440/1400 (31%)]\tLoss: 131.945679\n",
            "Train Epoch: 82 [480/1400 (34%)]\tLoss: 109.635384\n",
            "Train Epoch: 82 [520/1400 (37%)]\tLoss: 124.350975\n",
            "Train Epoch: 82 [560/1400 (40%)]\tLoss: 145.431671\n",
            "Train Epoch: 82 [600/1400 (43%)]\tLoss: 154.187729\n",
            "Train Epoch: 82 [640/1400 (46%)]\tLoss: 137.112823\n",
            "Train Epoch: 82 [680/1400 (49%)]\tLoss: 160.639618\n",
            "Train Epoch: 82 [720/1400 (51%)]\tLoss: 161.969757\n",
            "Train Epoch: 82 [760/1400 (54%)]\tLoss: 142.074966\n",
            "Train Epoch: 82 [800/1400 (57%)]\tLoss: 143.815369\n",
            "Train Epoch: 82 [840/1400 (60%)]\tLoss: 158.941895\n",
            "Train Epoch: 82 [880/1400 (63%)]\tLoss: 117.936295\n",
            "Train Epoch: 82 [920/1400 (66%)]\tLoss: 140.536972\n",
            "Train Epoch: 82 [960/1400 (69%)]\tLoss: 184.847519\n",
            "Train Epoch: 82 [1000/1400 (71%)]\tLoss: 150.845352\n",
            "Train Epoch: 82 [1040/1400 (74%)]\tLoss: 122.796906\n",
            "Train Epoch: 82 [1080/1400 (77%)]\tLoss: 126.911713\n",
            "Train Epoch: 82 [1120/1400 (80%)]\tLoss: 141.703674\n",
            "Train Epoch: 82 [1160/1400 (83%)]\tLoss: 130.819962\n",
            "Train Epoch: 82 [1200/1400 (86%)]\tLoss: 162.036469\n",
            "Train Epoch: 82 [1240/1400 (89%)]\tLoss: 128.234528\n",
            "Train Epoch: 82 [1280/1400 (91%)]\tLoss: 145.138412\n",
            "Train Epoch: 82 [1320/1400 (94%)]\tLoss: 146.553970\n",
            "Train Epoch: 82 [1360/1400 (97%)]\tLoss: 138.295135\n",
            "====> Epoch: 82 Average loss: 141.5896\n",
            "====> Test set loss: 145.7867\n",
            "Train Epoch: 83 [0/1400 (0%)]\tLoss: 128.918121\n",
            "Train Epoch: 83 [40/1400 (3%)]\tLoss: 151.636444\n",
            "Train Epoch: 83 [80/1400 (6%)]\tLoss: 123.623146\n",
            "Train Epoch: 83 [120/1400 (9%)]\tLoss: 168.557861\n",
            "Train Epoch: 83 [160/1400 (11%)]\tLoss: 140.565613\n",
            "Train Epoch: 83 [200/1400 (14%)]\tLoss: 172.695877\n",
            "Train Epoch: 83 [240/1400 (17%)]\tLoss: 141.142273\n",
            "Train Epoch: 83 [280/1400 (20%)]\tLoss: 113.030533\n",
            "Train Epoch: 83 [320/1400 (23%)]\tLoss: 157.290894\n",
            "Train Epoch: 83 [360/1400 (26%)]\tLoss: 143.536880\n",
            "Train Epoch: 83 [400/1400 (29%)]\tLoss: 118.641052\n",
            "Train Epoch: 83 [440/1400 (31%)]\tLoss: 133.961838\n",
            "Train Epoch: 83 [480/1400 (34%)]\tLoss: 135.659927\n",
            "Train Epoch: 83 [520/1400 (37%)]\tLoss: 146.730331\n",
            "Train Epoch: 83 [560/1400 (40%)]\tLoss: 124.284866\n",
            "Train Epoch: 83 [600/1400 (43%)]\tLoss: 136.275742\n",
            "Train Epoch: 83 [640/1400 (46%)]\tLoss: 152.506989\n",
            "Train Epoch: 83 [680/1400 (49%)]\tLoss: 143.167801\n",
            "Train Epoch: 83 [720/1400 (51%)]\tLoss: 137.126190\n",
            "Train Epoch: 83 [760/1400 (54%)]\tLoss: 165.048981\n",
            "Train Epoch: 83 [800/1400 (57%)]\tLoss: 121.403236\n",
            "Train Epoch: 83 [840/1400 (60%)]\tLoss: 148.745422\n",
            "Train Epoch: 83 [880/1400 (63%)]\tLoss: 144.626495\n",
            "Train Epoch: 83 [920/1400 (66%)]\tLoss: 131.624039\n",
            "Train Epoch: 83 [960/1400 (69%)]\tLoss: 151.741592\n",
            "Train Epoch: 83 [1000/1400 (71%)]\tLoss: 165.039169\n",
            "Train Epoch: 83 [1040/1400 (74%)]\tLoss: 136.802429\n",
            "Train Epoch: 83 [1080/1400 (77%)]\tLoss: 158.117828\n",
            "Train Epoch: 83 [1120/1400 (80%)]\tLoss: 147.311905\n",
            "Train Epoch: 83 [1160/1400 (83%)]\tLoss: 140.211945\n",
            "Train Epoch: 83 [1200/1400 (86%)]\tLoss: 139.226273\n",
            "Train Epoch: 83 [1240/1400 (89%)]\tLoss: 142.364761\n",
            "Train Epoch: 83 [1280/1400 (91%)]\tLoss: 157.724609\n",
            "Train Epoch: 83 [1320/1400 (94%)]\tLoss: 129.700836\n",
            "Train Epoch: 83 [1360/1400 (97%)]\tLoss: 129.792221\n",
            "====> Epoch: 83 Average loss: 141.5567\n",
            "====> Test set loss: 145.8244\n",
            "Train Epoch: 84 [0/1400 (0%)]\tLoss: 132.674484\n",
            "Train Epoch: 84 [40/1400 (3%)]\tLoss: 136.324646\n",
            "Train Epoch: 84 [80/1400 (6%)]\tLoss: 109.936089\n",
            "Train Epoch: 84 [120/1400 (9%)]\tLoss: 145.721100\n",
            "Train Epoch: 84 [160/1400 (11%)]\tLoss: 136.818344\n",
            "Train Epoch: 84 [200/1400 (14%)]\tLoss: 149.584824\n",
            "Train Epoch: 84 [240/1400 (17%)]\tLoss: 129.633148\n",
            "Train Epoch: 84 [280/1400 (20%)]\tLoss: 149.832886\n",
            "Train Epoch: 84 [320/1400 (23%)]\tLoss: 135.215317\n",
            "Train Epoch: 84 [360/1400 (26%)]\tLoss: 142.957047\n",
            "Train Epoch: 84 [400/1400 (29%)]\tLoss: 136.445709\n",
            "Train Epoch: 84 [440/1400 (31%)]\tLoss: 154.575928\n",
            "Train Epoch: 84 [480/1400 (34%)]\tLoss: 144.698730\n",
            "Train Epoch: 84 [520/1400 (37%)]\tLoss: 134.919174\n",
            "Train Epoch: 84 [560/1400 (40%)]\tLoss: 127.890839\n",
            "Train Epoch: 84 [600/1400 (43%)]\tLoss: 147.487762\n",
            "Train Epoch: 84 [640/1400 (46%)]\tLoss: 154.078430\n",
            "Train Epoch: 84 [680/1400 (49%)]\tLoss: 134.566864\n",
            "Train Epoch: 84 [720/1400 (51%)]\tLoss: 172.462723\n",
            "Train Epoch: 84 [760/1400 (54%)]\tLoss: 162.788239\n",
            "Train Epoch: 84 [800/1400 (57%)]\tLoss: 161.232910\n",
            "Train Epoch: 84 [840/1400 (60%)]\tLoss: 128.751160\n",
            "Train Epoch: 84 [880/1400 (63%)]\tLoss: 147.240967\n",
            "Train Epoch: 84 [920/1400 (66%)]\tLoss: 151.992737\n",
            "Train Epoch: 84 [960/1400 (69%)]\tLoss: 124.740784\n",
            "Train Epoch: 84 [1000/1400 (71%)]\tLoss: 117.833481\n",
            "Train Epoch: 84 [1040/1400 (74%)]\tLoss: 144.782593\n",
            "Train Epoch: 84 [1080/1400 (77%)]\tLoss: 138.781281\n",
            "Train Epoch: 84 [1120/1400 (80%)]\tLoss: 136.117661\n",
            "Train Epoch: 84 [1160/1400 (83%)]\tLoss: 129.562531\n",
            "Train Epoch: 84 [1200/1400 (86%)]\tLoss: 137.068161\n",
            "Train Epoch: 84 [1240/1400 (89%)]\tLoss: 138.070587\n",
            "Train Epoch: 84 [1280/1400 (91%)]\tLoss: 148.219391\n",
            "Train Epoch: 84 [1320/1400 (94%)]\tLoss: 142.752731\n",
            "Train Epoch: 84 [1360/1400 (97%)]\tLoss: 145.806442\n",
            "====> Epoch: 84 Average loss: 141.5506\n",
            "====> Test set loss: 145.7798\n",
            "Train Epoch: 85 [0/1400 (0%)]\tLoss: 154.903488\n",
            "Train Epoch: 85 [40/1400 (3%)]\tLoss: 153.242920\n",
            "Train Epoch: 85 [80/1400 (6%)]\tLoss: 132.159546\n",
            "Train Epoch: 85 [120/1400 (9%)]\tLoss: 126.571136\n",
            "Train Epoch: 85 [160/1400 (11%)]\tLoss: 117.968582\n",
            "Train Epoch: 85 [200/1400 (14%)]\tLoss: 139.555634\n",
            "Train Epoch: 85 [240/1400 (17%)]\tLoss: 160.317123\n",
            "Train Epoch: 85 [280/1400 (20%)]\tLoss: 160.570648\n",
            "Train Epoch: 85 [320/1400 (23%)]\tLoss: 148.529388\n",
            "Train Epoch: 85 [360/1400 (26%)]\tLoss: 135.012115\n",
            "Train Epoch: 85 [400/1400 (29%)]\tLoss: 137.667480\n",
            "Train Epoch: 85 [440/1400 (31%)]\tLoss: 143.998291\n",
            "Train Epoch: 85 [480/1400 (34%)]\tLoss: 132.123657\n",
            "Train Epoch: 85 [520/1400 (37%)]\tLoss: 111.207733\n",
            "Train Epoch: 85 [560/1400 (40%)]\tLoss: 157.228882\n",
            "Train Epoch: 85 [600/1400 (43%)]\tLoss: 132.462646\n",
            "Train Epoch: 85 [640/1400 (46%)]\tLoss: 158.381516\n",
            "Train Epoch: 85 [680/1400 (49%)]\tLoss: 112.884613\n",
            "Train Epoch: 85 [720/1400 (51%)]\tLoss: 140.471039\n",
            "Train Epoch: 85 [760/1400 (54%)]\tLoss: 136.067123\n",
            "Train Epoch: 85 [800/1400 (57%)]\tLoss: 154.686829\n",
            "Train Epoch: 85 [840/1400 (60%)]\tLoss: 146.630569\n",
            "Train Epoch: 85 [880/1400 (63%)]\tLoss: 131.723785\n",
            "Train Epoch: 85 [920/1400 (66%)]\tLoss: 158.010208\n",
            "Train Epoch: 85 [960/1400 (69%)]\tLoss: 138.040146\n",
            "Train Epoch: 85 [1000/1400 (71%)]\tLoss: 126.080475\n",
            "Train Epoch: 85 [1040/1400 (74%)]\tLoss: 156.204971\n",
            "Train Epoch: 85 [1080/1400 (77%)]\tLoss: 141.754761\n",
            "Train Epoch: 85 [1120/1400 (80%)]\tLoss: 125.948151\n",
            "Train Epoch: 85 [1160/1400 (83%)]\tLoss: 155.265808\n",
            "Train Epoch: 85 [1200/1400 (86%)]\tLoss: 135.328125\n",
            "Train Epoch: 85 [1240/1400 (89%)]\tLoss: 143.803619\n",
            "Train Epoch: 85 [1280/1400 (91%)]\tLoss: 143.910507\n",
            "Train Epoch: 85 [1320/1400 (94%)]\tLoss: 148.245209\n",
            "Train Epoch: 85 [1360/1400 (97%)]\tLoss: 139.642517\n",
            "====> Epoch: 85 Average loss: 141.5522\n",
            "====> Test set loss: 145.7101\n",
            "Train Epoch: 86 [0/1400 (0%)]\tLoss: 133.157318\n",
            "Train Epoch: 86 [40/1400 (3%)]\tLoss: 175.348175\n",
            "Train Epoch: 86 [80/1400 (6%)]\tLoss: 118.300171\n",
            "Train Epoch: 86 [120/1400 (9%)]\tLoss: 143.461853\n",
            "Train Epoch: 86 [160/1400 (11%)]\tLoss: 135.924240\n",
            "Train Epoch: 86 [200/1400 (14%)]\tLoss: 167.199326\n",
            "Train Epoch: 86 [240/1400 (17%)]\tLoss: 168.187195\n",
            "Train Epoch: 86 [280/1400 (20%)]\tLoss: 151.678314\n",
            "Train Epoch: 86 [320/1400 (23%)]\tLoss: 158.746979\n",
            "Train Epoch: 86 [360/1400 (26%)]\tLoss: 150.153320\n",
            "Train Epoch: 86 [400/1400 (29%)]\tLoss: 139.527817\n",
            "Train Epoch: 86 [440/1400 (31%)]\tLoss: 164.209320\n",
            "Train Epoch: 86 [480/1400 (34%)]\tLoss: 144.044617\n",
            "Train Epoch: 86 [520/1400 (37%)]\tLoss: 147.081070\n",
            "Train Epoch: 86 [560/1400 (40%)]\tLoss: 151.752914\n",
            "Train Epoch: 86 [600/1400 (43%)]\tLoss: 139.820297\n",
            "Train Epoch: 86 [640/1400 (46%)]\tLoss: 142.190948\n",
            "Train Epoch: 86 [680/1400 (49%)]\tLoss: 148.302124\n",
            "Train Epoch: 86 [720/1400 (51%)]\tLoss: 141.690659\n",
            "Train Epoch: 86 [760/1400 (54%)]\tLoss: 132.516342\n",
            "Train Epoch: 86 [800/1400 (57%)]\tLoss: 142.656906\n",
            "Train Epoch: 86 [840/1400 (60%)]\tLoss: 152.724136\n",
            "Train Epoch: 86 [880/1400 (63%)]\tLoss: 123.997940\n",
            "Train Epoch: 86 [920/1400 (66%)]\tLoss: 146.571350\n",
            "Train Epoch: 86 [960/1400 (69%)]\tLoss: 148.774002\n",
            "Train Epoch: 86 [1000/1400 (71%)]\tLoss: 144.682053\n",
            "Train Epoch: 86 [1040/1400 (74%)]\tLoss: 131.167847\n",
            "Train Epoch: 86 [1080/1400 (77%)]\tLoss: 154.825958\n",
            "Train Epoch: 86 [1120/1400 (80%)]\tLoss: 145.240356\n",
            "Train Epoch: 86 [1160/1400 (83%)]\tLoss: 114.551529\n",
            "Train Epoch: 86 [1200/1400 (86%)]\tLoss: 153.350693\n",
            "Train Epoch: 86 [1240/1400 (89%)]\tLoss: 140.184998\n",
            "Train Epoch: 86 [1280/1400 (91%)]\tLoss: 131.083923\n",
            "Train Epoch: 86 [1320/1400 (94%)]\tLoss: 159.351135\n",
            "Train Epoch: 86 [1360/1400 (97%)]\tLoss: 143.164993\n",
            "====> Epoch: 86 Average loss: 141.5738\n",
            "====> Test set loss: 145.7015\n",
            "Train Epoch: 87 [0/1400 (0%)]\tLoss: 140.474442\n",
            "Train Epoch: 87 [40/1400 (3%)]\tLoss: 157.901749\n",
            "Train Epoch: 87 [80/1400 (6%)]\tLoss: 156.461777\n",
            "Train Epoch: 87 [120/1400 (9%)]\tLoss: 149.955780\n",
            "Train Epoch: 87 [160/1400 (11%)]\tLoss: 138.213379\n",
            "Train Epoch: 87 [200/1400 (14%)]\tLoss: 145.858932\n",
            "Train Epoch: 87 [240/1400 (17%)]\tLoss: 141.689392\n",
            "Train Epoch: 87 [280/1400 (20%)]\tLoss: 115.752792\n",
            "Train Epoch: 87 [320/1400 (23%)]\tLoss: 144.834213\n",
            "Train Epoch: 87 [360/1400 (26%)]\tLoss: 135.694962\n",
            "Train Epoch: 87 [400/1400 (29%)]\tLoss: 136.445145\n",
            "Train Epoch: 87 [440/1400 (31%)]\tLoss: 137.110291\n",
            "Train Epoch: 87 [480/1400 (34%)]\tLoss: 142.820892\n",
            "Train Epoch: 87 [520/1400 (37%)]\tLoss: 135.941681\n",
            "Train Epoch: 87 [560/1400 (40%)]\tLoss: 152.690338\n",
            "Train Epoch: 87 [600/1400 (43%)]\tLoss: 119.764740\n",
            "Train Epoch: 87 [640/1400 (46%)]\tLoss: 139.598938\n",
            "Train Epoch: 87 [680/1400 (49%)]\tLoss: 148.436859\n",
            "Train Epoch: 87 [720/1400 (51%)]\tLoss: 140.135635\n",
            "Train Epoch: 87 [760/1400 (54%)]\tLoss: 127.447388\n",
            "Train Epoch: 87 [800/1400 (57%)]\tLoss: 130.066956\n",
            "Train Epoch: 87 [840/1400 (60%)]\tLoss: 136.849609\n",
            "Train Epoch: 87 [880/1400 (63%)]\tLoss: 165.397202\n",
            "Train Epoch: 87 [920/1400 (66%)]\tLoss: 136.824554\n",
            "Train Epoch: 87 [960/1400 (69%)]\tLoss: 148.090073\n",
            "Train Epoch: 87 [1000/1400 (71%)]\tLoss: 147.752335\n",
            "Train Epoch: 87 [1040/1400 (74%)]\tLoss: 137.136505\n",
            "Train Epoch: 87 [1080/1400 (77%)]\tLoss: 164.293839\n",
            "Train Epoch: 87 [1120/1400 (80%)]\tLoss: 142.604355\n",
            "Train Epoch: 87 [1160/1400 (83%)]\tLoss: 153.525986\n",
            "Train Epoch: 87 [1200/1400 (86%)]\tLoss: 116.117485\n",
            "Train Epoch: 87 [1240/1400 (89%)]\tLoss: 156.131195\n",
            "Train Epoch: 87 [1280/1400 (91%)]\tLoss: 150.721985\n",
            "Train Epoch: 87 [1320/1400 (94%)]\tLoss: 151.805328\n",
            "Train Epoch: 87 [1360/1400 (97%)]\tLoss: 148.722305\n",
            "====> Epoch: 87 Average loss: 141.5476\n",
            "====> Test set loss: 145.7684\n",
            "Train Epoch: 88 [0/1400 (0%)]\tLoss: 127.487381\n",
            "Train Epoch: 88 [40/1400 (3%)]\tLoss: 140.101395\n",
            "Train Epoch: 88 [80/1400 (6%)]\tLoss: 127.882957\n",
            "Train Epoch: 88 [120/1400 (9%)]\tLoss: 144.742996\n",
            "Train Epoch: 88 [160/1400 (11%)]\tLoss: 147.894394\n",
            "Train Epoch: 88 [200/1400 (14%)]\tLoss: 163.641602\n",
            "Train Epoch: 88 [240/1400 (17%)]\tLoss: 134.837830\n",
            "Train Epoch: 88 [280/1400 (20%)]\tLoss: 131.301697\n",
            "Train Epoch: 88 [320/1400 (23%)]\tLoss: 145.591385\n",
            "Train Epoch: 88 [360/1400 (26%)]\tLoss: 150.709763\n",
            "Train Epoch: 88 [400/1400 (29%)]\tLoss: 151.682739\n",
            "Train Epoch: 88 [440/1400 (31%)]\tLoss: 139.798691\n",
            "Train Epoch: 88 [480/1400 (34%)]\tLoss: 154.035858\n",
            "Train Epoch: 88 [520/1400 (37%)]\tLoss: 156.695297\n",
            "Train Epoch: 88 [560/1400 (40%)]\tLoss: 132.162415\n",
            "Train Epoch: 88 [600/1400 (43%)]\tLoss: 126.357727\n",
            "Train Epoch: 88 [640/1400 (46%)]\tLoss: 146.689377\n",
            "Train Epoch: 88 [680/1400 (49%)]\tLoss: 158.077194\n",
            "Train Epoch: 88 [720/1400 (51%)]\tLoss: 149.359253\n",
            "Train Epoch: 88 [760/1400 (54%)]\tLoss: 121.300339\n",
            "Train Epoch: 88 [800/1400 (57%)]\tLoss: 130.665771\n",
            "Train Epoch: 88 [840/1400 (60%)]\tLoss: 134.384491\n",
            "Train Epoch: 88 [880/1400 (63%)]\tLoss: 147.939178\n",
            "Train Epoch: 88 [920/1400 (66%)]\tLoss: 120.919884\n",
            "Train Epoch: 88 [960/1400 (69%)]\tLoss: 162.543243\n",
            "Train Epoch: 88 [1000/1400 (71%)]\tLoss: 144.176071\n",
            "Train Epoch: 88 [1040/1400 (74%)]\tLoss: 135.422775\n",
            "Train Epoch: 88 [1080/1400 (77%)]\tLoss: 142.116592\n",
            "Train Epoch: 88 [1120/1400 (80%)]\tLoss: 119.031151\n",
            "Train Epoch: 88 [1160/1400 (83%)]\tLoss: 139.092102\n",
            "Train Epoch: 88 [1200/1400 (86%)]\tLoss: 133.702866\n",
            "Train Epoch: 88 [1240/1400 (89%)]\tLoss: 135.524109\n",
            "Train Epoch: 88 [1280/1400 (91%)]\tLoss: 142.256836\n",
            "Train Epoch: 88 [1320/1400 (94%)]\tLoss: 141.206299\n",
            "Train Epoch: 88 [1360/1400 (97%)]\tLoss: 137.475830\n",
            "====> Epoch: 88 Average loss: 141.5099\n",
            "====> Test set loss: 145.8560\n",
            "Train Epoch: 89 [0/1400 (0%)]\tLoss: 138.662170\n",
            "Train Epoch: 89 [40/1400 (3%)]\tLoss: 137.457321\n",
            "Train Epoch: 89 [80/1400 (6%)]\tLoss: 142.613159\n",
            "Train Epoch: 89 [120/1400 (9%)]\tLoss: 151.292633\n",
            "Train Epoch: 89 [160/1400 (11%)]\tLoss: 125.746941\n",
            "Train Epoch: 89 [200/1400 (14%)]\tLoss: 156.359589\n",
            "Train Epoch: 89 [240/1400 (17%)]\tLoss: 151.413391\n",
            "Train Epoch: 89 [280/1400 (20%)]\tLoss: 122.550606\n",
            "Train Epoch: 89 [320/1400 (23%)]\tLoss: 133.522949\n",
            "Train Epoch: 89 [360/1400 (26%)]\tLoss: 149.925812\n",
            "Train Epoch: 89 [400/1400 (29%)]\tLoss: 147.275070\n",
            "Train Epoch: 89 [440/1400 (31%)]\tLoss: 148.653214\n",
            "Train Epoch: 89 [480/1400 (34%)]\tLoss: 175.206650\n",
            "Train Epoch: 89 [520/1400 (37%)]\tLoss: 127.486710\n",
            "Train Epoch: 89 [560/1400 (40%)]\tLoss: 152.072311\n",
            "Train Epoch: 89 [600/1400 (43%)]\tLoss: 161.333572\n",
            "Train Epoch: 89 [640/1400 (46%)]\tLoss: 139.088318\n",
            "Train Epoch: 89 [680/1400 (49%)]\tLoss: 121.524323\n",
            "Train Epoch: 89 [720/1400 (51%)]\tLoss: 143.951340\n",
            "Train Epoch: 89 [760/1400 (54%)]\tLoss: 127.770096\n",
            "Train Epoch: 89 [800/1400 (57%)]\tLoss: 120.799599\n",
            "Train Epoch: 89 [840/1400 (60%)]\tLoss: 134.927460\n",
            "Train Epoch: 89 [880/1400 (63%)]\tLoss: 154.125534\n",
            "Train Epoch: 89 [920/1400 (66%)]\tLoss: 131.030914\n",
            "Train Epoch: 89 [960/1400 (69%)]\tLoss: 133.491409\n",
            "Train Epoch: 89 [1000/1400 (71%)]\tLoss: 147.809128\n",
            "Train Epoch: 89 [1040/1400 (74%)]\tLoss: 161.927353\n",
            "Train Epoch: 89 [1080/1400 (77%)]\tLoss: 139.154633\n",
            "Train Epoch: 89 [1120/1400 (80%)]\tLoss: 132.589920\n",
            "Train Epoch: 89 [1160/1400 (83%)]\tLoss: 125.224152\n",
            "Train Epoch: 89 [1200/1400 (86%)]\tLoss: 127.704277\n",
            "Train Epoch: 89 [1240/1400 (89%)]\tLoss: 116.172668\n",
            "Train Epoch: 89 [1280/1400 (91%)]\tLoss: 135.772751\n",
            "Train Epoch: 89 [1320/1400 (94%)]\tLoss: 139.126480\n",
            "Train Epoch: 89 [1360/1400 (97%)]\tLoss: 145.082458\n",
            "====> Epoch: 89 Average loss: 141.5557\n",
            "====> Test set loss: 145.7798\n",
            "Train Epoch: 90 [0/1400 (0%)]\tLoss: 149.745743\n",
            "Train Epoch: 90 [40/1400 (3%)]\tLoss: 142.213608\n",
            "Train Epoch: 90 [80/1400 (6%)]\tLoss: 129.903503\n",
            "Train Epoch: 90 [120/1400 (9%)]\tLoss: 145.581635\n",
            "Train Epoch: 90 [160/1400 (11%)]\tLoss: 142.366943\n",
            "Train Epoch: 90 [200/1400 (14%)]\tLoss: 128.832870\n",
            "Train Epoch: 90 [240/1400 (17%)]\tLoss: 115.452309\n",
            "Train Epoch: 90 [280/1400 (20%)]\tLoss: 129.252121\n",
            "Train Epoch: 90 [320/1400 (23%)]\tLoss: 161.662750\n",
            "Train Epoch: 90 [360/1400 (26%)]\tLoss: 148.459488\n",
            "Train Epoch: 90 [400/1400 (29%)]\tLoss: 153.255997\n",
            "Train Epoch: 90 [440/1400 (31%)]\tLoss: 146.093521\n",
            "Train Epoch: 90 [480/1400 (34%)]\tLoss: 129.455063\n",
            "Train Epoch: 90 [520/1400 (37%)]\tLoss: 166.623474\n",
            "Train Epoch: 90 [560/1400 (40%)]\tLoss: 140.990891\n",
            "Train Epoch: 90 [600/1400 (43%)]\tLoss: 159.869827\n",
            "Train Epoch: 90 [640/1400 (46%)]\tLoss: 150.607544\n",
            "Train Epoch: 90 [680/1400 (49%)]\tLoss: 149.293777\n",
            "Train Epoch: 90 [720/1400 (51%)]\tLoss: 146.226517\n",
            "Train Epoch: 90 [760/1400 (54%)]\tLoss: 146.329483\n",
            "Train Epoch: 90 [800/1400 (57%)]\tLoss: 143.677505\n",
            "Train Epoch: 90 [840/1400 (60%)]\tLoss: 121.513657\n",
            "Train Epoch: 90 [880/1400 (63%)]\tLoss: 129.539185\n",
            "Train Epoch: 90 [920/1400 (66%)]\tLoss: 119.273735\n",
            "Train Epoch: 90 [960/1400 (69%)]\tLoss: 138.919922\n",
            "Train Epoch: 90 [1000/1400 (71%)]\tLoss: 119.139030\n",
            "Train Epoch: 90 [1040/1400 (74%)]\tLoss: 156.668411\n",
            "Train Epoch: 90 [1080/1400 (77%)]\tLoss: 135.014725\n",
            "Train Epoch: 90 [1120/1400 (80%)]\tLoss: 137.521774\n",
            "Train Epoch: 90 [1160/1400 (83%)]\tLoss: 131.949875\n",
            "Train Epoch: 90 [1200/1400 (86%)]\tLoss: 153.355530\n",
            "Train Epoch: 90 [1240/1400 (89%)]\tLoss: 132.257355\n",
            "Train Epoch: 90 [1280/1400 (91%)]\tLoss: 136.687927\n",
            "Train Epoch: 90 [1320/1400 (94%)]\tLoss: 146.614090\n",
            "Train Epoch: 90 [1360/1400 (97%)]\tLoss: 147.243668\n",
            "====> Epoch: 90 Average loss: 141.5622\n",
            "====> Test set loss: 145.7252\n",
            "Train Epoch: 91 [0/1400 (0%)]\tLoss: 143.768372\n",
            "Train Epoch: 91 [40/1400 (3%)]\tLoss: 142.121017\n",
            "Train Epoch: 91 [80/1400 (6%)]\tLoss: 132.313980\n",
            "Train Epoch: 91 [120/1400 (9%)]\tLoss: 170.332382\n",
            "Train Epoch: 91 [160/1400 (11%)]\tLoss: 130.572205\n",
            "Train Epoch: 91 [200/1400 (14%)]\tLoss: 143.855881\n",
            "Train Epoch: 91 [240/1400 (17%)]\tLoss: 147.107300\n",
            "Train Epoch: 91 [280/1400 (20%)]\tLoss: 119.320877\n",
            "Train Epoch: 91 [320/1400 (23%)]\tLoss: 138.037140\n",
            "Train Epoch: 91 [360/1400 (26%)]\tLoss: 153.874420\n",
            "Train Epoch: 91 [400/1400 (29%)]\tLoss: 172.007874\n",
            "Train Epoch: 91 [440/1400 (31%)]\tLoss: 138.822403\n",
            "Train Epoch: 91 [480/1400 (34%)]\tLoss: 138.416336\n",
            "Train Epoch: 91 [520/1400 (37%)]\tLoss: 140.770874\n",
            "Train Epoch: 91 [560/1400 (40%)]\tLoss: 143.258026\n",
            "Train Epoch: 91 [600/1400 (43%)]\tLoss: 160.432236\n",
            "Train Epoch: 91 [640/1400 (46%)]\tLoss: 138.915039\n",
            "Train Epoch: 91 [680/1400 (49%)]\tLoss: 133.713058\n",
            "Train Epoch: 91 [720/1400 (51%)]\tLoss: 145.297821\n",
            "Train Epoch: 91 [760/1400 (54%)]\tLoss: 130.438980\n",
            "Train Epoch: 91 [800/1400 (57%)]\tLoss: 139.890366\n",
            "Train Epoch: 91 [840/1400 (60%)]\tLoss: 133.085449\n",
            "Train Epoch: 91 [880/1400 (63%)]\tLoss: 143.667206\n",
            "Train Epoch: 91 [920/1400 (66%)]\tLoss: 135.303665\n",
            "Train Epoch: 91 [960/1400 (69%)]\tLoss: 134.868622\n",
            "Train Epoch: 91 [1000/1400 (71%)]\tLoss: 134.864914\n",
            "Train Epoch: 91 [1040/1400 (74%)]\tLoss: 172.456726\n",
            "Train Epoch: 91 [1080/1400 (77%)]\tLoss: 127.148819\n",
            "Train Epoch: 91 [1120/1400 (80%)]\tLoss: 140.791367\n",
            "Train Epoch: 91 [1160/1400 (83%)]\tLoss: 127.401314\n",
            "Train Epoch: 91 [1200/1400 (86%)]\tLoss: 136.534180\n",
            "Train Epoch: 91 [1240/1400 (89%)]\tLoss: 122.242393\n",
            "Train Epoch: 91 [1280/1400 (91%)]\tLoss: 127.669739\n",
            "Train Epoch: 91 [1320/1400 (94%)]\tLoss: 130.683533\n",
            "Train Epoch: 91 [1360/1400 (97%)]\tLoss: 165.254669\n",
            "====> Epoch: 91 Average loss: 141.5338\n",
            "====> Test set loss: 145.6147\n",
            "Train Epoch: 92 [0/1400 (0%)]\tLoss: 141.457458\n",
            "Train Epoch: 92 [40/1400 (3%)]\tLoss: 136.222702\n",
            "Train Epoch: 92 [80/1400 (6%)]\tLoss: 146.755722\n",
            "Train Epoch: 92 [120/1400 (9%)]\tLoss: 150.254547\n",
            "Train Epoch: 92 [160/1400 (11%)]\tLoss: 153.515579\n",
            "Train Epoch: 92 [200/1400 (14%)]\tLoss: 148.198944\n",
            "Train Epoch: 92 [240/1400 (17%)]\tLoss: 136.777954\n",
            "Train Epoch: 92 [280/1400 (20%)]\tLoss: 131.665634\n",
            "Train Epoch: 92 [320/1400 (23%)]\tLoss: 134.624451\n",
            "Train Epoch: 92 [360/1400 (26%)]\tLoss: 136.669708\n",
            "Train Epoch: 92 [400/1400 (29%)]\tLoss: 125.309883\n",
            "Train Epoch: 92 [440/1400 (31%)]\tLoss: 153.854950\n",
            "Train Epoch: 92 [480/1400 (34%)]\tLoss: 145.084641\n",
            "Train Epoch: 92 [520/1400 (37%)]\tLoss: 148.519653\n",
            "Train Epoch: 92 [560/1400 (40%)]\tLoss: 172.025513\n",
            "Train Epoch: 92 [600/1400 (43%)]\tLoss: 142.081314\n",
            "Train Epoch: 92 [640/1400 (46%)]\tLoss: 128.770065\n",
            "Train Epoch: 92 [680/1400 (49%)]\tLoss: 125.573776\n",
            "Train Epoch: 92 [720/1400 (51%)]\tLoss: 153.920029\n",
            "Train Epoch: 92 [760/1400 (54%)]\tLoss: 144.552124\n",
            "Train Epoch: 92 [800/1400 (57%)]\tLoss: 121.578392\n",
            "Train Epoch: 92 [840/1400 (60%)]\tLoss: 171.079254\n",
            "Train Epoch: 92 [880/1400 (63%)]\tLoss: 127.898743\n",
            "Train Epoch: 92 [920/1400 (66%)]\tLoss: 137.576874\n",
            "Train Epoch: 92 [960/1400 (69%)]\tLoss: 162.809189\n",
            "Train Epoch: 92 [1000/1400 (71%)]\tLoss: 144.557632\n",
            "Train Epoch: 92 [1040/1400 (74%)]\tLoss: 134.003693\n",
            "Train Epoch: 92 [1080/1400 (77%)]\tLoss: 126.904198\n",
            "Train Epoch: 92 [1120/1400 (80%)]\tLoss: 146.673691\n",
            "Train Epoch: 92 [1160/1400 (83%)]\tLoss: 128.331375\n",
            "Train Epoch: 92 [1200/1400 (86%)]\tLoss: 137.407211\n",
            "Train Epoch: 92 [1240/1400 (89%)]\tLoss: 140.103317\n",
            "Train Epoch: 92 [1280/1400 (91%)]\tLoss: 137.230347\n",
            "Train Epoch: 92 [1320/1400 (94%)]\tLoss: 146.973846\n",
            "Train Epoch: 92 [1360/1400 (97%)]\tLoss: 121.750450\n",
            "====> Epoch: 92 Average loss: 141.5150\n",
            "====> Test set loss: 145.8481\n",
            "Train Epoch: 93 [0/1400 (0%)]\tLoss: 143.984985\n",
            "Train Epoch: 93 [40/1400 (3%)]\tLoss: 147.253052\n",
            "Train Epoch: 93 [80/1400 (6%)]\tLoss: 126.361214\n",
            "Train Epoch: 93 [120/1400 (9%)]\tLoss: 133.685318\n",
            "Train Epoch: 93 [160/1400 (11%)]\tLoss: 132.951080\n",
            "Train Epoch: 93 [200/1400 (14%)]\tLoss: 130.679092\n",
            "Train Epoch: 93 [240/1400 (17%)]\tLoss: 156.111740\n",
            "Train Epoch: 93 [280/1400 (20%)]\tLoss: 156.711655\n",
            "Train Epoch: 93 [320/1400 (23%)]\tLoss: 140.949844\n",
            "Train Epoch: 93 [360/1400 (26%)]\tLoss: 142.140579\n",
            "Train Epoch: 93 [400/1400 (29%)]\tLoss: 127.076744\n",
            "Train Epoch: 93 [440/1400 (31%)]\tLoss: 131.463211\n",
            "Train Epoch: 93 [480/1400 (34%)]\tLoss: 124.601974\n",
            "Train Epoch: 93 [520/1400 (37%)]\tLoss: 137.457458\n",
            "Train Epoch: 93 [560/1400 (40%)]\tLoss: 149.102386\n",
            "Train Epoch: 93 [600/1400 (43%)]\tLoss: 180.645111\n",
            "Train Epoch: 93 [640/1400 (46%)]\tLoss: 129.200760\n",
            "Train Epoch: 93 [680/1400 (49%)]\tLoss: 138.071152\n",
            "Train Epoch: 93 [720/1400 (51%)]\tLoss: 165.401932\n",
            "Train Epoch: 93 [760/1400 (54%)]\tLoss: 144.993835\n",
            "Train Epoch: 93 [800/1400 (57%)]\tLoss: 138.007690\n",
            "Train Epoch: 93 [840/1400 (60%)]\tLoss: 135.739151\n",
            "Train Epoch: 93 [880/1400 (63%)]\tLoss: 133.074600\n",
            "Train Epoch: 93 [920/1400 (66%)]\tLoss: 152.850067\n",
            "Train Epoch: 93 [960/1400 (69%)]\tLoss: 147.873611\n",
            "Train Epoch: 93 [1000/1400 (71%)]\tLoss: 136.995132\n",
            "Train Epoch: 93 [1040/1400 (74%)]\tLoss: 156.474121\n",
            "Train Epoch: 93 [1080/1400 (77%)]\tLoss: 150.712677\n",
            "Train Epoch: 93 [1120/1400 (80%)]\tLoss: 160.218933\n",
            "Train Epoch: 93 [1160/1400 (83%)]\tLoss: 148.097092\n",
            "Train Epoch: 93 [1200/1400 (86%)]\tLoss: 162.297195\n",
            "Train Epoch: 93 [1240/1400 (89%)]\tLoss: 162.157730\n",
            "Train Epoch: 93 [1280/1400 (91%)]\tLoss: 144.067795\n",
            "Train Epoch: 93 [1320/1400 (94%)]\tLoss: 129.882919\n",
            "Train Epoch: 93 [1360/1400 (97%)]\tLoss: 122.971916\n",
            "====> Epoch: 93 Average loss: 141.5295\n",
            "====> Test set loss: 145.7696\n",
            "Train Epoch: 94 [0/1400 (0%)]\tLoss: 149.172256\n",
            "Train Epoch: 94 [40/1400 (3%)]\tLoss: 142.016388\n",
            "Train Epoch: 94 [80/1400 (6%)]\tLoss: 135.031509\n",
            "Train Epoch: 94 [120/1400 (9%)]\tLoss: 154.746628\n",
            "Train Epoch: 94 [160/1400 (11%)]\tLoss: 132.713394\n",
            "Train Epoch: 94 [200/1400 (14%)]\tLoss: 153.768875\n",
            "Train Epoch: 94 [240/1400 (17%)]\tLoss: 132.996109\n",
            "Train Epoch: 94 [280/1400 (20%)]\tLoss: 140.781570\n",
            "Train Epoch: 94 [320/1400 (23%)]\tLoss: 156.221237\n",
            "Train Epoch: 94 [360/1400 (26%)]\tLoss: 145.660980\n",
            "Train Epoch: 94 [400/1400 (29%)]\tLoss: 150.627182\n",
            "Train Epoch: 94 [440/1400 (31%)]\tLoss: 135.772919\n",
            "Train Epoch: 94 [480/1400 (34%)]\tLoss: 138.542007\n",
            "Train Epoch: 94 [520/1400 (37%)]\tLoss: 138.545593\n",
            "Train Epoch: 94 [560/1400 (40%)]\tLoss: 130.805939\n",
            "Train Epoch: 94 [600/1400 (43%)]\tLoss: 145.300842\n",
            "Train Epoch: 94 [640/1400 (46%)]\tLoss: 160.221115\n",
            "Train Epoch: 94 [680/1400 (49%)]\tLoss: 136.301666\n",
            "Train Epoch: 94 [720/1400 (51%)]\tLoss: 139.805023\n",
            "Train Epoch: 94 [760/1400 (54%)]\tLoss: 160.830460\n",
            "Train Epoch: 94 [800/1400 (57%)]\tLoss: 114.690987\n",
            "Train Epoch: 94 [840/1400 (60%)]\tLoss: 150.285889\n",
            "Train Epoch: 94 [880/1400 (63%)]\tLoss: 134.729965\n",
            "Train Epoch: 94 [920/1400 (66%)]\tLoss: 146.989502\n",
            "Train Epoch: 94 [960/1400 (69%)]\tLoss: 129.958603\n",
            "Train Epoch: 94 [1000/1400 (71%)]\tLoss: 125.299309\n",
            "Train Epoch: 94 [1040/1400 (74%)]\tLoss: 140.300873\n",
            "Train Epoch: 94 [1080/1400 (77%)]\tLoss: 135.868759\n",
            "Train Epoch: 94 [1120/1400 (80%)]\tLoss: 160.927658\n",
            "Train Epoch: 94 [1160/1400 (83%)]\tLoss: 143.691650\n",
            "Train Epoch: 94 [1200/1400 (86%)]\tLoss: 145.366333\n",
            "Train Epoch: 94 [1240/1400 (89%)]\tLoss: 143.593643\n",
            "Train Epoch: 94 [1280/1400 (91%)]\tLoss: 143.767715\n",
            "Train Epoch: 94 [1320/1400 (94%)]\tLoss: 142.312515\n",
            "Train Epoch: 94 [1360/1400 (97%)]\tLoss: 124.116241\n",
            "====> Epoch: 94 Average loss: 141.5315\n",
            "====> Test set loss: 145.8240\n",
            "Train Epoch: 95 [0/1400 (0%)]\tLoss: 143.970779\n",
            "Train Epoch: 95 [40/1400 (3%)]\tLoss: 146.125290\n",
            "Train Epoch: 95 [80/1400 (6%)]\tLoss: 133.607407\n",
            "Train Epoch: 95 [120/1400 (9%)]\tLoss: 151.436722\n",
            "Train Epoch: 95 [160/1400 (11%)]\tLoss: 115.975189\n",
            "Train Epoch: 95 [200/1400 (14%)]\tLoss: 149.918808\n",
            "Train Epoch: 95 [240/1400 (17%)]\tLoss: 126.703972\n",
            "Train Epoch: 95 [280/1400 (20%)]\tLoss: 165.142242\n",
            "Train Epoch: 95 [320/1400 (23%)]\tLoss: 136.249329\n",
            "Train Epoch: 95 [360/1400 (26%)]\tLoss: 173.497070\n",
            "Train Epoch: 95 [400/1400 (29%)]\tLoss: 130.020294\n",
            "Train Epoch: 95 [440/1400 (31%)]\tLoss: 134.413132\n",
            "Train Epoch: 95 [480/1400 (34%)]\tLoss: 134.369049\n",
            "Train Epoch: 95 [520/1400 (37%)]\tLoss: 145.132309\n",
            "Train Epoch: 95 [560/1400 (40%)]\tLoss: 137.432999\n",
            "Train Epoch: 95 [600/1400 (43%)]\tLoss: 152.956741\n",
            "Train Epoch: 95 [640/1400 (46%)]\tLoss: 137.117416\n",
            "Train Epoch: 95 [680/1400 (49%)]\tLoss: 121.409607\n",
            "Train Epoch: 95 [720/1400 (51%)]\tLoss: 135.597290\n",
            "Train Epoch: 95 [760/1400 (54%)]\tLoss: 148.385773\n",
            "Train Epoch: 95 [800/1400 (57%)]\tLoss: 148.090027\n",
            "Train Epoch: 95 [840/1400 (60%)]\tLoss: 130.253082\n",
            "Train Epoch: 95 [880/1400 (63%)]\tLoss: 144.349442\n",
            "Train Epoch: 95 [920/1400 (66%)]\tLoss: 142.152847\n",
            "Train Epoch: 95 [960/1400 (69%)]\tLoss: 149.261002\n",
            "Train Epoch: 95 [1000/1400 (71%)]\tLoss: 155.570038\n",
            "Train Epoch: 95 [1040/1400 (74%)]\tLoss: 141.535858\n",
            "Train Epoch: 95 [1080/1400 (77%)]\tLoss: 155.239609\n",
            "Train Epoch: 95 [1120/1400 (80%)]\tLoss: 162.627686\n",
            "Train Epoch: 95 [1160/1400 (83%)]\tLoss: 146.397476\n",
            "Train Epoch: 95 [1200/1400 (86%)]\tLoss: 137.090897\n",
            "Train Epoch: 95 [1240/1400 (89%)]\tLoss: 133.365280\n",
            "Train Epoch: 95 [1280/1400 (91%)]\tLoss: 136.306549\n",
            "Train Epoch: 95 [1320/1400 (94%)]\tLoss: 120.453682\n",
            "Train Epoch: 95 [1360/1400 (97%)]\tLoss: 113.781151\n",
            "====> Epoch: 95 Average loss: 141.5510\n",
            "====> Test set loss: 145.8117\n",
            "Train Epoch: 96 [0/1400 (0%)]\tLoss: 162.088730\n",
            "Train Epoch: 96 [40/1400 (3%)]\tLoss: 126.012268\n",
            "Train Epoch: 96 [80/1400 (6%)]\tLoss: 130.796783\n",
            "Train Epoch: 96 [120/1400 (9%)]\tLoss: 151.265808\n",
            "Train Epoch: 96 [160/1400 (11%)]\tLoss: 143.045959\n",
            "Train Epoch: 96 [200/1400 (14%)]\tLoss: 135.716187\n",
            "Train Epoch: 96 [240/1400 (17%)]\tLoss: 159.038116\n",
            "Train Epoch: 96 [280/1400 (20%)]\tLoss: 148.870621\n",
            "Train Epoch: 96 [320/1400 (23%)]\tLoss: 158.854126\n",
            "Train Epoch: 96 [360/1400 (26%)]\tLoss: 172.925003\n",
            "Train Epoch: 96 [400/1400 (29%)]\tLoss: 128.198975\n",
            "Train Epoch: 96 [440/1400 (31%)]\tLoss: 127.813377\n",
            "Train Epoch: 96 [480/1400 (34%)]\tLoss: 146.096649\n",
            "Train Epoch: 96 [520/1400 (37%)]\tLoss: 121.972847\n",
            "Train Epoch: 96 [560/1400 (40%)]\tLoss: 153.488754\n",
            "Train Epoch: 96 [600/1400 (43%)]\tLoss: 129.213837\n",
            "Train Epoch: 96 [640/1400 (46%)]\tLoss: 139.167648\n",
            "Train Epoch: 96 [680/1400 (49%)]\tLoss: 137.427597\n",
            "Train Epoch: 96 [720/1400 (51%)]\tLoss: 143.180710\n",
            "Train Epoch: 96 [760/1400 (54%)]\tLoss: 131.641785\n",
            "Train Epoch: 96 [800/1400 (57%)]\tLoss: 161.235199\n",
            "Train Epoch: 96 [840/1400 (60%)]\tLoss: 148.016541\n",
            "Train Epoch: 96 [880/1400 (63%)]\tLoss: 141.081360\n",
            "Train Epoch: 96 [920/1400 (66%)]\tLoss: 139.097748\n",
            "Train Epoch: 96 [960/1400 (69%)]\tLoss: 133.230118\n",
            "Train Epoch: 96 [1000/1400 (71%)]\tLoss: 138.529800\n",
            "Train Epoch: 96 [1040/1400 (74%)]\tLoss: 114.322678\n",
            "Train Epoch: 96 [1080/1400 (77%)]\tLoss: 126.919647\n",
            "Train Epoch: 96 [1120/1400 (80%)]\tLoss: 145.946930\n",
            "Train Epoch: 96 [1160/1400 (83%)]\tLoss: 144.980026\n",
            "Train Epoch: 96 [1200/1400 (86%)]\tLoss: 138.837372\n",
            "Train Epoch: 96 [1240/1400 (89%)]\tLoss: 140.747406\n",
            "Train Epoch: 96 [1280/1400 (91%)]\tLoss: 137.666901\n",
            "Train Epoch: 96 [1320/1400 (94%)]\tLoss: 137.096207\n",
            "Train Epoch: 96 [1360/1400 (97%)]\tLoss: 139.400757\n",
            "====> Epoch: 96 Average loss: 141.4891\n",
            "====> Test set loss: 145.6766\n",
            "Train Epoch: 97 [0/1400 (0%)]\tLoss: 117.746651\n",
            "Train Epoch: 97 [40/1400 (3%)]\tLoss: 125.631805\n",
            "Train Epoch: 97 [80/1400 (6%)]\tLoss: 172.146744\n",
            "Train Epoch: 97 [120/1400 (9%)]\tLoss: 132.106323\n",
            "Train Epoch: 97 [160/1400 (11%)]\tLoss: 139.588669\n",
            "Train Epoch: 97 [200/1400 (14%)]\tLoss: 163.732742\n",
            "Train Epoch: 97 [240/1400 (17%)]\tLoss: 146.702621\n",
            "Train Epoch: 97 [280/1400 (20%)]\tLoss: 142.466125\n",
            "Train Epoch: 97 [320/1400 (23%)]\tLoss: 131.826096\n",
            "Train Epoch: 97 [360/1400 (26%)]\tLoss: 136.082565\n",
            "Train Epoch: 97 [400/1400 (29%)]\tLoss: 145.024368\n",
            "Train Epoch: 97 [440/1400 (31%)]\tLoss: 145.546036\n",
            "Train Epoch: 97 [480/1400 (34%)]\tLoss: 148.747772\n",
            "Train Epoch: 97 [520/1400 (37%)]\tLoss: 149.114807\n",
            "Train Epoch: 97 [560/1400 (40%)]\tLoss: 138.595306\n",
            "Train Epoch: 97 [600/1400 (43%)]\tLoss: 151.118210\n",
            "Train Epoch: 97 [640/1400 (46%)]\tLoss: 140.887650\n",
            "Train Epoch: 97 [680/1400 (49%)]\tLoss: 145.103470\n",
            "Train Epoch: 97 [720/1400 (51%)]\tLoss: 133.648193\n",
            "Train Epoch: 97 [760/1400 (54%)]\tLoss: 148.349075\n",
            "Train Epoch: 97 [800/1400 (57%)]\tLoss: 143.808060\n",
            "Train Epoch: 97 [840/1400 (60%)]\tLoss: 163.538986\n",
            "Train Epoch: 97 [880/1400 (63%)]\tLoss: 139.785934\n",
            "Train Epoch: 97 [920/1400 (66%)]\tLoss: 148.766205\n",
            "Train Epoch: 97 [960/1400 (69%)]\tLoss: 111.393105\n",
            "Train Epoch: 97 [1000/1400 (71%)]\tLoss: 133.786972\n",
            "Train Epoch: 97 [1040/1400 (74%)]\tLoss: 114.830048\n",
            "Train Epoch: 97 [1080/1400 (77%)]\tLoss: 145.099487\n",
            "Train Epoch: 97 [1120/1400 (80%)]\tLoss: 152.611038\n",
            "Train Epoch: 97 [1160/1400 (83%)]\tLoss: 144.151932\n",
            "Train Epoch: 97 [1200/1400 (86%)]\tLoss: 142.686417\n",
            "Train Epoch: 97 [1240/1400 (89%)]\tLoss: 137.602600\n",
            "Train Epoch: 97 [1280/1400 (91%)]\tLoss: 142.725449\n",
            "Train Epoch: 97 [1320/1400 (94%)]\tLoss: 140.716278\n",
            "Train Epoch: 97 [1360/1400 (97%)]\tLoss: 143.449234\n",
            "====> Epoch: 97 Average loss: 141.5570\n",
            "====> Test set loss: 145.7635\n",
            "Train Epoch: 98 [0/1400 (0%)]\tLoss: 153.562439\n",
            "Train Epoch: 98 [40/1400 (3%)]\tLoss: 161.714981\n",
            "Train Epoch: 98 [80/1400 (6%)]\tLoss: 121.226067\n",
            "Train Epoch: 98 [120/1400 (9%)]\tLoss: 144.000458\n",
            "Train Epoch: 98 [160/1400 (11%)]\tLoss: 113.707588\n",
            "Train Epoch: 98 [200/1400 (14%)]\tLoss: 164.532196\n",
            "Train Epoch: 98 [240/1400 (17%)]\tLoss: 139.673203\n",
            "Train Epoch: 98 [280/1400 (20%)]\tLoss: 132.155502\n",
            "Train Epoch: 98 [320/1400 (23%)]\tLoss: 124.921722\n",
            "Train Epoch: 98 [360/1400 (26%)]\tLoss: 155.603470\n",
            "Train Epoch: 98 [400/1400 (29%)]\tLoss: 139.454330\n",
            "Train Epoch: 98 [440/1400 (31%)]\tLoss: 135.962906\n",
            "Train Epoch: 98 [480/1400 (34%)]\tLoss: 159.671631\n",
            "Train Epoch: 98 [520/1400 (37%)]\tLoss: 172.562927\n",
            "Train Epoch: 98 [560/1400 (40%)]\tLoss: 148.357178\n",
            "Train Epoch: 98 [600/1400 (43%)]\tLoss: 137.196274\n",
            "Train Epoch: 98 [640/1400 (46%)]\tLoss: 155.248611\n",
            "Train Epoch: 98 [680/1400 (49%)]\tLoss: 144.083298\n",
            "Train Epoch: 98 [720/1400 (51%)]\tLoss: 135.893433\n",
            "Train Epoch: 98 [760/1400 (54%)]\tLoss: 128.856674\n",
            "Train Epoch: 98 [800/1400 (57%)]\tLoss: 120.014275\n",
            "Train Epoch: 98 [840/1400 (60%)]\tLoss: 125.468750\n",
            "Train Epoch: 98 [880/1400 (63%)]\tLoss: 161.390686\n",
            "Train Epoch: 98 [920/1400 (66%)]\tLoss: 139.445099\n",
            "Train Epoch: 98 [960/1400 (69%)]\tLoss: 123.041054\n",
            "Train Epoch: 98 [1000/1400 (71%)]\tLoss: 132.108215\n",
            "Train Epoch: 98 [1040/1400 (74%)]\tLoss: 132.536530\n",
            "Train Epoch: 98 [1080/1400 (77%)]\tLoss: 143.924042\n",
            "Train Epoch: 98 [1120/1400 (80%)]\tLoss: 158.737732\n",
            "Train Epoch: 98 [1160/1400 (83%)]\tLoss: 148.732193\n",
            "Train Epoch: 98 [1200/1400 (86%)]\tLoss: 151.328094\n",
            "Train Epoch: 98 [1240/1400 (89%)]\tLoss: 136.640747\n",
            "Train Epoch: 98 [1280/1400 (91%)]\tLoss: 142.064270\n",
            "Train Epoch: 98 [1320/1400 (94%)]\tLoss: 159.499969\n",
            "Train Epoch: 98 [1360/1400 (97%)]\tLoss: 165.712555\n",
            "====> Epoch: 98 Average loss: 141.5340\n",
            "====> Test set loss: 145.7958\n",
            "Train Epoch: 99 [0/1400 (0%)]\tLoss: 129.339371\n",
            "Train Epoch: 99 [40/1400 (3%)]\tLoss: 119.476341\n",
            "Train Epoch: 99 [80/1400 (6%)]\tLoss: 145.752380\n",
            "Train Epoch: 99 [120/1400 (9%)]\tLoss: 131.728119\n",
            "Train Epoch: 99 [160/1400 (11%)]\tLoss: 135.996002\n",
            "Train Epoch: 99 [200/1400 (14%)]\tLoss: 155.549957\n",
            "Train Epoch: 99 [240/1400 (17%)]\tLoss: 125.775429\n",
            "Train Epoch: 99 [280/1400 (20%)]\tLoss: 160.933151\n",
            "Train Epoch: 99 [320/1400 (23%)]\tLoss: 132.730896\n",
            "Train Epoch: 99 [360/1400 (26%)]\tLoss: 137.906357\n",
            "Train Epoch: 99 [400/1400 (29%)]\tLoss: 158.280792\n",
            "Train Epoch: 99 [440/1400 (31%)]\tLoss: 140.908447\n",
            "Train Epoch: 99 [480/1400 (34%)]\tLoss: 159.335632\n",
            "Train Epoch: 99 [520/1400 (37%)]\tLoss: 121.557533\n",
            "Train Epoch: 99 [560/1400 (40%)]\tLoss: 131.224258\n",
            "Train Epoch: 99 [600/1400 (43%)]\tLoss: 162.708923\n",
            "Train Epoch: 99 [640/1400 (46%)]\tLoss: 107.911072\n",
            "Train Epoch: 99 [680/1400 (49%)]\tLoss: 138.318268\n",
            "Train Epoch: 99 [720/1400 (51%)]\tLoss: 127.967072\n",
            "Train Epoch: 99 [760/1400 (54%)]\tLoss: 146.650284\n",
            "Train Epoch: 99 [800/1400 (57%)]\tLoss: 125.994843\n",
            "Train Epoch: 99 [840/1400 (60%)]\tLoss: 159.491882\n",
            "Train Epoch: 99 [880/1400 (63%)]\tLoss: 118.338043\n",
            "Train Epoch: 99 [920/1400 (66%)]\tLoss: 118.005379\n",
            "Train Epoch: 99 [960/1400 (69%)]\tLoss: 114.660133\n",
            "Train Epoch: 99 [1000/1400 (71%)]\tLoss: 122.375168\n",
            "Train Epoch: 99 [1040/1400 (74%)]\tLoss: 137.155869\n",
            "Train Epoch: 99 [1080/1400 (77%)]\tLoss: 159.167938\n",
            "Train Epoch: 99 [1120/1400 (80%)]\tLoss: 131.337448\n",
            "Train Epoch: 99 [1160/1400 (83%)]\tLoss: 148.849091\n",
            "Train Epoch: 99 [1200/1400 (86%)]\tLoss: 108.464104\n",
            "Train Epoch: 99 [1240/1400 (89%)]\tLoss: 139.820969\n",
            "Train Epoch: 99 [1280/1400 (91%)]\tLoss: 145.356842\n",
            "Train Epoch: 99 [1320/1400 (94%)]\tLoss: 163.212906\n",
            "Train Epoch: 99 [1360/1400 (97%)]\tLoss: 155.240952\n",
            "====> Epoch: 99 Average loss: 141.5001\n",
            "====> Test set loss: 145.8139\n",
            "Train Epoch: 100 [0/1400 (0%)]\tLoss: 145.307648\n",
            "Train Epoch: 100 [40/1400 (3%)]\tLoss: 131.457245\n",
            "Train Epoch: 100 [80/1400 (6%)]\tLoss: 135.003052\n",
            "Train Epoch: 100 [120/1400 (9%)]\tLoss: 142.133942\n",
            "Train Epoch: 100 [160/1400 (11%)]\tLoss: 137.068542\n",
            "Train Epoch: 100 [200/1400 (14%)]\tLoss: 124.613098\n",
            "Train Epoch: 100 [240/1400 (17%)]\tLoss: 149.674118\n",
            "Train Epoch: 100 [280/1400 (20%)]\tLoss: 151.711975\n",
            "Train Epoch: 100 [320/1400 (23%)]\tLoss: 143.495453\n",
            "Train Epoch: 100 [360/1400 (26%)]\tLoss: 145.204971\n",
            "Train Epoch: 100 [400/1400 (29%)]\tLoss: 145.125946\n",
            "Train Epoch: 100 [440/1400 (31%)]\tLoss: 133.486328\n",
            "Train Epoch: 100 [480/1400 (34%)]\tLoss: 124.503998\n",
            "Train Epoch: 100 [520/1400 (37%)]\tLoss: 120.553169\n",
            "Train Epoch: 100 [560/1400 (40%)]\tLoss: 148.045532\n",
            "Train Epoch: 100 [600/1400 (43%)]\tLoss: 137.261032\n",
            "Train Epoch: 100 [640/1400 (46%)]\tLoss: 149.024643\n",
            "Train Epoch: 100 [680/1400 (49%)]\tLoss: 150.127609\n",
            "Train Epoch: 100 [720/1400 (51%)]\tLoss: 122.411209\n",
            "Train Epoch: 100 [760/1400 (54%)]\tLoss: 147.077225\n",
            "Train Epoch: 100 [800/1400 (57%)]\tLoss: 134.994690\n",
            "Train Epoch: 100 [840/1400 (60%)]\tLoss: 123.954140\n",
            "Train Epoch: 100 [880/1400 (63%)]\tLoss: 140.064362\n",
            "Train Epoch: 100 [920/1400 (66%)]\tLoss: 134.922577\n",
            "Train Epoch: 100 [960/1400 (69%)]\tLoss: 131.043060\n",
            "Train Epoch: 100 [1000/1400 (71%)]\tLoss: 143.118958\n",
            "Train Epoch: 100 [1040/1400 (74%)]\tLoss: 151.114471\n",
            "Train Epoch: 100 [1080/1400 (77%)]\tLoss: 129.290924\n",
            "Train Epoch: 100 [1120/1400 (80%)]\tLoss: 145.664536\n",
            "Train Epoch: 100 [1160/1400 (83%)]\tLoss: 150.133514\n",
            "Train Epoch: 100 [1200/1400 (86%)]\tLoss: 125.223869\n",
            "Train Epoch: 100 [1240/1400 (89%)]\tLoss: 157.765411\n",
            "Train Epoch: 100 [1280/1400 (91%)]\tLoss: 148.956512\n",
            "Train Epoch: 100 [1320/1400 (94%)]\tLoss: 142.929092\n",
            "Train Epoch: 100 [1360/1400 (97%)]\tLoss: 150.661713\n",
            "====> Epoch: 100 Average loss: 141.4887\n",
            "====> Test set loss: 145.8409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "zjimJsLKMOA9",
        "outputId": "f911a573-a2fe-4083-cd4f-111cec3b1919"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# training loop\n",
        "\n",
        "\n",
        "# testing\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(val_losses) / 4, label=\"val\")\n",
        "plt.plot(np.array(train_losses) / 4, label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f899f7e4350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 617
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "f_omdtHa54oV",
        "outputId": "82ee67e0-a8b0-4518-f8c6-b601abd868ba"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#data, y = next(iter(test_loader))\n",
        "#data, y = data.cuda(), y.cuda()\n",
        "\n",
        "recon_batch, mu, logvar, pred = model(keepx.cuda())\n",
        "\n",
        "plt.imshow(keepx[1])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "rROeIGTm57m7",
        "outputId": "fb2f3ffd-9d44-4207-8d87-23d45c130b7a"
      },
      "source": [
        "plt.imshow(recon_batch.view(4, 27, 27)[1].cpu().detach().numpy())\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93zXleeF6v6n",
        "outputId": "92fff233-9442-4775-bb9c-3fe18fdae558"
      },
      "source": [
        "y[0], pred[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([12.0543,  0.6352,  7.2949], device='cuda:0'),\n",
              " tensor([8.6152, 8.7800, 8.7319], device='cuda:0', grad_fn=<SelectBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roAykwEMq6ti"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bW0Gz99rgWQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}