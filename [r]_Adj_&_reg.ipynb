{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[r] Adj & reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvDU2rb//T+LM11nZy6bjh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taechanha/inverseDesignMetaMatl/blob/main/%5Br%5D_Adj_%26_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d-CtwD3G6RH",
        "outputId": "a8f027a6-12a5-4989-99f5-6667d74f5c9c"
      },
      "source": [
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = open(\"/content/dataset_2.txt\", 'r')\n",
        "data = []\n",
        "length = 0\n",
        "for i in f:\n",
        "  new = []\n",
        "  new.append(i)\n",
        "  data.append(new)\n",
        "\n",
        "  # length of dataset\n",
        "  length += 1\n",
        "\n",
        "f.close()\n",
        "\n",
        "# create dataset from data\n",
        "dataset = []\n",
        "for i in range(length):\n",
        "  new = []\n",
        "  for j in data[i][0].split(','):\n",
        "    new.append(float(j))\n",
        "  dataset.append(new)\n",
        "\n",
        "\n",
        "# trim out label from dataset\n",
        "# Ex Ey Ez\n",
        "label = []\n",
        "new = []\n",
        "for line in dataset:\n",
        "  tmp = []\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.reverse()\n",
        "  label.append(tmp)\n",
        "\n",
        "print(\"label.shape: \", np.array(label).shape)\n",
        "\n",
        "\n",
        "# create edge_index\n",
        "edge_mat = np.zeros(shape=(27,27))\n",
        "edge_index = []\n",
        "\n",
        "for e in range(len(dataset)):\n",
        "  for i in range(0, 27):\n",
        "    for j in range(i+1, 27):\n",
        "      edge_mat[i][j] = dataset[e].pop(0)\n",
        "  edge_index.append(edge_mat + edge_mat.T)\n",
        "\n",
        "\n",
        "# whole dataset to Tensor & train/test split\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "edge_index = torch.FloatTensor(edge_index)\n",
        "label = torch.FloatTensor(label)\n",
        "\n",
        "split = int(length * 0.8)\n",
        "\n",
        "x_train = edge_index[:split]\n",
        "y_train = label[:split]\n",
        "x_val = edge_index[split:]\n",
        "y_val = label[split:]\n",
        "\n",
        "print(\"x_train, y_train, x_val shape: \", x_train.shape, y_train.shape, x_val.shape)\n",
        "\n",
        "# create torch dataset\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, label, dataset, transform=None, target_transform=None):\n",
        "        self.labels = label\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return data, label\n",
        "\n",
        "# create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_data   = CustomDataset(label=y_train, dataset=x_train)\n",
        "test_data       = CustomDataset(label=y_val, dataset=x_val)\n",
        "train_loader    = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=4, shuffle=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label.shape:  (2000, 3)\n",
            "x_train, y_train, x_val shape:  torch.Size([1600, 27, 27]) torch.Size([1600, 3]) torch.Size([400, 27, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfJZYdEUNMBu",
        "outputId": "04296239-d882-4042-a80d-600e50efdeb3"
      },
      "source": [
        "x_train = edge_index[:split]\n",
        "y_train = label[:split][:, 0]\n",
        "x_val = edge_index[split:]\n",
        "y_val = label[split:][:, 0]\n",
        "\n",
        "print(\"x_train, y_train, x_val shape: \", x_train.shape, y_train.shape, x_val.shape)\n",
        "\n",
        "# create torch dataset\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, label, dataset, transform=None, target_transform=None):\n",
        "        self.labels = label\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return data, label\n",
        "\n",
        "# create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_data   = CustomDataset(label=y_train, dataset=x_train)\n",
        "test_data       = CustomDataset(label=y_val, dataset=x_val)\n",
        "train_loader    = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=4, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train, y_train, x_val shape:  torch.Size([1600, 27, 27]) torch.Size([1600]) torch.Size([400, 27, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmswwxkQN6jl",
        "outputId": "c0a49cfa-5ec0-45bb-8adb-08a08825cb38"
      },
      "source": [
        "y_train.shape, y_val.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1600]), torch.Size([400]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fVyUoF2PX5i"
      },
      "source": [
        "def r2(output, target):\n",
        "    target_mean = torch.mean(target)\n",
        "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
        "    ss_res = torch.sum((target - output) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRxva3oEI8X_"
      },
      "source": [
        "epochs = 500"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rx-pOccJllZ"
      },
      "source": [
        "def train(model, train_losses, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        o = model(x)\n",
        "        loss = loss_function(o.squeeze(), y)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    print('====> Epoch: {} loss: {:.4f}'.format(e, train_loss / len(train_loader)))\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "def test(model, val_losses):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda().float(), y.cuda()       \n",
        "            o = model(x)\n",
        "            loss = loss_function(o.squeeze(), y)\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))\n",
        "    val_losses.append(test_loss / len(test_loader))\n",
        "\n",
        "def test_pred(model):\n",
        "    y_pred = []\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda().float(), y.cuda()       \n",
        "            o = model(x)\n",
        "            y_pred.append(o)\n",
        "\n",
        "            loss = loss_function(o.squeeze(), y)\n",
        "            test_loss += loss.item()\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIbPgPpLJXZX",
        "outputId": "aded7777-3b9f-4476-e17a-116372789777"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 4)\n",
        "        self.fc4 = nn.Linear(4, 2)\n",
        "        self.fc5 = nn.Linear(2, 1)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc5(x)\n",
        "\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc4(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=16, bias=True)\n",
            "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (fc3): Linear(in_features=8, out_features=4, bias=True)\n",
            "  (fc4): Linear(in_features=4, out_features=2, bias=True)\n",
            "  (fc5): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ1KoOdUJhoG",
        "outputId": "7e806b88-d8d8-4695-ffce-14ca3f02735b"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11865"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "fDV5B9gDJmRl",
        "outputId": "bb35dbc6-6309-4877-859d-0e973b930b5b"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 200\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 1.8649\n",
            "====> Test set loss: 14.3648\n",
            "====> Epoch: 2 loss: 1.4478\n",
            "====> Test set loss: 14.2455\n",
            "====> Epoch: 3 loss: 1.2479\n",
            "====> Test set loss: 14.2580\n",
            "====> Epoch: 4 loss: 1.1412\n",
            "====> Test set loss: 14.1296\n",
            "====> Epoch: 5 loss: 1.0741\n",
            "====> Test set loss: 14.1706\n",
            "====> Epoch: 6 loss: 1.0256\n",
            "====> Test set loss: 14.1324\n",
            "====> Epoch: 7 loss: 0.9952\n",
            "====> Test set loss: 14.1428\n",
            "====> Epoch: 8 loss: 0.9675\n",
            "====> Test set loss: 14.1092\n",
            "====> Epoch: 9 loss: 0.9483\n",
            "====> Test set loss: 14.1378\n",
            "====> Epoch: 10 loss: 0.9320\n",
            "====> Test set loss: 14.1190\n",
            "====> Epoch: 11 loss: 0.9179\n",
            "====> Test set loss: 14.1073\n",
            "====> Epoch: 12 loss: 0.9059\n",
            "====> Test set loss: 14.1596\n",
            "====> Epoch: 13 loss: 0.8964\n",
            "====> Test set loss: 14.1207\n",
            "====> Epoch: 14 loss: 0.8908\n",
            "====> Test set loss: 14.1386\n",
            "====> Epoch: 15 loss: 0.8844\n",
            "====> Test set loss: 14.1053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-210-bce91159fde1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cea082b7058d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_losses, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-199-36ff58a79012>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# x = F.leaky_relu(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee6T2qONJtaE",
        "outputId": "973d0c6e-c5d7-47ad-89a3-a6a6edfbc13a"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o.squeeze(), y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.5039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el8RuD_ar7m9",
        "outputId": "be562c0b-5a83-480b-ee08-ca74304cb556"
      },
      "source": [
        "# R square\n",
        "o=model(x_val.cuda())\n",
        "r2(o.squeeze(), y_val.cuda())"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5963, device='cuda:0', grad_fn=<RsubBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZjBUzDfJ3RK"
      },
      "source": [
        "y1 = y_pred"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "ZPRIRfrYJqth",
        "outputId": "261df0a3-cc16-42de-f7f6-3d03aeeb5bdb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2c1ad93b90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBX8iKkQPp1C",
        "outputId": "d1909505-023c-4e48-982f-79264db5157e"
      },
      "source": [
        "# R square\n",
        "o=model(x_val.cuda())\n",
        "r2(o.squeeze(), y_val.cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6933, device='cuda:0', grad_fn=<RsubBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5ApAlnVJ1mS"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dpFvYAFJ81R",
        "outputId": "53ee6c49-58bc-4dae-c7f1-889763cc4e12"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 16)\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc3(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc4(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=16, bias=True)\n",
            "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrWkhmk7KV1_",
        "outputId": "1821f9f2-cadf-4681-d301-8d9dd6ef4270"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11697"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "n8JGCdACKXiv",
        "outputId": "6d68591b-d583-4cde-9288-b556545f722c"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 117\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-402120688781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-cea082b7058d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_losses, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4GehNDjKiBe"
      },
      "source": [
        "y2 = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5P8sm4eTU7q"
      },
      "source": [
        "y_pred = test_pred(model)\n",
        "# [torch(4,3), torch(4,3), ... 100개]를 torch [400, 3]으로 변환\n",
        "y_pred = torch.cat((y_pred), 0)\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "gnIoc5xMSQHJ",
        "outputId": "490a651d-4fe5-45a5-efbd-1dc7cc262170"
      },
      "source": [
        "torch.argmin(torch.tensor(y2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b53d11769834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "YddurwCnKjzn",
        "outputId": "5910f9fa-cd66-43b5-c2f5-868166ffd67f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2c1ad93150>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFFQ7OxIKkhB",
        "outputId": "5c46fd39-fe96-41e0-abf4-17a9c75bcc0d"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o.squeeze(), y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.1316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvzgb0TGQTLg",
        "outputId": "cc76eeab-f213-455e-b690-3720ab1d356e"
      },
      "source": [
        "# R square\n",
        "o=model(x_val.cuda())\n",
        "r2(o.squeeze(), y_val.cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6711, device='cuda:0', grad_fn=<RsubBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcM4bDTYKlbq"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHwuSnOJKlp9",
        "outputId": "c76c15da-2542-49b5-9cc1-ff1c7aa16d42"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc4(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hol0TR3YeG3o",
        "outputId": "d6fca6ad-0f45-4f14-ca3b-876bbba2dc89"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23393"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8K_FXedhK2cc",
        "outputId": "2aa719ed-1de5-4499-c840-da83d04821f9"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 500\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 115.7770\n",
            "====> Test set loss: 101.7309\n",
            "====> Epoch: 2 loss: 86.7515\n",
            "====> Test set loss: 69.8029\n",
            "====> Epoch: 3 loss: 56.5455\n",
            "====> Test set loss: 42.9710\n",
            "====> Epoch: 4 loss: 35.7865\n",
            "====> Test set loss: 28.0231\n",
            "====> Epoch: 5 loss: 26.2275\n",
            "====> Test set loss: 22.6708\n",
            "====> Epoch: 6 loss: 22.8976\n",
            "====> Test set loss: 20.8242\n",
            "====> Epoch: 7 loss: 21.5491\n",
            "====> Test set loss: 20.0508\n",
            "====> Epoch: 8 loss: 20.6231\n",
            "====> Test set loss: 19.4580\n",
            "====> Epoch: 9 loss: 19.7488\n",
            "====> Test set loss: 18.9166\n",
            "====> Epoch: 10 loss: 18.8458\n",
            "====> Test set loss: 18.3830\n",
            "====> Epoch: 11 loss: 17.9075\n",
            "====> Test set loss: 17.8178\n",
            "====> Epoch: 12 loss: 16.9174\n",
            "====> Test set loss: 17.2440\n",
            "====> Epoch: 13 loss: 15.9093\n",
            "====> Test set loss: 16.6383\n",
            "====> Epoch: 14 loss: 14.9029\n",
            "====> Test set loss: 16.0498\n",
            "====> Epoch: 15 loss: 13.9220\n",
            "====> Test set loss: 15.5034\n",
            "====> Epoch: 16 loss: 12.9991\n",
            "====> Test set loss: 15.0013\n",
            "====> Epoch: 17 loss: 12.1316\n",
            "====> Test set loss: 14.5178\n",
            "====> Epoch: 18 loss: 11.3407\n",
            "====> Test set loss: 14.0937\n",
            "====> Epoch: 19 loss: 10.6439\n",
            "====> Test set loss: 13.7317\n",
            "====> Epoch: 20 loss: 10.0093\n",
            "====> Test set loss: 13.3991\n",
            "====> Epoch: 21 loss: 9.4391\n",
            "====> Test set loss: 13.1140\n",
            "====> Epoch: 22 loss: 8.9273\n",
            "====> Test set loss: 12.8790\n",
            "====> Epoch: 23 loss: 8.4682\n",
            "====> Test set loss: 12.6393\n",
            "====> Epoch: 24 loss: 8.0505\n",
            "====> Test set loss: 12.4330\n",
            "====> Epoch: 25 loss: 7.6701\n",
            "====> Test set loss: 12.2361\n",
            "====> Epoch: 26 loss: 7.3201\n",
            "====> Test set loss: 12.0473\n",
            "====> Epoch: 27 loss: 6.9984\n",
            "====> Test set loss: 11.8899\n",
            "====> Epoch: 28 loss: 6.6915\n",
            "====> Test set loss: 11.7494\n",
            "====> Epoch: 29 loss: 6.4116\n",
            "====> Test set loss: 11.6077\n",
            "====> Epoch: 30 loss: 6.1429\n",
            "====> Test set loss: 11.4767\n",
            "====> Epoch: 31 loss: 5.8947\n",
            "====> Test set loss: 11.3502\n",
            "====> Epoch: 32 loss: 5.6490\n",
            "====> Test set loss: 11.2052\n",
            "====> Epoch: 33 loss: 5.4219\n",
            "====> Test set loss: 11.1408\n",
            "====> Epoch: 34 loss: 5.2159\n",
            "====> Test set loss: 11.0325\n",
            "====> Epoch: 35 loss: 5.0054\n",
            "====> Test set loss: 10.9095\n",
            "====> Epoch: 36 loss: 4.8018\n",
            "====> Test set loss: 10.8109\n",
            "====> Epoch: 37 loss: 4.6084\n",
            "====> Test set loss: 10.6996\n",
            "====> Epoch: 38 loss: 4.4274\n",
            "====> Test set loss: 10.6021\n",
            "====> Epoch: 39 loss: 4.2409\n",
            "====> Test set loss: 10.5372\n",
            "====> Epoch: 40 loss: 4.0792\n",
            "====> Test set loss: 10.4254\n",
            "====> Epoch: 41 loss: 3.9125\n",
            "====> Test set loss: 10.3234\n",
            "====> Epoch: 42 loss: 3.7518\n",
            "====> Test set loss: 10.2551\n",
            "====> Epoch: 43 loss: 3.5963\n",
            "====> Test set loss: 10.1414\n",
            "====> Epoch: 44 loss: 3.4409\n",
            "====> Test set loss: 10.0974\n",
            "====> Epoch: 45 loss: 3.2953\n",
            "====> Test set loss: 9.9986\n",
            "====> Epoch: 46 loss: 3.1564\n",
            "====> Test set loss: 9.9248\n",
            "====> Epoch: 47 loss: 3.0241\n",
            "====> Test set loss: 9.8656\n",
            "====> Epoch: 48 loss: 2.8875\n",
            "====> Test set loss: 9.7867\n",
            "====> Epoch: 49 loss: 2.7669\n",
            "====> Test set loss: 9.6913\n",
            "====> Epoch: 50 loss: 2.6439\n",
            "====> Test set loss: 9.6304\n",
            "====> Epoch: 51 loss: 2.5284\n",
            "====> Test set loss: 9.5860\n",
            "====> Epoch: 52 loss: 2.4132\n",
            "====> Test set loss: 9.5283\n",
            "====> Epoch: 53 loss: 2.3049\n",
            "====> Test set loss: 9.4591\n",
            "====> Epoch: 54 loss: 2.2020\n",
            "====> Test set loss: 9.4177\n",
            "====> Epoch: 55 loss: 2.0951\n",
            "====> Test set loss: 9.3625\n",
            "====> Epoch: 56 loss: 2.0042\n",
            "====> Test set loss: 9.3152\n",
            "====> Epoch: 57 loss: 1.9108\n",
            "====> Test set loss: 9.2521\n",
            "====> Epoch: 58 loss: 1.8257\n",
            "====> Test set loss: 9.2028\n",
            "====> Epoch: 59 loss: 1.7394\n",
            "====> Test set loss: 9.1746\n",
            "====> Epoch: 60 loss: 1.6523\n",
            "====> Test set loss: 9.1323\n",
            "====> Epoch: 61 loss: 1.5788\n",
            "====> Test set loss: 9.0717\n",
            "====> Epoch: 62 loss: 1.5019\n",
            "====> Test set loss: 9.0396\n",
            "====> Epoch: 63 loss: 1.4256\n",
            "====> Test set loss: 9.0336\n",
            "====> Epoch: 64 loss: 1.3579\n",
            "====> Test set loss: 8.9963\n",
            "====> Epoch: 65 loss: 1.2912\n",
            "====> Test set loss: 9.0185\n",
            "====> Epoch: 66 loss: 1.2291\n",
            "====> Test set loss: 8.9413\n",
            "====> Epoch: 67 loss: 1.1714\n",
            "====> Test set loss: 8.9290\n",
            "====> Epoch: 68 loss: 1.1143\n",
            "====> Test set loss: 8.9359\n",
            "====> Epoch: 69 loss: 1.0558\n",
            "====> Test set loss: 8.9058\n",
            "====> Epoch: 70 loss: 1.0008\n",
            "====> Test set loss: 8.8891\n",
            "====> Epoch: 71 loss: 0.9515\n",
            "====> Test set loss: 8.8814\n",
            "====> Epoch: 72 loss: 0.8991\n",
            "====> Test set loss: 8.8766\n",
            "====> Epoch: 73 loss: 0.8536\n",
            "====> Test set loss: 8.8626\n",
            "====> Epoch: 74 loss: 0.8065\n",
            "====> Test set loss: 8.9023\n",
            "====> Epoch: 75 loss: 0.7658\n",
            "====> Test set loss: 8.8984\n",
            "====> Epoch: 76 loss: 0.7263\n",
            "====> Test set loss: 8.8922\n",
            "====> Epoch: 77 loss: 0.6862\n",
            "====> Test set loss: 8.9078\n",
            "====> Epoch: 78 loss: 0.6467\n",
            "====> Test set loss: 8.9080\n",
            "====> Epoch: 79 loss: 0.6073\n",
            "====> Test set loss: 8.8745\n",
            "====> Epoch: 80 loss: 0.5763\n",
            "====> Test set loss: 8.9140\n",
            "====> Epoch: 81 loss: 0.5423\n",
            "====> Test set loss: 8.9301\n",
            "====> Epoch: 82 loss: 0.5085\n",
            "====> Test set loss: 8.9565\n",
            "====> Epoch: 83 loss: 0.4781\n",
            "====> Test set loss: 8.9536\n",
            "====> Epoch: 84 loss: 0.4487\n",
            "====> Test set loss: 8.9653\n",
            "====> Epoch: 85 loss: 0.4200\n",
            "====> Test set loss: 8.9631\n",
            "====> Epoch: 86 loss: 0.3932\n",
            "====> Test set loss: 8.9929\n",
            "====> Epoch: 87 loss: 0.3669\n",
            "====> Test set loss: 9.0295\n",
            "====> Epoch: 88 loss: 0.3416\n",
            "====> Test set loss: 9.0405\n",
            "====> Epoch: 89 loss: 0.3195\n",
            "====> Test set loss: 9.0633\n",
            "====> Epoch: 90 loss: 0.2968\n",
            "====> Test set loss: 9.1077\n",
            "====> Epoch: 91 loss: 0.2738\n",
            "====> Test set loss: 9.0831\n",
            "====> Epoch: 92 loss: 0.2523\n",
            "====> Test set loss: 9.1258\n",
            "====> Epoch: 93 loss: 0.2348\n",
            "====> Test set loss: 9.1346\n",
            "====> Epoch: 94 loss: 0.2175\n",
            "====> Test set loss: 9.1405\n",
            "====> Epoch: 95 loss: 0.1999\n",
            "====> Test set loss: 9.1972\n",
            "====> Epoch: 96 loss: 0.1834\n",
            "====> Test set loss: 9.2039\n",
            "====> Epoch: 97 loss: 0.1666\n",
            "====> Test set loss: 9.2632\n",
            "====> Epoch: 98 loss: 0.1525\n",
            "====> Test set loss: 9.2848\n",
            "====> Epoch: 99 loss: 0.1396\n",
            "====> Test set loss: 9.2769\n",
            "====> Epoch: 100 loss: 0.1261\n",
            "====> Test set loss: 9.3062\n",
            "====> Epoch: 101 loss: 0.1136\n",
            "====> Test set loss: 9.3282\n",
            "====> Epoch: 102 loss: 0.1035\n",
            "====> Test set loss: 9.3294\n",
            "====> Epoch: 103 loss: 0.0929\n",
            "====> Test set loss: 9.3625\n",
            "====> Epoch: 104 loss: 0.0826\n",
            "====> Test set loss: 9.3637\n",
            "====> Epoch: 105 loss: 0.0740\n",
            "====> Test set loss: 9.3895\n",
            "====> Epoch: 106 loss: 0.0663\n",
            "====> Test set loss: 9.3840\n",
            "====> Epoch: 107 loss: 0.0575\n",
            "====> Test set loss: 9.4370\n",
            "====> Epoch: 108 loss: 0.0510\n",
            "====> Test set loss: 9.4500\n",
            "====> Epoch: 109 loss: 0.0439\n",
            "====> Test set loss: 9.4777\n",
            "====> Epoch: 110 loss: 0.0380\n",
            "====> Test set loss: 9.4882\n",
            "====> Epoch: 111 loss: 0.0342\n",
            "====> Test set loss: 9.4750\n",
            "====> Epoch: 112 loss: 0.0282\n",
            "====> Test set loss: 9.5087\n",
            "====> Epoch: 113 loss: 0.0251\n",
            "====> Test set loss: 9.5309\n",
            "====> Epoch: 114 loss: 0.0213\n",
            "====> Test set loss: 9.5307\n",
            "====> Epoch: 115 loss: 0.0177\n",
            "====> Test set loss: 9.5385\n",
            "====> Epoch: 116 loss: 0.0144\n",
            "====> Test set loss: 9.5628\n",
            "====> Epoch: 117 loss: 0.0128\n",
            "====> Test set loss: 9.5737\n",
            "====> Epoch: 118 loss: 0.0111\n",
            "====> Test set loss: 9.5452\n",
            "====> Epoch: 119 loss: 0.0094\n",
            "====> Test set loss: 9.6171\n",
            "====> Epoch: 120 loss: 0.0085\n",
            "====> Test set loss: 9.6308\n",
            "====> Epoch: 121 loss: 0.0068\n",
            "====> Test set loss: 9.6010\n",
            "====> Epoch: 122 loss: 0.0073\n",
            "====> Test set loss: 9.6201\n",
            "====> Epoch: 123 loss: 0.0071\n",
            "====> Test set loss: 9.6515\n",
            "====> Epoch: 124 loss: 0.0082\n",
            "====> Test set loss: 9.6846\n",
            "====> Epoch: 125 loss: 0.0084\n",
            "====> Test set loss: 9.6725\n",
            "====> Epoch: 126 loss: 0.0080\n",
            "====> Test set loss: 9.6347\n",
            "====> Epoch: 127 loss: 0.0066\n",
            "====> Test set loss: 9.6590\n",
            "====> Epoch: 128 loss: 0.0061\n",
            "====> Test set loss: 9.6748\n",
            "====> Epoch: 129 loss: 0.0069\n",
            "====> Test set loss: 9.6961\n",
            "====> Epoch: 130 loss: 0.0071\n",
            "====> Test set loss: 9.6767\n",
            "====> Epoch: 131 loss: 0.0079\n",
            "====> Test set loss: 9.6387\n",
            "====> Epoch: 132 loss: 0.0076\n",
            "====> Test set loss: 9.7129\n",
            "====> Epoch: 133 loss: 0.0077\n",
            "====> Test set loss: 9.6775\n",
            "====> Epoch: 134 loss: 0.0070\n",
            "====> Test set loss: 9.6747\n",
            "====> Epoch: 135 loss: 0.0070\n",
            "====> Test set loss: 9.7234\n",
            "====> Epoch: 136 loss: 0.0070\n",
            "====> Test set loss: 9.7016\n",
            "====> Epoch: 137 loss: 0.0079\n",
            "====> Test set loss: 9.6604\n",
            "====> Epoch: 138 loss: 0.0077\n",
            "====> Test set loss: 9.6895\n",
            "====> Epoch: 139 loss: 0.0073\n",
            "====> Test set loss: 9.7108\n",
            "====> Epoch: 140 loss: 0.0072\n",
            "====> Test set loss: 9.6821\n",
            "====> Epoch: 141 loss: 0.0074\n",
            "====> Test set loss: 9.7182\n",
            "====> Epoch: 142 loss: 0.0065\n",
            "====> Test set loss: 9.6745\n",
            "====> Epoch: 143 loss: 0.0067\n",
            "====> Test set loss: 9.6954\n",
            "====> Epoch: 144 loss: 0.0075\n",
            "====> Test set loss: 9.6853\n",
            "====> Epoch: 145 loss: 0.0090\n",
            "====> Test set loss: 9.7357\n",
            "====> Epoch: 146 loss: 0.0085\n",
            "====> Test set loss: 9.6747\n",
            "====> Epoch: 147 loss: 0.0067\n",
            "====> Test set loss: 9.7155\n",
            "====> Epoch: 148 loss: 0.0065\n",
            "====> Test set loss: 9.6824\n",
            "====> Epoch: 149 loss: 0.0056\n",
            "====> Test set loss: 9.6718\n",
            "====> Epoch: 150 loss: 0.0068\n",
            "====> Test set loss: 9.7178\n",
            "====> Epoch: 151 loss: 0.0074\n",
            "====> Test set loss: 9.6456\n",
            "====> Epoch: 152 loss: 0.0083\n",
            "====> Test set loss: 9.6968\n",
            "====> Epoch: 153 loss: 0.0079\n",
            "====> Test set loss: 9.7108\n",
            "====> Epoch: 154 loss: 0.0075\n",
            "====> Test set loss: 9.7376\n",
            "====> Epoch: 155 loss: 0.0074\n",
            "====> Test set loss: 9.6912\n",
            "====> Epoch: 156 loss: 0.0070\n",
            "====> Test set loss: 9.6616\n",
            "====> Epoch: 157 loss: 0.0067\n",
            "====> Test set loss: 9.6909\n",
            "====> Epoch: 158 loss: 0.0063\n",
            "====> Test set loss: 9.6887\n",
            "====> Epoch: 159 loss: 0.0069\n",
            "====> Test set loss: 9.7108\n",
            "====> Epoch: 160 loss: 0.0071\n",
            "====> Test set loss: 9.6868\n",
            "====> Epoch: 161 loss: 0.0079\n",
            "====> Test set loss: 9.7009\n",
            "====> Epoch: 162 loss: 0.0089\n",
            "====> Test set loss: 9.6870\n",
            "====> Epoch: 163 loss: 0.0077\n",
            "====> Test set loss: 9.6887\n",
            "====> Epoch: 164 loss: 0.0067\n",
            "====> Test set loss: 9.6828\n",
            "====> Epoch: 165 loss: 0.0066\n",
            "====> Test set loss: 9.6864\n",
            "====> Epoch: 166 loss: 0.0073\n",
            "====> Test set loss: 9.7226\n",
            "====> Epoch: 167 loss: 0.0075\n",
            "====> Test set loss: 9.7077\n",
            "====> Epoch: 168 loss: 0.0080\n",
            "====> Test set loss: 9.6826\n",
            "====> Epoch: 169 loss: 0.0076\n",
            "====> Test set loss: 9.6863\n",
            "====> Epoch: 170 loss: 0.0070\n",
            "====> Test set loss: 9.6784\n",
            "====> Epoch: 171 loss: 0.0067\n",
            "====> Test set loss: 9.6789\n",
            "====> Epoch: 172 loss: 0.0073\n",
            "====> Test set loss: 9.7070\n",
            "====> Epoch: 173 loss: 0.0067\n",
            "====> Test set loss: 9.7102\n",
            "====> Epoch: 174 loss: 0.0077\n",
            "====> Test set loss: 9.6814\n",
            "====> Epoch: 175 loss: 0.0078\n",
            "====> Test set loss: 9.7184\n",
            "====> Epoch: 176 loss: 0.0069\n",
            "====> Test set loss: 9.6895\n",
            "====> Epoch: 177 loss: 0.0077\n",
            "====> Test set loss: 9.6948\n",
            "====> Epoch: 178 loss: 0.0067\n",
            "====> Test set loss: 9.7245\n",
            "====> Epoch: 179 loss: 0.0063\n",
            "====> Test set loss: 9.6997\n",
            "====> Epoch: 180 loss: 0.0065\n",
            "====> Test set loss: 9.6906\n",
            "====> Epoch: 181 loss: 0.0078\n",
            "====> Test set loss: 9.7693\n",
            "====> Epoch: 182 loss: 0.0078\n",
            "====> Test set loss: 9.6714\n",
            "====> Epoch: 183 loss: 0.0068\n",
            "====> Test set loss: 9.7032\n",
            "====> Epoch: 184 loss: 0.0071\n",
            "====> Test set loss: 9.6838\n",
            "====> Epoch: 185 loss: 0.0069\n",
            "====> Test set loss: 9.6867\n",
            "====> Epoch: 186 loss: 0.0072\n",
            "====> Test set loss: 9.6721\n",
            "====> Epoch: 187 loss: 0.0076\n",
            "====> Test set loss: 9.7178\n",
            "====> Epoch: 188 loss: 0.0083\n",
            "====> Test set loss: 9.6553\n",
            "====> Epoch: 189 loss: 0.0075\n",
            "====> Test set loss: 9.7060\n",
            "====> Epoch: 190 loss: 0.0066\n",
            "====> Test set loss: 9.6696\n",
            "====> Epoch: 191 loss: 0.0068\n",
            "====> Test set loss: 9.6912\n",
            "====> Epoch: 192 loss: 0.0067\n",
            "====> Test set loss: 9.7202\n",
            "====> Epoch: 193 loss: 0.0070\n",
            "====> Test set loss: 9.7086\n",
            "====> Epoch: 194 loss: 0.0072\n",
            "====> Test set loss: 9.6970\n",
            "====> Epoch: 195 loss: 0.0073\n",
            "====> Test set loss: 9.7151\n",
            "====> Epoch: 196 loss: 0.0077\n",
            "====> Test set loss: 9.7032\n",
            "====> Epoch: 197 loss: 0.0074\n",
            "====> Test set loss: 9.7046\n",
            "====> Epoch: 198 loss: 0.0065\n",
            "====> Test set loss: 9.7137\n",
            "====> Epoch: 199 loss: 0.0063\n",
            "====> Test set loss: 9.7228\n",
            "====> Epoch: 200 loss: 0.0068\n",
            "====> Test set loss: 9.6829\n",
            "====> Epoch: 201 loss: 0.0069\n",
            "====> Test set loss: 9.7007\n",
            "====> Epoch: 202 loss: 0.0076\n",
            "====> Test set loss: 9.7127\n",
            "====> Epoch: 203 loss: 0.0076\n",
            "====> Test set loss: 9.6791\n",
            "====> Epoch: 204 loss: 0.0072\n",
            "====> Test set loss: 9.6932\n",
            "====> Epoch: 205 loss: 0.0070\n",
            "====> Test set loss: 9.6936\n",
            "====> Epoch: 206 loss: 0.0070\n",
            "====> Test set loss: 9.6905\n",
            "====> Epoch: 207 loss: 0.0073\n",
            "====> Test set loss: 9.6779\n",
            "====> Epoch: 208 loss: 0.0067\n",
            "====> Test set loss: 9.7301\n",
            "====> Epoch: 209 loss: 0.0067\n",
            "====> Test set loss: 9.6914\n",
            "====> Epoch: 210 loss: 0.0072\n",
            "====> Test set loss: 9.6689\n",
            "====> Epoch: 211 loss: 0.0072\n",
            "====> Test set loss: 9.6874\n",
            "====> Epoch: 212 loss: 0.0064\n",
            "====> Test set loss: 9.7108\n",
            "====> Epoch: 213 loss: 0.0075\n",
            "====> Test set loss: 9.6996\n",
            "====> Epoch: 214 loss: 0.0072\n",
            "====> Test set loss: 9.6750\n",
            "====> Epoch: 215 loss: 0.0072\n",
            "====> Test set loss: 9.7060\n",
            "====> Epoch: 216 loss: 0.0067\n",
            "====> Test set loss: 9.6956\n",
            "====> Epoch: 217 loss: 0.0059\n",
            "====> Test set loss: 9.6879\n",
            "====> Epoch: 218 loss: 0.0059\n",
            "====> Test set loss: 9.6585\n",
            "====> Epoch: 219 loss: 0.0067\n",
            "====> Test set loss: 9.6924\n",
            "====> Epoch: 220 loss: 0.0080\n",
            "====> Test set loss: 9.7270\n",
            "====> Epoch: 221 loss: 0.0080\n",
            "====> Test set loss: 9.7090\n",
            "====> Epoch: 222 loss: 0.0074\n",
            "====> Test set loss: 9.6792\n",
            "====> Epoch: 223 loss: 0.0068\n",
            "====> Test set loss: 9.6783\n",
            "====> Epoch: 224 loss: 0.0065\n",
            "====> Test set loss: 9.6918\n",
            "====> Epoch: 225 loss: 0.0061\n",
            "====> Test set loss: 9.7078\n",
            "====> Epoch: 226 loss: 0.0066\n",
            "====> Test set loss: 9.6823\n",
            "====> Epoch: 227 loss: 0.0079\n",
            "====> Test set loss: 9.6670\n",
            "====> Epoch: 228 loss: 0.0080\n",
            "====> Test set loss: 9.7482\n",
            "====> Epoch: 229 loss: 0.0073\n",
            "====> Test set loss: 9.6432\n",
            "====> Epoch: 230 loss: 0.0068\n",
            "====> Test set loss: 9.6774\n",
            "====> Epoch: 231 loss: 0.0065\n",
            "====> Test set loss: 9.6638\n",
            "====> Epoch: 232 loss: 0.0055\n",
            "====> Test set loss: 9.6851\n",
            "====> Epoch: 233 loss: 0.0060\n",
            "====> Test set loss: 9.7181\n",
            "====> Epoch: 234 loss: 0.0072\n",
            "====> Test set loss: 9.6952\n",
            "====> Epoch: 235 loss: 0.0083\n",
            "====> Test set loss: 9.6836\n",
            "====> Epoch: 236 loss: 0.0073\n",
            "====> Test set loss: 9.7133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7152d1477002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-cea082b7058d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_losses, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iZjJWmRK3-w"
      },
      "source": [
        "y3 = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOW8COQXK6cc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VxOL0-pK7DR",
        "outputId": "0728b547-3d90-4146-923e-0b455e4d7483"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o.squeeze(), y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 2.8025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGNWtEUQQe-G"
      },
      "source": [
        "# R square\n",
        "o=model(x_val.cuda())\n",
        "r2(o.squeeze(), y_val.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjhue5WsK79J"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CqjBeSeK8Qx",
        "outputId": "d39d827d-01f5-42db-9b3d-1c3f59c408d5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=364, bias=True)\n",
            "  (fc2): Linear(in_features=364, out_features=182, bias=True)\n",
            "  (fc3): Linear(in_features=182, out_features=91, bias=True)\n",
            "  (fc4): Linear(in_features=91, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL7Xw1e1eOse",
        "outputId": "52b91bd6-34a4-4fbe-81fb-6f533890e753"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "348895"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANYqgRBHLErV",
        "outputId": "df0fb796-2658-4263-ab52-428be512e788"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = epochs\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 38.6767\n",
            "====> Test set loss: 21.8808\n",
            "====> Epoch: 2 loss: 20.1779\n",
            "====> Test set loss: 18.3174\n",
            "====> Epoch: 3 loss: 12.9671\n",
            "====> Test set loss: 15.2371\n",
            "====> Epoch: 4 loss: 7.7493\n",
            "====> Test set loss: 14.0519\n",
            "====> Epoch: 5 loss: 4.4304\n",
            "====> Test set loss: 12.7256\n",
            "====> Epoch: 6 loss: 2.3621\n",
            "====> Test set loss: 11.8337\n",
            "====> Epoch: 7 loss: 1.2242\n",
            "====> Test set loss: 11.7461\n",
            "====> Epoch: 8 loss: 0.5531\n",
            "====> Test set loss: 11.6483\n",
            "====> Epoch: 9 loss: 0.2513\n",
            "====> Test set loss: 11.4328\n",
            "====> Epoch: 10 loss: 0.1152\n",
            "====> Test set loss: 11.4306\n",
            "====> Epoch: 11 loss: 0.0561\n",
            "====> Test set loss: 11.4027\n",
            "====> Epoch: 12 loss: 0.0414\n",
            "====> Test set loss: 11.3603\n",
            "====> Epoch: 13 loss: 0.0748\n",
            "====> Test set loss: 11.5374\n",
            "====> Epoch: 14 loss: 0.2372\n",
            "====> Test set loss: 11.6470\n",
            "====> Epoch: 15 loss: 0.5702\n",
            "====> Test set loss: 11.5077\n",
            "====> Epoch: 16 loss: 0.6402\n",
            "====> Test set loss: 11.8161\n",
            "====> Epoch: 17 loss: 0.4219\n",
            "====> Test set loss: 11.5147\n",
            "====> Epoch: 18 loss: 0.2480\n",
            "====> Test set loss: 11.4326\n",
            "====> Epoch: 19 loss: 0.1871\n",
            "====> Test set loss: 11.2592\n",
            "====> Epoch: 20 loss: 0.1755\n",
            "====> Test set loss: 11.5793\n",
            "====> Epoch: 21 loss: 0.2498\n",
            "====> Test set loss: 11.1034\n",
            "====> Epoch: 22 loss: 0.3520\n",
            "====> Test set loss: 11.2158\n",
            "====> Epoch: 23 loss: 0.4113\n",
            "====> Test set loss: 11.1247\n",
            "====> Epoch: 24 loss: 0.3373\n",
            "====> Test set loss: 11.0863\n",
            "====> Epoch: 25 loss: 0.2605\n",
            "====> Test set loss: 11.1729\n",
            "====> Epoch: 26 loss: 0.2317\n",
            "====> Test set loss: 11.0896\n",
            "====> Epoch: 27 loss: 0.2120\n",
            "====> Test set loss: 11.3068\n",
            "====> Epoch: 28 loss: 0.2055\n",
            "====> Test set loss: 11.0055\n",
            "====> Epoch: 29 loss: 0.2492\n",
            "====> Test set loss: 11.2732\n",
            "====> Epoch: 30 loss: 0.2743\n",
            "====> Test set loss: 11.0004\n",
            "====> Epoch: 31 loss: 0.2800\n",
            "====> Test set loss: 11.1914\n",
            "====> Epoch: 32 loss: 0.2418\n",
            "====> Test set loss: 10.9226\n",
            "====> Epoch: 33 loss: 0.2091\n",
            "====> Test set loss: 10.9823\n",
            "====> Epoch: 34 loss: 0.2173\n",
            "====> Test set loss: 11.2324\n",
            "====> Epoch: 35 loss: 0.2287\n",
            "====> Test set loss: 11.2263\n",
            "====> Epoch: 36 loss: 0.2307\n",
            "====> Test set loss: 10.8580\n",
            "====> Epoch: 37 loss: 0.2157\n",
            "====> Test set loss: 11.0564\n",
            "====> Epoch: 38 loss: 0.2284\n",
            "====> Test set loss: 10.8328\n",
            "====> Epoch: 39 loss: 0.2270\n",
            "====> Test set loss: 10.8255\n",
            "====> Epoch: 40 loss: 0.2184\n",
            "====> Test set loss: 11.1734\n",
            "====> Epoch: 41 loss: 0.2315\n",
            "====> Test set loss: 11.1583\n",
            "====> Epoch: 42 loss: 0.2248\n",
            "====> Test set loss: 10.7960\n",
            "====> Epoch: 43 loss: 0.1900\n",
            "====> Test set loss: 10.8799\n",
            "====> Epoch: 44 loss: 0.1702\n",
            "====> Test set loss: 10.8539\n",
            "====> Epoch: 45 loss: 0.1630\n",
            "====> Test set loss: 10.9441\n",
            "====> Epoch: 46 loss: 0.1872\n",
            "====> Test set loss: 10.7738\n",
            "====> Epoch: 47 loss: 0.1970\n",
            "====> Test set loss: 10.8716\n",
            "====> Epoch: 48 loss: 0.2181\n",
            "====> Test set loss: 10.7654\n",
            "====> Epoch: 49 loss: 0.2091\n",
            "====> Test set loss: 11.0367\n",
            "====> Epoch: 50 loss: 0.1960\n",
            "====> Test set loss: 10.7209\n",
            "====> Epoch: 51 loss: 0.1835\n",
            "====> Test set loss: 10.7786\n",
            "====> Epoch: 52 loss: 0.1753\n",
            "====> Test set loss: 10.9322\n",
            "====> Epoch: 53 loss: 0.1748\n",
            "====> Test set loss: 10.6785\n",
            "====> Epoch: 54 loss: 0.1626\n",
            "====> Test set loss: 10.8005\n",
            "====> Epoch: 55 loss: 0.1592\n",
            "====> Test set loss: 10.7084\n",
            "====> Epoch: 56 loss: 0.1646\n",
            "====> Test set loss: 10.8196\n",
            "====> Epoch: 57 loss: 0.1846\n",
            "====> Test set loss: 10.8598\n",
            "====> Epoch: 58 loss: 0.1827\n",
            "====> Test set loss: 10.5765\n",
            "====> Epoch: 59 loss: 0.1722\n",
            "====> Test set loss: 10.6987\n",
            "====> Epoch: 60 loss: 0.1536\n",
            "====> Test set loss: 10.6282\n",
            "====> Epoch: 61 loss: 0.1546\n",
            "====> Test set loss: 10.6242\n",
            "====> Epoch: 62 loss: 0.1574\n",
            "====> Test set loss: 10.7014\n",
            "====> Epoch: 63 loss: 0.1671\n",
            "====> Test set loss: 10.7506\n",
            "====> Epoch: 64 loss: 0.1638\n",
            "====> Test set loss: 10.9477\n",
            "====> Epoch: 65 loss: 0.1658\n",
            "====> Test set loss: 10.5315\n",
            "====> Epoch: 66 loss: 0.1557\n",
            "====> Test set loss: 10.6590\n",
            "====> Epoch: 67 loss: 0.1593\n",
            "====> Test set loss: 10.9586\n",
            "====> Epoch: 68 loss: 0.1488\n",
            "====> Test set loss: 10.6876\n",
            "====> Epoch: 69 loss: 0.1521\n",
            "====> Test set loss: 10.5170\n",
            "====> Epoch: 70 loss: 0.1583\n",
            "====> Test set loss: 10.7298\n",
            "====> Epoch: 71 loss: 0.1406\n",
            "====> Test set loss: 10.7030\n",
            "====> Epoch: 72 loss: 0.1237\n",
            "====> Test set loss: 10.8394\n",
            "====> Epoch: 73 loss: 0.1364\n",
            "====> Test set loss: 10.6466\n",
            "====> Epoch: 74 loss: 0.1485\n",
            "====> Test set loss: 10.5669\n",
            "====> Epoch: 75 loss: 0.1379\n",
            "====> Test set loss: 10.7682\n",
            "====> Epoch: 76 loss: 0.1356\n",
            "====> Test set loss: 10.4406\n",
            "====> Epoch: 77 loss: 0.1362\n",
            "====> Test set loss: 10.6009\n",
            "====> Epoch: 78 loss: 0.1541\n",
            "====> Test set loss: 10.5363\n",
            "====> Epoch: 79 loss: 0.1566\n",
            "====> Test set loss: 10.7553\n",
            "====> Epoch: 80 loss: 0.1275\n",
            "====> Test set loss: 10.8734\n",
            "====> Epoch: 81 loss: 0.1354\n",
            "====> Test set loss: 10.6878\n",
            "====> Epoch: 82 loss: 0.1393\n",
            "====> Test set loss: 10.4726\n",
            "====> Epoch: 83 loss: 0.1345\n",
            "====> Test set loss: 10.7191\n",
            "====> Epoch: 84 loss: 0.1320\n",
            "====> Test set loss: 10.6060\n",
            "====> Epoch: 85 loss: 0.1233\n",
            "====> Test set loss: 10.5999\n",
            "====> Epoch: 86 loss: 0.1406\n",
            "====> Test set loss: 10.5269\n",
            "====> Epoch: 87 loss: 0.1409\n",
            "====> Test set loss: 10.5519\n",
            "====> Epoch: 88 loss: 0.1322\n",
            "====> Test set loss: 10.6217\n",
            "====> Epoch: 89 loss: 0.1304\n",
            "====> Test set loss: 10.3528\n",
            "====> Epoch: 90 loss: 0.1106\n",
            "====> Test set loss: 10.5537\n",
            "====> Epoch: 91 loss: 0.1129\n",
            "====> Test set loss: 10.4525\n",
            "====> Epoch: 92 loss: 0.1242\n",
            "====> Test set loss: 10.3157\n",
            "====> Epoch: 93 loss: 0.1234\n",
            "====> Test set loss: 10.3684\n",
            "====> Epoch: 94 loss: 0.1425\n",
            "====> Test set loss: 10.7531\n",
            "====> Epoch: 95 loss: 0.1441\n",
            "====> Test set loss: 10.5953\n",
            "====> Epoch: 96 loss: 0.1169\n",
            "====> Test set loss: 10.4644\n",
            "====> Epoch: 97 loss: 0.0974\n",
            "====> Test set loss: 10.5337\n",
            "====> Epoch: 98 loss: 0.0979\n",
            "====> Test set loss: 10.5896\n",
            "====> Epoch: 99 loss: 0.1137\n",
            "====> Test set loss: 10.4580\n",
            "====> Epoch: 100 loss: 0.1370\n",
            "====> Test set loss: 10.4513\n",
            "====> Epoch: 101 loss: 0.1568\n",
            "====> Test set loss: 10.6219\n",
            "====> Epoch: 102 loss: 0.1216\n",
            "====> Test set loss: 10.5660\n",
            "====> Epoch: 103 loss: 0.0997\n",
            "====> Test set loss: 10.6319\n",
            "====> Epoch: 104 loss: 0.0883\n",
            "====> Test set loss: 10.5802\n",
            "====> Epoch: 105 loss: 0.0921\n",
            "====> Test set loss: 10.5512\n",
            "====> Epoch: 106 loss: 0.1021\n",
            "====> Test set loss: 10.6064\n",
            "====> Epoch: 107 loss: 0.1143\n",
            "====> Test set loss: 10.3495\n",
            "====> Epoch: 108 loss: 0.1248\n",
            "====> Test set loss: 10.3449\n",
            "====> Epoch: 109 loss: 0.1082\n",
            "====> Test set loss: 10.5750\n",
            "====> Epoch: 110 loss: 0.1062\n",
            "====> Test set loss: 10.4242\n",
            "====> Epoch: 111 loss: 0.1148\n",
            "====> Test set loss: 10.4922\n",
            "====> Epoch: 112 loss: 0.1151\n",
            "====> Test set loss: 10.4782\n",
            "====> Epoch: 113 loss: 0.1149\n",
            "====> Test set loss: 10.4759\n",
            "====> Epoch: 114 loss: 0.1070\n",
            "====> Test set loss: 10.4307\n",
            "====> Epoch: 115 loss: 0.1107\n",
            "====> Test set loss: 10.4027\n",
            "====> Epoch: 116 loss: 0.1017\n",
            "====> Test set loss: 10.3726\n",
            "====> Epoch: 117 loss: 0.0979\n",
            "====> Test set loss: 10.3606\n",
            "====> Epoch: 118 loss: 0.0996\n",
            "====> Test set loss: 10.3715\n",
            "====> Epoch: 119 loss: 0.0987\n",
            "====> Test set loss: 10.4163\n",
            "====> Epoch: 120 loss: 0.1163\n",
            "====> Test set loss: 10.2611\n",
            "====> Epoch: 121 loss: 0.1258\n",
            "====> Test set loss: 10.3037\n",
            "====> Epoch: 122 loss: 0.0965\n",
            "====> Test set loss: 10.2089\n",
            "====> Epoch: 123 loss: 0.0900\n",
            "====> Test set loss: 10.3410\n",
            "====> Epoch: 124 loss: 0.0996\n",
            "====> Test set loss: 10.3980\n",
            "====> Epoch: 125 loss: 0.1052\n",
            "====> Test set loss: 10.5216\n",
            "====> Epoch: 126 loss: 0.1028\n",
            "====> Test set loss: 10.3983\n",
            "====> Epoch: 127 loss: 0.1156\n",
            "====> Test set loss: 10.2687\n",
            "====> Epoch: 128 loss: 0.1004\n",
            "====> Test set loss: 10.5186\n",
            "====> Epoch: 129 loss: 0.0891\n",
            "====> Test set loss: 10.2930\n",
            "====> Epoch: 130 loss: 0.0816\n",
            "====> Test set loss: 10.4159\n",
            "====> Epoch: 131 loss: 0.0847\n",
            "====> Test set loss: 10.4755\n",
            "====> Epoch: 132 loss: 0.0934\n",
            "====> Test set loss: 10.3446\n",
            "====> Epoch: 133 loss: 0.1071\n",
            "====> Test set loss: 10.2661\n",
            "====> Epoch: 134 loss: 0.1001\n",
            "====> Test set loss: 10.2996\n",
            "====> Epoch: 135 loss: 0.0920\n",
            "====> Test set loss: 10.3368\n",
            "====> Epoch: 136 loss: 0.0970\n",
            "====> Test set loss: 10.2864\n",
            "====> Epoch: 137 loss: 0.0937\n",
            "====> Test set loss: 10.4680\n",
            "====> Epoch: 138 loss: 0.0931\n",
            "====> Test set loss: 10.3315\n",
            "====> Epoch: 139 loss: 0.0967\n",
            "====> Test set loss: 10.2181\n",
            "====> Epoch: 140 loss: 0.0907\n",
            "====> Test set loss: 10.3051\n",
            "====> Epoch: 141 loss: 0.0899\n",
            "====> Test set loss: 10.4218\n",
            "====> Epoch: 142 loss: 0.0876\n",
            "====> Test set loss: 10.4300\n",
            "====> Epoch: 143 loss: 0.0960\n",
            "====> Test set loss: 10.3750\n",
            "====> Epoch: 144 loss: 0.0966\n",
            "====> Test set loss: 10.3839\n",
            "====> Epoch: 145 loss: 0.1010\n",
            "====> Test set loss: 10.2773\n",
            "====> Epoch: 146 loss: 0.0870\n",
            "====> Test set loss: 10.1534\n",
            "====> Epoch: 147 loss: 0.0900\n",
            "====> Test set loss: 10.5012\n",
            "====> Epoch: 148 loss: 0.0829\n",
            "====> Test set loss: 10.2246\n",
            "====> Epoch: 149 loss: 0.0806\n",
            "====> Test set loss: 10.2033\n",
            "====> Epoch: 150 loss: 0.0900\n",
            "====> Test set loss: 10.2721\n",
            "====> Epoch: 151 loss: 0.0977\n",
            "====> Test set loss: 10.2581\n",
            "====> Epoch: 152 loss: 0.0933\n",
            "====> Test set loss: 10.1725\n",
            "====> Epoch: 153 loss: 0.0903\n",
            "====> Test set loss: 10.1854\n",
            "====> Epoch: 154 loss: 0.0844\n",
            "====> Test set loss: 10.1355\n",
            "====> Epoch: 155 loss: 0.0833\n",
            "====> Test set loss: 10.1191\n",
            "====> Epoch: 156 loss: 0.0819\n",
            "====> Test set loss: 10.1817\n",
            "====> Epoch: 157 loss: 0.0840\n",
            "====> Test set loss: 10.1285\n",
            "====> Epoch: 158 loss: 0.0817\n",
            "====> Test set loss: 10.2073\n",
            "====> Epoch: 159 loss: 0.0846\n",
            "====> Test set loss: 10.3411\n",
            "====> Epoch: 160 loss: 0.0844\n",
            "====> Test set loss: 10.3197\n",
            "====> Epoch: 161 loss: 0.0877\n",
            "====> Test set loss: 10.1417\n",
            "====> Epoch: 162 loss: 0.0791\n",
            "====> Test set loss: 10.2322\n",
            "====> Epoch: 163 loss: 0.0750\n",
            "====> Test set loss: 10.0322\n",
            "====> Epoch: 164 loss: 0.0873\n",
            "====> Test set loss: 10.4141\n",
            "====> Epoch: 165 loss: 0.0892\n",
            "====> Test set loss: 10.1966\n",
            "====> Epoch: 166 loss: 0.0899\n",
            "====> Test set loss: 10.0990\n",
            "====> Epoch: 167 loss: 0.0858\n",
            "====> Test set loss: 10.0971\n",
            "====> Epoch: 168 loss: 0.0811\n",
            "====> Test set loss: 10.2457\n",
            "====> Epoch: 169 loss: 0.0795\n",
            "====> Test set loss: 10.1592\n",
            "====> Epoch: 170 loss: 0.0713\n",
            "====> Test set loss: 10.1736\n",
            "====> Epoch: 171 loss: 0.0716\n",
            "====> Test set loss: 10.1965\n",
            "====> Epoch: 172 loss: 0.0732\n",
            "====> Test set loss: 10.2730\n",
            "====> Epoch: 173 loss: 0.0819\n",
            "====> Test set loss: 10.2555\n",
            "====> Epoch: 174 loss: 0.0896\n",
            "====> Test set loss: 10.1791\n",
            "====> Epoch: 175 loss: 0.0943\n",
            "====> Test set loss: 10.1036\n",
            "====> Epoch: 176 loss: 0.0822\n",
            "====> Test set loss: 10.0053\n",
            "====> Epoch: 177 loss: 0.0686\n",
            "====> Test set loss: 10.1433\n",
            "====> Epoch: 178 loss: 0.0696\n",
            "====> Test set loss: 10.0980\n",
            "====> Epoch: 179 loss: 0.0738\n",
            "====> Test set loss: 10.2259\n",
            "====> Epoch: 180 loss: 0.0757\n",
            "====> Test set loss: 10.0810\n",
            "====> Epoch: 181 loss: 0.0934\n",
            "====> Test set loss: 10.1835\n",
            "====> Epoch: 182 loss: 0.0913\n",
            "====> Test set loss: 10.1270\n",
            "====> Epoch: 183 loss: 0.0776\n",
            "====> Test set loss: 10.0901\n",
            "====> Epoch: 184 loss: 0.0648\n",
            "====> Test set loss: 10.1412\n",
            "====> Epoch: 185 loss: 0.0660\n",
            "====> Test set loss: 10.1379\n",
            "====> Epoch: 186 loss: 0.0668\n",
            "====> Test set loss: 10.3239\n",
            "====> Epoch: 187 loss: 0.0774\n",
            "====> Test set loss: 10.1717\n",
            "====> Epoch: 188 loss: 0.0788\n",
            "====> Test set loss: 9.9938\n",
            "====> Epoch: 189 loss: 0.0808\n",
            "====> Test set loss: 10.1465\n",
            "====> Epoch: 190 loss: 0.0715\n",
            "====> Test set loss: 10.0063\n",
            "====> Epoch: 191 loss: 0.0711\n",
            "====> Test set loss: 10.0572\n",
            "====> Epoch: 192 loss: 0.0724\n",
            "====> Test set loss: 10.1700\n",
            "====> Epoch: 193 loss: 0.0701\n",
            "====> Test set loss: 10.0723\n",
            "====> Epoch: 194 loss: 0.0718\n",
            "====> Test set loss: 10.1157\n",
            "====> Epoch: 195 loss: 0.0752\n",
            "====> Test set loss: 9.9380\n",
            "====> Epoch: 196 loss: 0.0733\n",
            "====> Test set loss: 10.1615\n",
            "====> Epoch: 197 loss: 0.0755\n",
            "====> Test set loss: 10.0453\n",
            "====> Epoch: 198 loss: 0.0792\n",
            "====> Test set loss: 10.2544\n",
            "====> Epoch: 199 loss: 0.0743\n",
            "====> Test set loss: 10.0745\n",
            "====> Epoch: 200 loss: 0.0686\n",
            "====> Test set loss: 10.1756\n",
            "====> Epoch: 201 loss: 0.0615\n",
            "====> Test set loss: 9.9450\n",
            "====> Epoch: 202 loss: 0.0682\n",
            "====> Test set loss: 9.9692\n",
            "====> Epoch: 203 loss: 0.0760\n",
            "====> Test set loss: 10.2720\n",
            "====> Epoch: 204 loss: 0.0721\n",
            "====> Test set loss: 10.0455\n",
            "====> Epoch: 205 loss: 0.0712\n",
            "====> Test set loss: 9.9935\n",
            "====> Epoch: 206 loss: 0.0740\n",
            "====> Test set loss: 10.0474\n",
            "====> Epoch: 207 loss: 0.0790\n",
            "====> Test set loss: 10.0524\n",
            "====> Epoch: 208 loss: 0.0715\n",
            "====> Test set loss: 10.0524\n",
            "====> Epoch: 209 loss: 0.0701\n",
            "====> Test set loss: 9.9625\n",
            "====> Epoch: 210 loss: 0.0658\n",
            "====> Test set loss: 10.1259\n",
            "====> Epoch: 211 loss: 0.0658\n",
            "====> Test set loss: 10.0543\n",
            "====> Epoch: 212 loss: 0.0584\n",
            "====> Test set loss: 10.1507\n",
            "====> Epoch: 213 loss: 0.0600\n",
            "====> Test set loss: 10.0281\n",
            "====> Epoch: 214 loss: 0.0635\n",
            "====> Test set loss: 10.1506\n",
            "====> Epoch: 215 loss: 0.0696\n",
            "====> Test set loss: 10.2346\n",
            "====> Epoch: 216 loss: 0.0779\n",
            "====> Test set loss: 10.1897\n",
            "====> Epoch: 217 loss: 0.0698\n",
            "====> Test set loss: 10.0531\n",
            "====> Epoch: 218 loss: 0.0649\n",
            "====> Test set loss: 9.9700\n",
            "====> Epoch: 219 loss: 0.0622\n",
            "====> Test set loss: 10.0636\n",
            "====> Epoch: 220 loss: 0.0642\n",
            "====> Test set loss: 10.0706\n",
            "====> Epoch: 221 loss: 0.0683\n",
            "====> Test set loss: 10.1234\n",
            "====> Epoch: 222 loss: 0.0697\n",
            "====> Test set loss: 9.9174\n",
            "====> Epoch: 223 loss: 0.0584\n",
            "====> Test set loss: 10.1703\n",
            "====> Epoch: 224 loss: 0.0746\n",
            "====> Test set loss: 9.9800\n",
            "====> Epoch: 225 loss: 0.0802\n",
            "====> Test set loss: 10.2318\n",
            "====> Epoch: 226 loss: 0.0653\n",
            "====> Test set loss: 10.0991\n",
            "====> Epoch: 227 loss: 0.0526\n",
            "====> Test set loss: 10.0000\n",
            "====> Epoch: 228 loss: 0.0555\n",
            "====> Test set loss: 10.0806\n",
            "====> Epoch: 229 loss: 0.0667\n",
            "====> Test set loss: 9.9105\n",
            "====> Epoch: 230 loss: 0.0648\n",
            "====> Test set loss: 10.2062\n",
            "====> Epoch: 231 loss: 0.0639\n",
            "====> Test set loss: 10.0261\n",
            "====> Epoch: 232 loss: 0.0609\n",
            "====> Test set loss: 10.0264\n",
            "====> Epoch: 233 loss: 0.0630\n",
            "====> Test set loss: 9.9483\n",
            "====> Epoch: 234 loss: 0.0564\n",
            "====> Test set loss: 9.9032\n",
            "====> Epoch: 235 loss: 0.0585\n",
            "====> Test set loss: 9.9528\n",
            "====> Epoch: 236 loss: 0.0596\n",
            "====> Test set loss: 10.1705\n",
            "====> Epoch: 237 loss: 0.0672\n",
            "====> Test set loss: 9.8366\n",
            "====> Epoch: 238 loss: 0.0690\n",
            "====> Test set loss: 9.8813\n",
            "====> Epoch: 239 loss: 0.0634\n",
            "====> Test set loss: 9.9274\n",
            "====> Epoch: 240 loss: 0.0579\n",
            "====> Test set loss: 10.0165\n",
            "====> Epoch: 241 loss: 0.0536\n",
            "====> Test set loss: 9.9671\n",
            "====> Epoch: 242 loss: 0.0550\n",
            "====> Test set loss: 10.2811\n",
            "====> Epoch: 243 loss: 0.0659\n",
            "====> Test set loss: 9.8491\n",
            "====> Epoch: 244 loss: 0.0676\n",
            "====> Test set loss: 10.2444\n",
            "====> Epoch: 245 loss: 0.0654\n",
            "====> Test set loss: 9.9999\n",
            "====> Epoch: 246 loss: 0.0666\n",
            "====> Test set loss: 10.0318\n",
            "====> Epoch: 247 loss: 0.0665\n",
            "====> Test set loss: 9.9627\n",
            "====> Epoch: 248 loss: 0.0651\n",
            "====> Test set loss: 9.8182\n",
            "====> Epoch: 249 loss: 0.0574\n",
            "====> Test set loss: 9.8785\n",
            "====> Epoch: 250 loss: 0.0557\n",
            "====> Test set loss: 10.0745\n",
            "====> Epoch: 251 loss: 0.0596\n",
            "====> Test set loss: 10.0202\n",
            "====> Epoch: 252 loss: 0.0640\n",
            "====> Test set loss: 10.0460\n",
            "====> Epoch: 253 loss: 0.0566\n",
            "====> Test set loss: 10.0733\n",
            "====> Epoch: 254 loss: 0.0537\n",
            "====> Test set loss: 9.9081\n",
            "====> Epoch: 255 loss: 0.0562\n",
            "====> Test set loss: 9.9939\n",
            "====> Epoch: 256 loss: 0.0567\n",
            "====> Test set loss: 9.9400\n",
            "====> Epoch: 257 loss: 0.0607\n",
            "====> Test set loss: 9.9495\n",
            "====> Epoch: 258 loss: 0.0582\n",
            "====> Test set loss: 9.9761\n",
            "====> Epoch: 259 loss: 0.0629\n",
            "====> Test set loss: 9.8870\n",
            "====> Epoch: 260 loss: 0.0609\n",
            "====> Test set loss: 10.0358\n",
            "====> Epoch: 261 loss: 0.0652\n",
            "====> Test set loss: 9.8525\n",
            "====> Epoch: 262 loss: 0.0541\n",
            "====> Test set loss: 9.9286\n",
            "====> Epoch: 263 loss: 0.0457\n",
            "====> Test set loss: 10.0635\n",
            "====> Epoch: 264 loss: 0.0451\n",
            "====> Test set loss: 9.8508\n",
            "====> Epoch: 265 loss: 0.0538\n",
            "====> Test set loss: 9.9678\n",
            "====> Epoch: 266 loss: 0.0646\n",
            "====> Test set loss: 10.0468\n",
            "====> Epoch: 267 loss: 0.0795\n",
            "====> Test set loss: 9.8658\n",
            "====> Epoch: 268 loss: 0.0736\n",
            "====> Test set loss: 9.9753\n",
            "====> Epoch: 269 loss: 0.0545\n",
            "====> Test set loss: 9.9707\n",
            "====> Epoch: 270 loss: 0.0455\n",
            "====> Test set loss: 9.8806\n",
            "====> Epoch: 271 loss: 0.0469\n",
            "====> Test set loss: 9.8964\n",
            "====> Epoch: 272 loss: 0.0526\n",
            "====> Test set loss: 10.0095\n",
            "====> Epoch: 273 loss: 0.0566\n",
            "====> Test set loss: 10.0266\n",
            "====> Epoch: 274 loss: 0.0511\n",
            "====> Test set loss: 9.9863\n",
            "====> Epoch: 275 loss: 0.0533\n",
            "====> Test set loss: 9.9079\n",
            "====> Epoch: 276 loss: 0.0536\n",
            "====> Test set loss: 10.0211\n",
            "====> Epoch: 277 loss: 0.0554\n",
            "====> Test set loss: 9.7621\n",
            "====> Epoch: 278 loss: 0.0570\n",
            "====> Test set loss: 10.1419\n",
            "====> Epoch: 279 loss: 0.0592\n",
            "====> Test set loss: 9.9240\n",
            "====> Epoch: 280 loss: 0.0672\n",
            "====> Test set loss: 9.9627\n",
            "====> Epoch: 281 loss: 0.0655\n",
            "====> Test set loss: 9.9416\n",
            "====> Epoch: 282 loss: 0.0586\n",
            "====> Test set loss: 10.0929\n",
            "====> Epoch: 283 loss: 0.0598\n",
            "====> Test set loss: 9.9220\n",
            "====> Epoch: 284 loss: 0.0471\n",
            "====> Test set loss: 9.9181\n",
            "====> Epoch: 285 loss: 0.0425\n",
            "====> Test set loss: 9.9104\n",
            "====> Epoch: 286 loss: 0.0498\n",
            "====> Test set loss: 9.9456\n",
            "====> Epoch: 287 loss: 0.0609\n",
            "====> Test set loss: 9.8728\n",
            "====> Epoch: 288 loss: 0.0625\n",
            "====> Test set loss: 10.0144\n",
            "====> Epoch: 289 loss: 0.0522\n",
            "====> Test set loss: 9.8290\n",
            "====> Epoch: 290 loss: 0.0492\n",
            "====> Test set loss: 10.0370\n",
            "====> Epoch: 291 loss: 0.0487\n",
            "====> Test set loss: 10.0247\n",
            "====> Epoch: 292 loss: 0.0523\n",
            "====> Test set loss: 9.8080\n",
            "====> Epoch: 293 loss: 0.0530\n",
            "====> Test set loss: 9.9806\n",
            "====> Epoch: 294 loss: 0.0538\n",
            "====> Test set loss: 9.7886\n",
            "====> Epoch: 295 loss: 0.0519\n",
            "====> Test set loss: 9.8588\n",
            "====> Epoch: 296 loss: 0.0650\n",
            "====> Test set loss: 9.8523\n",
            "====> Epoch: 297 loss: 0.0572\n",
            "====> Test set loss: 9.9685\n",
            "====> Epoch: 298 loss: 0.0476\n",
            "====> Test set loss: 9.9399\n",
            "====> Epoch: 299 loss: 0.0520\n",
            "====> Test set loss: 9.8806\n",
            "====> Epoch: 300 loss: 0.0449\n",
            "====> Test set loss: 9.9695\n",
            "====> Epoch: 301 loss: 0.0474\n",
            "====> Test set loss: 10.0346\n",
            "====> Epoch: 302 loss: 0.0555\n",
            "====> Test set loss: 9.8277\n",
            "====> Epoch: 303 loss: 0.0577\n",
            "====> Test set loss: 9.9408\n",
            "====> Epoch: 304 loss: 0.0594\n",
            "====> Test set loss: 9.9476\n",
            "====> Epoch: 305 loss: 0.0506\n",
            "====> Test set loss: 9.9208\n",
            "====> Epoch: 306 loss: 0.0466\n",
            "====> Test set loss: 10.0432\n",
            "====> Epoch: 307 loss: 0.0423\n",
            "====> Test set loss: 9.9612\n",
            "====> Epoch: 308 loss: 0.0435\n",
            "====> Test set loss: 9.8062\n",
            "====> Epoch: 309 loss: 0.0477\n",
            "====> Test set loss: 9.8843\n",
            "====> Epoch: 310 loss: 0.0569\n",
            "====> Test set loss: 10.0569\n",
            "====> Epoch: 311 loss: 0.0569\n",
            "====> Test set loss: 9.8679\n",
            "====> Epoch: 312 loss: 0.0546\n",
            "====> Test set loss: 9.8225\n",
            "====> Epoch: 313 loss: 0.0481\n",
            "====> Test set loss: 9.8431\n",
            "====> Epoch: 314 loss: 0.0441\n",
            "====> Test set loss: 9.9542\n",
            "====> Epoch: 315 loss: 0.0425\n",
            "====> Test set loss: 9.8752\n",
            "====> Epoch: 316 loss: 0.0490\n",
            "====> Test set loss: 9.9407\n",
            "====> Epoch: 317 loss: 0.0499\n",
            "====> Test set loss: 9.8831\n",
            "====> Epoch: 318 loss: 0.0539\n",
            "====> Test set loss: 9.8976\n",
            "====> Epoch: 319 loss: 0.0545\n",
            "====> Test set loss: 9.8098\n",
            "====> Epoch: 320 loss: 0.0492\n",
            "====> Test set loss: 9.8807\n",
            "====> Epoch: 321 loss: 0.0475\n",
            "====> Test set loss: 9.8985\n",
            "====> Epoch: 322 loss: 0.0489\n",
            "====> Test set loss: 9.9537\n",
            "====> Epoch: 323 loss: 0.0454\n",
            "====> Test set loss: 9.8748\n",
            "====> Epoch: 324 loss: 0.0439\n",
            "====> Test set loss: 9.9286\n",
            "====> Epoch: 325 loss: 0.0460\n",
            "====> Test set loss: 9.8891\n",
            "====> Epoch: 326 loss: 0.0502\n",
            "====> Test set loss: 9.9010\n",
            "====> Epoch: 327 loss: 0.0505\n",
            "====> Test set loss: 9.8800\n",
            "====> Epoch: 328 loss: 0.0560\n",
            "====> Test set loss: 10.0542\n",
            "====> Epoch: 329 loss: 0.0569\n",
            "====> Test set loss: 9.9302\n",
            "====> Epoch: 330 loss: 0.0473\n",
            "====> Test set loss: 9.9446\n",
            "====> Epoch: 331 loss: 0.0414\n",
            "====> Test set loss: 9.9102\n",
            "====> Epoch: 332 loss: 0.0387\n",
            "====> Test set loss: 9.9268\n",
            "====> Epoch: 333 loss: 0.0439\n",
            "====> Test set loss: 9.9236\n",
            "====> Epoch: 334 loss: 0.0486\n",
            "====> Test set loss: 9.9440\n",
            "====> Epoch: 335 loss: 0.0577\n",
            "====> Test set loss: 9.8493\n",
            "====> Epoch: 336 loss: 0.0517\n",
            "====> Test set loss: 9.8465\n",
            "====> Epoch: 337 loss: 0.0422\n",
            "====> Test set loss: 9.8023\n",
            "====> Epoch: 338 loss: 0.0386\n",
            "====> Test set loss: 9.8169\n",
            "====> Epoch: 339 loss: 0.0404\n",
            "====> Test set loss: 9.8942\n",
            "====> Epoch: 340 loss: 0.0503\n",
            "====> Test set loss: 9.8574\n",
            "====> Epoch: 341 loss: 0.0502\n",
            "====> Test set loss: 9.9290\n",
            "====> Epoch: 342 loss: 0.0524\n",
            "====> Test set loss: 9.8651\n",
            "====> Epoch: 343 loss: 0.0479\n",
            "====> Test set loss: 9.8028\n",
            "====> Epoch: 344 loss: 0.0455\n",
            "====> Test set loss: 9.9564\n",
            "====> Epoch: 345 loss: 0.0430\n",
            "====> Test set loss: 9.8765\n",
            "====> Epoch: 346 loss: 0.0414\n",
            "====> Test set loss: 9.7903\n",
            "====> Epoch: 347 loss: 0.0390\n",
            "====> Test set loss: 9.8838\n",
            "====> Epoch: 348 loss: 0.0524\n",
            "====> Test set loss: 9.7415\n",
            "====> Epoch: 349 loss: 0.0578\n",
            "====> Test set loss: 9.9370\n",
            "====> Epoch: 350 loss: 0.0476\n",
            "====> Test set loss: 10.0513\n",
            "====> Epoch: 351 loss: 0.0432\n",
            "====> Test set loss: 9.9729\n",
            "====> Epoch: 352 loss: 0.0441\n",
            "====> Test set loss: 9.9675\n",
            "====> Epoch: 353 loss: 0.0436\n",
            "====> Test set loss: 9.9023\n",
            "====> Epoch: 354 loss: 0.0456\n",
            "====> Test set loss: 9.8610\n",
            "====> Epoch: 355 loss: 0.0449\n",
            "====> Test set loss: 9.8440\n",
            "====> Epoch: 356 loss: 0.0410\n",
            "====> Test set loss: 9.8504\n",
            "====> Epoch: 357 loss: 0.0399\n",
            "====> Test set loss: 9.9016\n",
            "====> Epoch: 358 loss: 0.0393\n",
            "====> Test set loss: 9.8821\n",
            "====> Epoch: 359 loss: 0.0429\n",
            "====> Test set loss: 9.9176\n",
            "====> Epoch: 360 loss: 0.0480\n",
            "====> Test set loss: 9.8517\n",
            "====> Epoch: 361 loss: 0.0461\n",
            "====> Test set loss: 9.8945\n",
            "====> Epoch: 362 loss: 0.0428\n",
            "====> Test set loss: 10.1138\n",
            "====> Epoch: 363 loss: 0.0443\n",
            "====> Test set loss: 9.7619\n",
            "====> Epoch: 364 loss: 0.0481\n",
            "====> Test set loss: 9.8429\n",
            "====> Epoch: 365 loss: 0.0454\n",
            "====> Test set loss: 9.8416\n",
            "====> Epoch: 366 loss: 0.0495\n",
            "====> Test set loss: 10.1168\n",
            "====> Epoch: 367 loss: 0.0486\n",
            "====> Test set loss: 9.7938\n",
            "====> Epoch: 368 loss: 0.0381\n",
            "====> Test set loss: 9.8420\n",
            "====> Epoch: 369 loss: 0.0352\n",
            "====> Test set loss: 9.8537\n",
            "====> Epoch: 370 loss: 0.0389\n",
            "====> Test set loss: 9.9189\n",
            "====> Epoch: 371 loss: 0.0463\n",
            "====> Test set loss: 9.8553\n",
            "====> Epoch: 372 loss: 0.0509\n",
            "====> Test set loss: 9.9501\n",
            "====> Epoch: 373 loss: 0.0407\n",
            "====> Test set loss: 9.9450\n",
            "====> Epoch: 374 loss: 0.0361\n",
            "====> Test set loss: 9.9423\n",
            "====> Epoch: 375 loss: 0.0471\n",
            "====> Test set loss: 9.9863\n",
            "====> Epoch: 376 loss: 0.0473\n",
            "====> Test set loss: 9.8557\n",
            "====> Epoch: 377 loss: 0.0454\n",
            "====> Test set loss: 10.0240\n",
            "====> Epoch: 378 loss: 0.0386\n",
            "====> Test set loss: 9.8122\n",
            "====> Epoch: 379 loss: 0.0399\n",
            "====> Test set loss: 9.8802\n",
            "====> Epoch: 380 loss: 0.0414\n",
            "====> Test set loss: 9.8646\n",
            "====> Epoch: 381 loss: 0.0476\n",
            "====> Test set loss: 10.0310\n",
            "====> Epoch: 382 loss: 0.0453\n",
            "====> Test set loss: 9.9315\n",
            "====> Epoch: 383 loss: 0.0479\n",
            "====> Test set loss: 9.9272\n",
            "====> Epoch: 384 loss: 0.0455\n",
            "====> Test set loss: 10.0080\n",
            "====> Epoch: 385 loss: 0.0424\n",
            "====> Test set loss: 9.8792\n",
            "====> Epoch: 386 loss: 0.0420\n",
            "====> Test set loss: 9.8158\n",
            "====> Epoch: 387 loss: 0.0377\n",
            "====> Test set loss: 9.9407\n",
            "====> Epoch: 388 loss: 0.0389\n",
            "====> Test set loss: 9.9481\n",
            "====> Epoch: 389 loss: 0.0437\n",
            "====> Test set loss: 9.8343\n",
            "====> Epoch: 390 loss: 0.0469\n",
            "====> Test set loss: 10.0347\n",
            "====> Epoch: 391 loss: 0.0416\n",
            "====> Test set loss: 9.8465\n",
            "====> Epoch: 392 loss: 0.0429\n",
            "====> Test set loss: 9.8530\n",
            "====> Epoch: 393 loss: 0.0388\n",
            "====> Test set loss: 9.8391\n",
            "====> Epoch: 394 loss: 0.0385\n",
            "====> Test set loss: 9.8780\n",
            "====> Epoch: 395 loss: 0.0443\n",
            "====> Test set loss: 9.7254\n",
            "====> Epoch: 396 loss: 0.0445\n",
            "====> Test set loss: 10.0320\n",
            "====> Epoch: 397 loss: 0.0397\n",
            "====> Test set loss: 9.8020\n",
            "====> Epoch: 398 loss: 0.0338\n",
            "====> Test set loss: 9.9552\n",
            "====> Epoch: 399 loss: 0.0377\n",
            "====> Test set loss: 9.9904\n",
            "====> Epoch: 400 loss: 0.0520\n",
            "====> Test set loss: 9.8151\n",
            "====> Epoch: 401 loss: 0.0460\n",
            "====> Test set loss: 9.7295\n",
            "====> Epoch: 402 loss: 0.0379\n",
            "====> Test set loss: 9.9203\n",
            "====> Epoch: 403 loss: 0.0367\n",
            "====> Test set loss: 9.8717\n",
            "====> Epoch: 404 loss: 0.0374\n",
            "====> Test set loss: 9.8640\n",
            "====> Epoch: 405 loss: 0.0383\n",
            "====> Test set loss: 9.9531\n",
            "====> Epoch: 406 loss: 0.0394\n",
            "====> Test set loss: 9.8503\n",
            "====> Epoch: 407 loss: 0.0440\n",
            "====> Test set loss: 9.7982\n",
            "====> Epoch: 408 loss: 0.0426\n",
            "====> Test set loss: 9.8158\n",
            "====> Epoch: 409 loss: 0.0382\n",
            "====> Test set loss: 9.7895\n",
            "====> Epoch: 410 loss: 0.0393\n",
            "====> Test set loss: 9.8983\n",
            "====> Epoch: 411 loss: 0.0411\n",
            "====> Test set loss: 9.7606\n",
            "====> Epoch: 412 loss: 0.0410\n",
            "====> Test set loss: 9.7421\n",
            "====> Epoch: 413 loss: 0.0397\n",
            "====> Test set loss: 9.8166\n",
            "====> Epoch: 414 loss: 0.0373\n",
            "====> Test set loss: 9.8115\n",
            "====> Epoch: 415 loss: 0.0336\n",
            "====> Test set loss: 9.7899\n",
            "====> Epoch: 416 loss: 0.0412\n",
            "====> Test set loss: 9.9967\n",
            "====> Epoch: 417 loss: 0.0476\n",
            "====> Test set loss: 9.8679\n",
            "====> Epoch: 418 loss: 0.0479\n",
            "====> Test set loss: 9.8238\n",
            "====> Epoch: 419 loss: 0.0481\n",
            "====> Test set loss: 9.9623\n",
            "====> Epoch: 420 loss: 0.0408\n",
            "====> Test set loss: 9.8778\n",
            "====> Epoch: 421 loss: 0.0336\n",
            "====> Test set loss: 9.8371\n",
            "====> Epoch: 422 loss: 0.0318\n",
            "====> Test set loss: 9.8821\n",
            "====> Epoch: 423 loss: 0.0321\n",
            "====> Test set loss: 9.8033\n",
            "====> Epoch: 424 loss: 0.0401\n",
            "====> Test set loss: 9.8884\n",
            "====> Epoch: 425 loss: 0.0433\n",
            "====> Test set loss: 9.7391\n",
            "====> Epoch: 426 loss: 0.0399\n",
            "====> Test set loss: 9.8279\n",
            "====> Epoch: 427 loss: 0.0385\n",
            "====> Test set loss: 9.7304\n",
            "====> Epoch: 428 loss: 0.0390\n",
            "====> Test set loss: 10.0025\n",
            "====> Epoch: 429 loss: 0.0376\n",
            "====> Test set loss: 9.9388\n",
            "====> Epoch: 430 loss: 0.0366\n",
            "====> Test set loss: 9.7136\n",
            "====> Epoch: 431 loss: 0.0339\n",
            "====> Test set loss: 9.8730\n",
            "====> Epoch: 432 loss: 0.0408\n",
            "====> Test set loss: 9.7805\n",
            "====> Epoch: 433 loss: 0.0440\n",
            "====> Test set loss: 9.7367\n",
            "====> Epoch: 434 loss: 0.0400\n",
            "====> Test set loss: 9.8007\n",
            "====> Epoch: 435 loss: 0.0451\n",
            "====> Test set loss: 9.9267\n",
            "====> Epoch: 436 loss: 0.0391\n",
            "====> Test set loss: 9.8689\n",
            "====> Epoch: 437 loss: 0.0309\n",
            "====> Test set loss: 9.8153\n",
            "====> Epoch: 438 loss: 0.0364\n",
            "====> Test set loss: 9.8864\n",
            "====> Epoch: 439 loss: 0.0448\n",
            "====> Test set loss: 9.8352\n",
            "====> Epoch: 440 loss: 0.0397\n",
            "====> Test set loss: 9.8406\n",
            "====> Epoch: 441 loss: 0.0320\n",
            "====> Test set loss: 9.8318\n",
            "====> Epoch: 442 loss: 0.0335\n",
            "====> Test set loss: 9.8180\n",
            "====> Epoch: 443 loss: 0.0383\n",
            "====> Test set loss: 9.8789\n",
            "====> Epoch: 444 loss: 0.0394\n",
            "====> Test set loss: 9.8753\n",
            "====> Epoch: 445 loss: 0.0369\n",
            "====> Test set loss: 9.7966\n",
            "====> Epoch: 446 loss: 0.0379\n",
            "====> Test set loss: 9.7827\n",
            "====> Epoch: 447 loss: 0.0382\n",
            "====> Test set loss: 9.8679\n",
            "====> Epoch: 448 loss: 0.0352\n",
            "====> Test set loss: 9.7430\n",
            "====> Epoch: 449 loss: 0.0363\n",
            "====> Test set loss: 9.9292\n",
            "====> Epoch: 450 loss: 0.0452\n",
            "====> Test set loss: 9.7145\n",
            "====> Epoch: 451 loss: 0.0461\n",
            "====> Test set loss: 9.9453\n",
            "====> Epoch: 452 loss: 0.0388\n",
            "====> Test set loss: 9.7993\n",
            "====> Epoch: 453 loss: 0.0337\n",
            "====> Test set loss: 9.9823\n",
            "====> Epoch: 454 loss: 0.0286\n",
            "====> Test set loss: 9.7689\n",
            "====> Epoch: 455 loss: 0.0272\n",
            "====> Test set loss: 9.8104\n",
            "====> Epoch: 456 loss: 0.0347\n",
            "====> Test set loss: 9.8564\n",
            "====> Epoch: 457 loss: 0.0404\n",
            "====> Test set loss: 9.8251\n",
            "====> Epoch: 458 loss: 0.0443\n",
            "====> Test set loss: 9.9379\n",
            "====> Epoch: 459 loss: 0.0426\n",
            "====> Test set loss: 9.7710\n",
            "====> Epoch: 460 loss: 0.0337\n",
            "====> Test set loss: 9.8657\n",
            "====> Epoch: 461 loss: 0.0304\n",
            "====> Test set loss: 9.9342\n",
            "====> Epoch: 462 loss: 0.0333\n",
            "====> Test set loss: 9.7767\n",
            "====> Epoch: 463 loss: 0.0345\n",
            "====> Test set loss: 9.9367\n",
            "====> Epoch: 464 loss: 0.0394\n",
            "====> Test set loss: 9.8866\n",
            "====> Epoch: 465 loss: 0.0436\n",
            "====> Test set loss: 9.9135\n",
            "====> Epoch: 466 loss: 0.0436\n",
            "====> Test set loss: 9.7393\n",
            "====> Epoch: 467 loss: 0.0374\n",
            "====> Test set loss: 9.8077\n",
            "====> Epoch: 468 loss: 0.0330\n",
            "====> Test set loss: 9.6786\n",
            "====> Epoch: 469 loss: 0.0297\n",
            "====> Test set loss: 9.9710\n",
            "====> Epoch: 470 loss: 0.0339\n",
            "====> Test set loss: 9.7005\n",
            "====> Epoch: 471 loss: 0.0359\n",
            "====> Test set loss: 9.8833\n",
            "====> Epoch: 472 loss: 0.0386\n",
            "====> Test set loss: 9.8250\n",
            "====> Epoch: 473 loss: 0.0357\n",
            "====> Test set loss: 9.7568\n",
            "====> Epoch: 474 loss: 0.0368\n",
            "====> Test set loss: 9.7497\n",
            "====> Epoch: 475 loss: 0.0358\n",
            "====> Test set loss: 9.8127\n",
            "====> Epoch: 476 loss: 0.0312\n",
            "====> Test set loss: 9.7713\n",
            "====> Epoch: 477 loss: 0.0304\n",
            "====> Test set loss: 9.8213\n",
            "====> Epoch: 478 loss: 0.0313\n",
            "====> Test set loss: 9.8159\n",
            "====> Epoch: 479 loss: 0.0329\n",
            "====> Test set loss: 9.8262\n",
            "====> Epoch: 480 loss: 0.0358\n",
            "====> Test set loss: 9.7968\n",
            "====> Epoch: 481 loss: 0.0379\n",
            "====> Test set loss: 9.7638\n",
            "====> Epoch: 482 loss: 0.0409\n",
            "====> Test set loss: 9.7774\n",
            "====> Epoch: 483 loss: 0.0365\n",
            "====> Test set loss: 9.9117\n",
            "====> Epoch: 484 loss: 0.0344\n",
            "====> Test set loss: 9.7602\n",
            "====> Epoch: 485 loss: 0.0326\n",
            "====> Test set loss: 9.9168\n",
            "====> Epoch: 486 loss: 0.0376\n",
            "====> Test set loss: 9.7770\n",
            "====> Epoch: 487 loss: 0.0395\n",
            "====> Test set loss: 9.8796\n",
            "====> Epoch: 488 loss: 0.0318\n",
            "====> Test set loss: 9.8673\n",
            "====> Epoch: 489 loss: 0.0265\n",
            "====> Test set loss: 9.8008\n",
            "====> Epoch: 490 loss: 0.0296\n",
            "====> Test set loss: 9.7902\n",
            "====> Epoch: 491 loss: 0.0376\n",
            "====> Test set loss: 9.8572\n",
            "====> Epoch: 492 loss: 0.0402\n",
            "====> Test set loss: 9.8515\n",
            "====> Epoch: 493 loss: 0.0393\n",
            "====> Test set loss: 9.7400\n",
            "====> Epoch: 494 loss: 0.0358\n",
            "====> Test set loss: 9.9370\n",
            "====> Epoch: 495 loss: 0.0295\n",
            "====> Test set loss: 9.8928\n",
            "====> Epoch: 496 loss: 0.0248\n",
            "====> Test set loss: 9.8250\n",
            "====> Epoch: 497 loss: 0.0284\n",
            "====> Test set loss: 9.7563\n",
            "====> Epoch: 498 loss: 0.0352\n",
            "====> Test set loss: 9.7889\n",
            "====> Epoch: 499 loss: 0.0419\n",
            "====> Test set loss: 9.9347\n",
            "====> Epoch: 500 loss: 0.0470\n",
            "====> Test set loss: 9.7036\n",
            "====> Test set loss: 9.7036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCHhgcQWLHj4"
      },
      "source": [
        "y4 = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "0bmv6o9SLJyT",
        "outputId": "65395380-d47a-437e-e59b-4b080f6cff4f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f521763e650>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWhNPgubLKhg",
        "outputId": "a7a33e7a-4486-4a63-f3b7-03a3048164ea"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o.squeeze(), y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.1349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9sN8aiPQnEJ"
      },
      "source": [
        "# R square\n",
        "o=model(x_val.cuda())\n",
        "r2(o.squeeze(), y_val.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKDQxV7ePjAC"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYR3QtqVLLUy",
        "outputId": "d068f433-074c-4434-c532-78774f5b4097"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zd4YGXLeR5o",
        "outputId": "6e7d16ba-ce1c-4a2d-b418-72a39e3754b7"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93569"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "raD3iaJPPn1K",
        "outputId": "0a71ede1-5d8b-4cab-88a3-e5fb7df69fe1"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 500\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-7152d1477002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-cea082b7058d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_losses, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "n2_80ow2Pp-r",
        "outputId": "20d0ab70-bcf4-4859-b2a2-92953786396b"
      },
      "source": [
        "y5 = y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1df15f889851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "8mYAg-EQPrc_",
        "outputId": "0dd3492f-a483-434b-eabe-8e5b2cf656dc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa4b13e5e90>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIwAAAFNCAYAAABi2vQZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZjkZ13v/c9d+9JVve/dM92zTyb7HhIgRFDAgKBg0IgREY5eqOByFM/xOcIjKuojKh4Ro8ATkQg5gTxhi0LIRkImYTLZZt+X3veufb+fP37Vle7pmclMMl1Vk3m/rquv6tp+9a2q7pnuT3+/922stQIAAAAAAAAWuGpdAAAAAAAAAOoLgREAAAAAAACWIDACAAAAAADAEgRGAAAAAAAAWILACAAAAAAAAEsQGAEAAAAAAGAJAiMAAHDOGGMeMMbcca5vW0vGmCPGmDevwHEfMcb8Wvnz240x3zuT276Cx1lljEkYY9yvtFYAAHDhITACAOACVw4TFj5Kxpj0ovO3n82xrLVvs9beda5vW4+MMR83xjx2ksvbjDE5Y8zFZ3osa+1XrLU/eY7qWhJwWWuPWWsbrLXFc3H8Ex7LGmPWnevjAgCA2iMwAgDgAlcOExqstQ2Sjkl6x6LLvrJwO2OMp3ZV1qV/l/Q6Y8zgCZe/T9KL1todNagJAADgnCAwAgAAJ2WMudkYM2SM+UNjzJikLxljmo0x3zbGTBpjZsuf9y26z+Ixq18xxjxujPl/yrc9bIx52yu87aAx5jFjTNwY86Ax5h+NMf9+irrPpMY/NcY8UT7e94wxbYuuf78x5qgxZtoY8z9P9fpYa4ckPSTp/Sdc9cuS/u3l6jih5l8xxjy+6PxbjDF7jDHzxpj/Lcksum6tMeahcn1TxpivGGOaytd9WdIqSd8qd4j9gTFmoNwJ5CnfpscY801jzIwx5oAx5kOLjv0JY8w9xph/K782O40xV5/qNTgVY0xj+RiT5dfyj40xrvJ164wxj5af25Qx5mvly40x5m+NMRPGmJgx5sWz6dICAADnFoERAAA4nS5JLZJWS/qwnJ8dvlQ+v0pSWtL/Ps39r5O0V1KbpL+S9AVjjHkFt71b0tOSWiV9QstDmsXOpMZflPQBSR2SfJJ+X5KMMRdJ+qfy8XvKj3fSkKfsrsW1GGM2Srq8XO/ZvlYLx2iT9A1JfyzntTgo6cbFN5H0F+X6Nkvql/OayFr7fi3tEvurkzzEVyUNle//Hkl/boy5ZdH17yzfpknSN8+k5pP4B0mNktZIeqOcEO0D5ev+VNL3JDXLeW3/oXz5T0p6g6QN5fv+vKTpV/DYAADgHCAwAgAAp1OS9CfW2qy1Nm2tnbbWft1am7LWxiX9mZxA4FSOWmv/pbx+zl2SuiV1ns1tjTGrJF0j6X9Za3PW2sflBBkndYY1fslau89am5Z0j5yQR3IClG9bax+z1mYl/V/l1+BU7ivX+Lry+V+W9IC1dvIVvFYL3i5pp7X2XmttXtLfSRpb9PwOWGu/X35PJiV95gyPK2NMv5zw6Q+ttRlr7XOS/rVc94LHrbXfLb8PX5Z02Zkce9FjuOWM5f2RtTZurT0i6W/0UrCWlxOi9ZRreHzR5RFJmyQZa+1ua+3o2Tw2AAA4dwiMAADA6UxaazMLZ4wxIWPMP5fHjGKSHpPUZE69A9fioCNV/rThLG/bI2lm0WWSdPxUBZ9hjWOLPk8tqqln8bGttUmdpsulXNP/kfTL5W6o2yX921nUcTIn1mAXnzfGdBpjvmqMGS4f99/ldCKdiYXXMr7osqOSehedP/G1CZizW7+qTZK3fNyTPcYfyOmSero88varkmStfUhON9M/SpowxtxpjImexeMCAIBziMAIAACcjj3h/O9J2ijpOmttVM4IkbRojZ0VMCqpxRgTWnRZ/2lu/2pqHF187PJjtr7Mfe6SMz71FjkdMt96lXWcWIPR0uf753Lel0vKx/2lE4554nu22Iic1zKy6LJVkoZfpqazMaWXuoiWPYa1dsxa+yFrbY+k/ybpc6a805q19rPW2qskXSRnNO2/n8O6AADAWSAwAgAAZyMiZy2eOWNMi6Q/WekHtNYelbRN0ieMMT5jzA2S3rFCNd4r6VZjzE3GGJ+k/1sv//PSDyXNSbpT0lettblXWcd3JG0xxvxsubPnt+WsJbUgIikhad4Y06vlocq4nLWDlrHWHpf0I0l/YYwJGGMulfRBOV1Kr5SvfKyAMSZQvuweSX9mjIkYY1ZL+t2FxzDGvHfR4t+zcgKukjHmGmPMdcYYr6SkpIxOPw4IAABWEIERAAA4G38nKSini2SrpP+s0uPeLukGOeNhn5L0NUnZU9z2Fddord0p6SNyFq0elRNoDL3MfaycMbTV5dNXVYe1dkrSeyV9Ws7zXS/piUU3+aSkKyXNywmXvnHCIf5C0h8bY+aMMb9/kof4BUkDcrqN7pOzRtWDZ1LbKeyUE4wtfHxA0m/JCX0OSXpczuv5xfLtr5H0lDEmIWctqo9aaw9Jikr6Fzmv+VE5z/2vX0VdAADgVTDOzzgAAADnj/JW7HustSve4QQAAHAhosMIAADUvfK40lpjjMsY81ZJPyPp/6t1XQAAAK9VZ7PjBQAAQK10yRm9apUzIvYb1tpna1sSAADAaxcjaQAAAAAAAFiCkTQAAAAAAAAsQWAEAAAAAACAJc6LNYza2trswMBArcsAAAAAAAB4zXjmmWemrLXtJ7vuvAiMBgYGtG3btlqXAQAAAAAA8JphjDl6qusYSQMAAAAAAMASBEYAAAAAAABYgsAIAAAAAAAAS5wXaxgBAAAAAACcS/l8XkNDQ8pkMrUuZcUFAgH19fXJ6/We8X0IjAAAAAAAwAVnaGhIkUhEAwMDMsbUupwVY63V9PS0hoaGNDg4eMb3YyQNAAAAAABccDKZjFpbW1/TYZEkGWPU2tp61p1UBEYAAAAAAOCC9FoPixa8kudJYAQAAAAAAFDnGhoaJEkjIyN6z3vec9Lb3Hzzzdq2bds5eTwCIwAAAAAAgPNET0+P7r333hV/HAKjaklOSdu+JM0dq3UlAAAAAACgxj7+8Y/rH//xHyvnP/GJT+hTn/qUfuInfkJXXnmlLrnkEt1///3L7nfkyBFdfPHFkqR0Oq33ve992rx5s9797ncrnU6fs/oIjKplfkj69seksR21rgQAAAAAANTYbbfdpnvuuady/p577tEdd9yh++67T9u3b9fDDz+s3/u935O19pTH+Kd/+ieFQiHt3r1bn/zkJ/XMM8+cs/o85+xIOD23zzkt5mpbBwAAAAAAWOKT39qpXSOxc3rMi3qi+pN3bDnl9VdccYUmJiY0MjKiyclJNTc3q6urS7/zO7+jxx57TC6XS8PDwxofH1dXV9dJj/HYY4/pt3/7tyVJl156qS699NJzVj+BUbVUAqN8besAAAAAAAB14b3vfa/uvfdejY2N6bbbbtNXvvIVTU5O6plnnpHX69XAwIAymUxNaiMwqhZ3+aWmwwgAAAAAgLpyuk6glXTbbbfpQx/6kKampvToo4/qnnvuUUdHh7xerx5++GEdPXr0tPd/wxveoLvvvlu33HKLduzYoRdeeOGc1UZgVC2MpAEAAAAAgEW2bNmieDyu3t5edXd36/bbb9c73vEOXXLJJbr66qu1adOm097/N37jN/SBD3xAmzdv1ubNm3XVVVeds9oIjKqFkTQAAAAAAHCCF198sfJ5W1ubnnzyyZPeLpFISJIGBga0Y4ezoVYwGNRXv/rVFamLXdKqxe11TukwAgAAAAAAdY7AqFoWOoxKdBgBAAAAAID6RmBULYykAQAAAACA8wSBUbW43JJxMZIGAAAAAADqHoFRNbl9BEYAAAAAAKDuERhVk9vHSBoAAAAAAKh7BEbV5PbSYQQAAAAAADQ3N6fPfe5zZ32/t7/97Zqbm1uBipYiMKomRtIAAAAAAIBOHRgVCoXT3u+73/2umpqaVqqsCs+KPwJe4vYykgYAAAAAAPTxj39cBw8e1OWXXy6v16tAIKDm5mbt2bNH+/bt07ve9S4dP35cmUxGH/3oR/XhD39YkjQwMKBt27YpkUjobW97m2666Sb96Ec/Um9vr+6//34Fg8FzUh8dRtVEhxEAAAAAAJD06U9/WmvXrtVzzz2nv/7rv9b27dv193//99q3b58k6Ytf/KKeeeYZbdu2TZ/97Gc1PT297Bj79+/XRz7yEe3cuVNNTU36+te/fs7qo8OomgiMAAAAAACoPw98XBp78dwes+sS6W2fPuObX3vttRocHKyc/+xnP6v77rtPknT8+HHt379fra2tS+4zODioyy+/XJJ01VVX6ciRI6++7jICo2pyeRhJAwAAAAAAy4TD4crnjzzyiB588EE9+eSTCoVCuvnmm5XJZJbdx+/3Vz53u91Kp9PnrB4Co2qiwwgAAAAAgPpzFp1A50okElE8Hj/pdfPz82publYoFNKePXu0devWKldHYFRdbh8dRgAAAAAAQK2trbrxxht18cUXKxgMqrOzs3LdW9/6Vn3+85/X5s2btXHjRl1//fVVr4/AqJrcXqmwvIUMAAAAAABceO6+++6TXu73+/XAAw+c9LqFdYra2tq0Y8eOyuW///u/f05rY5e0amIkDQAAAAAAnAcIjKrJ7ZOKhVpXAQAAAAAAcFoERtXk9tJhBAAAAAAA6h6BUTUxkgYAAAAAQN2w1ta6hKp4Jc9zxQMjY4zbGPOsMebb5fODxpinjDEHjDFfM8b4VrqGusEuaQAAAAAA1IVAIKDp6enXfGhkrdX09LQCgcBZ3a8au6R9VNJuSdHy+b+U9LfW2q8aYz4v6YOS/qkKddQeI2kAAAAAANSFvr4+DQ0NaXJystalrLhAIKC+vr6zus+KBkbGmD5JPy3pzyT9rjHGSLpF0i+Wb3KXpE/oggmMGEkDAAAAAKAeeL1eDQ4O1rqMurXSI2l/J+kPJJXK51slzVlrF7YKG5LUe7I7GmM+bIzZZozZ9ppJ+9xeRtIAAAAAAEDdW7HAyBhzq6QJa+0zr+T+1to7rbVXW2uvbm9vP8fV1QgdRgAAAAAA4DywkiNpN0p6pzHm7ZICctYw+ntJTcYYT7nLqE/S8ArWUF/cPqmUl6yVjKl1NQAAAAAAACe1Yh1G1to/stb2WWsHJL1P0kPW2tslPSzpPeWb3SHp/pWqoe64y/kcY2kAAAAAAKCOrfQaRifzh3IWwD4gZ02jL9Sghtpw+5xTxtIAAAAAAEAdW9Fd0hZYax+R9Ej580OSrq3G49YdAiMAAAAAAHAeqEWH0YXL7XVOGUkDAAAAAAB1jMComugwAgAAAAAA5wECo2paCIxKdBgBAAAAAID6RWBUTYykAQAAAACA8wCBUTUxkgYAAAAAAM4DBEbVRGAEAAAAAADOAwRG1cRIGgAAAAAAOA8QGFUTHUYAAAAAAOA8QGBUTQRGAAAAAADgPEBgVE2MpAEAAAAAgPMAgVE10WEEAAAAAADOAwRG1VQJjOgwAgAAAAAA9YvAqJpcHueUDiMAAAAAAFDHCIyqiZE0AAAAAABwHiAwqiZG0gAAAAAAwHmAwKiaKruk0WEEAAAAAADqF4FRNTGSBgAAAAAAzgMERtVUCYwKta0DAAAAAADgNAiMqsnllmToMAIAAAAAAHWNwKiajHG6jAiMAAAAAABAHSMwqja3j13SAAAAAABAXSMwqja3lw4jAAAAAABQ1wiMqo2RNAAAAAAAUOcIjKqNkTQAAAAAAFDnCIyqjZE0AAAAAABQ5wiMqo2RNAAAAAAAUOcIjKrN7WEkDQAAAAAA1DUCo2qjwwgAAAAAANQ5AqNqIzACAAAAAAB1jsCo2txeRtIAAAAAAEBdIzCqNjqMAAAAAABAnSMwqja3jw4jAAAAAABQ1wiMqs3tlUoERgAAAAAAoH4RGFUbI2kAAAAAAKDOERhVGyNpAAAAAACgzhEYVZvbS4cRAAAAAACoawRG1cZIGgAAAAAAqHMERtXGSBoAAAAAAKhzBEbVxkgaAAAAAACocwRG1bYwkmZtrSsBAAAAAAA4KQKjanN5ndNSobZ1AAAAAAAAnAKBUbW5y4ERY2kAAAAAAKBOERhVm9vnnBIYAQAAAACAOkVgVG2VDiN2SgMAAAAAAPWJwKja6DACAAAAAAB1bsUCI2NMwBjztDHmeWPMTmPMJ8uXDxpjnjLGHDDGfM0Y41upGuoSgREAAAAAAKhzK9lhlJV0i7X2MkmXS3qrMeZ6SX8p6W+tteskzUr64ArWUH8qI2nskgYAAAAAAOrTigVG1pEon/WWP6ykWyTdW778LknvWqka6hIdRgAAAAAAoM6t6BpGxhi3MeY5SROSvi/poKQ5a+1Ce82QpN6VrKHuEBgBAAAAAIA6t6KBkbW2aK29XFKfpGslbTrT+xpjPmyM2WaM2TY5ObliNVYdu6QBAAAAAIA6V5Vd0qy1c5IelnSDpCZjjKd8VZ+k4VPc505r7dXW2qvb29urUWZ10GEEAAAAAADq3EruktZujGkqfx6U9BZJu+UER+8p3+wOSfevVA11icAIAAAAAADUOc/L3+QV65Z0lzHGLSeYusda+21jzC5JXzXGfErSs5K+sII11B9G0gAAAAAAQJ1bscDIWvuCpCtOcvkhOesZXZjoMAIAAAAAAHWuKmsYYZFKhxGBEQAAAAAAqE8ERtXGSBoAAAAAAKhzBEbVxkgaAAAAAACocwRG1UZgBAAAAAAA6hyBUbUxkgYAAAAAAOocgVG10WEEAAAAAADqHIFRtVUCIzqMAAAAAABAfSIwqjaXxzktERgBAAAAAID6RGBUbcY4XUaMpAEAAAAAgDpFYFQLbh8jaQAAAAAAoG4RGNWC20uHEQAAAAAAqFsERrXASBoAAAAAAKhjBEa1wEgaAAAAAACoYwRGtcBIGgAAAAAAqGMERrXgIjACAAAAAAD1i8CoFhhJAwAAAAAAdYzAqBYYSQMAAAAAAHWMwKgW2CUNAAAAAADUMQKjWnB7GUkDAAAAAAB1i8CoFugwAgAAAAAAdYzAqErmUjl96/kRTcazBEYAAAAAAKCuERhVyfGZtH7rP57Vj4/MMJIGAAAAAADqGoFRlazvbJDLSHvH4uUOIwIjAAAAAABQnwiMqiTgdWugNUxgBAAAAAAA6h6BURVt6Ixo73i8PJLGGkYAAAAAAKA+ERhV0cauiI5MJ1UwBEYAAAAAAKB+ERhV0aauiKyV5rJiJA0AAAAAANStMwqMjDFhY4yr/PkGY8w7jTHelS3ttWdjV0SSNJW2dBgBAAAAAIC6daYdRo9JChhjeiV9T9L7Jf2/K1XUa9Xq1rD8HpcmUyUnMLK21iUBAAAAAAAsc6aBkbHWpiT9rKTPWWvfK2nLypX12uR2Ga3vbNB4sijJSqVirUsCAAAAAABY5owDI2PMDZJul/Sd8mXulSnptW1jZ1RjiZJzhrE0AAAAAABQh840MPqYpD+SdJ+1dqcxZo2kh1eurNeuTV0RzWbLZwiMAAAAAABAHfKcyY2stY9KelSSyotfT1lrf3slC3ut2tAV0bGFl52d0gAAAAAAQB06013S7jbGRI0xYUk7JO0yxvz3lS3ttWlTV0T5SmBEhxEAAAAAAKg/ZzqSdpG1NibpXZIekDQoZ6c0nKWOiF9en985Q2AEAAAAAADq0JkGRl5jjFdOYPRNa21eEnvCvwLGGLU2Rp0z+XRtiwEAAAAAADiJMw2M/lnSEUlhSY8ZY1ZLiq1UUa91jW3dkiSbmqpxJQAAAAAAAMudUWBkrf2stbbXWvt26zgq6U0rXNtrVntXryRpenykxpUAAAAAAAAsd6aLXjcaYz5jjNlW/vgbOd1GeAVW9a+WJE2OD9W4EgAAAAAAgOXOdCTti5Likn6+/BGT9KWVKuq1bt3qVSpZo/mp0VqXAgAAAAAAsIznDG+31lr7c4vOf9IY89xKFHQhCAX8mnNFlJkbr3UpAAAAAAAAy5xph1HaGHPTwhljzI2S2OLrVUh7m6XkZK3LAAAAAAAAWOZMO4x+XdK/GWMay+dnJd2xMiVdGEqhNoVmZjWfzqsx6K11OQAAAAAAABVnukva89bayyRdKulSa+0Vkm5Z0cpe47zRTrUqpt2jsVqXAgAAAAAAsMSZjqRJkqy1MWvtQsLxuytQzwWjoaVLrSamXSMERgAAAAAAoL6cVWB0AnPOqrgAhZq61GSS2js8XetSAAAAAAAAlng1gZE93ZXGmH5jzMPGmF3GmJ3GmI+WL28xxnzfGLO/fNr8Kmo4f4XbJEnDI0M1LgQAAAAAAGCp0wZGxpi4MSZ2ko+4pJ6XOXZB0u9Zay+SdL2kjxhjLpL0cUk/sNaul/SD8vkLT7hdkhSbGlGuUKpxMQAAAAAAAC85bWBkrY1Ya6Mn+YhYa0+7w5q1dtRau738eVzSbkm9kn5G0l3lm90l6V2v/mmch8odRlEb04GJRI2LAQAAAAAAeMmrGUk7Y8aYAUlXSHpKUqe1drR81ZikzmrUUHfKHUatmtcudkoDAAAAAAB1ZMUDI2NMg6SvS/rYoh3WJEnWWqtTrIVkjPmwMWabMWbb5OTkSpdZfeUOoy5PnJ3SAAAAAABAXVnRwMgY45UTFn3FWvuN8sXjxpju8vXdkiZOdl9r7Z3W2quttVe3t7evZJm1EWiSXB6tD2e0a3S+1tUAAAAAAABUrFhgZIwxkr4gabe19jOLrvqmpDvKn98h6f6VqqGuGSOF2rQ6mNaukZicZisAAAAAAIDaW8kOoxslvV/SLcaY58ofb5f0aUlvMcbsl/Tm8vkLU7hd3Z6EYpmCjk6nal0NAAAAAACAJOm0O529GtbaxyWZU1z9Eyv1uOeVcJtaEnOSpOeH5jTQFq5xQQAAAAAAAFXaJQ2nEG5XMDejoNetZ4/N1boaAAAAAAAASQRGtRVuk0lN65LeRj13nMAIAAAAAADUBwKjWgq3SbmEru4NaNdITLlCqdYVAQAAAAAAEBjVVLhdknR1e1G5Ykm7R2M1LggAAAAAAIDAqLbKgdHFTTlJYiwNAAAAAADUBQKjWgq1SZLaTUztEb+eJzACAAAAAAB1gMColsJOYGRS07q8v4kOIwAAAAAAUBcIjGqpPJKm5KQu72/Soamk5lP52tYEAAAAAAAueARGteQLS55gJTCSpOeH6DICAAAAAAC1RWBUS8Y4Y2nJKV3S1yhjWPgaAAAAAADUHoFRrYXbpNSUogGv1rY3EBgBAAAAAICaIzCqtXC7lJyUJF25qknbj82qVLI1LgoAAAAAAFzICIxqLdwuJackSdcNtmouldfe8XiNiwIAAAAAABcyAqNaC7c5HUbW6ro1LZKkrYema1wUAAAAAAC4kBEY1Vq4QyrmpMy8+ppD6m8JEhgBAAAAAICaIjCqtYZO5zQxIUm6frBVTx2eYR0jAAAAAABQMwRGtdbQ4ZwmxiVJ169hHSMAAAAAAFBbBEa1VukwcgIj1jECAAAAAAC1RmBUa5UOI2ckbWEdoycPEhgBAAAAAIDaIDCqtWCz5PJWOowk1jECAAAAAAC1RWBUa8Y4Y2nlDiPJWcdoPp3XnjHWMQIAAAAAANVHYFQPGjqWdBixjhEAAAAAAKglAqN6cEKH0cI6Rj/YM36aOwEAAAAAAKwMAqN6cEKHkST9wrWr9MSBaT1zdLZGRQEAAAAAgAsVgVE9aOiUUlNSqVi56I4bBtQS9unvHtxXw8IAAAAAAMCFiMCoHjR0SLYkJacqF4X9Hv36G9foh/un9OMjMzUsDgAAAAAAXGgIjOpBQ6dzesJY2vuvH1Bbg19/+326jAAAAAAAQPUQGNWDSmA0seTioM+tX3/jGv3o4DQ7pgEAAAAAgKohMKoHDR3OaWL5rmi/dP1qdUb9+vPv7lapZKtcGAAAAAAAuBARGNWD0wRGAa9b/+Ptm/XC0Lzu2Xa8yoUBAAAAAIALEYFRPfCFJV9k2Ujagnde1qNrB1r0V/+1V/OpfJWLAwAAAAAAFxoCo3rR0HHSDiNJMsboE+/corlUTp/5/t4qFwYAAAAAAC40BEb1oqHzlB1GknRRT1Tvv361vrz1qB7fP3XOHz5fLOmpQ9MqFEvn/NgAAAAAAOD8QmBUL07TYbTgd9+yUQOtYb3/i0/pz76zS5l88Zw89FQiq9v/9SnddudW3foPj7MjGwAAAAAAFzhPrQtAWUOndPDh096kMeTVt37rJv3FA7v1Lz88rO/tGld3Y0DJbFHJXEGp8qnHZbS+M6KNnRE1Br2aTuY0ncg6DxPwKBrwanVrSJu7o3IZo4999VlNJ3P6zTet033PDut9d27V69e3qa85qLDPo46oX5f3N+uS3kYFfe4lNWULRY3PZzWfziuezSubL6mvOaiBtrC87qV55O7RmP7qP/coVyzp8790lSIBrySnu+nvH9yvS/oa9VNbul72pTo6ndTwbFrXrWmV22XO5lUGAAAAAABngMCoXjR0SNl5KZ+WvMFT3izs9+hT77pEb97cqc89fFClktTa4NMqf0hhn1shn0fZQkn7xuO679lhpXIFtYT9ag37ZIwUzxQUS+cVzxYqx+xpDOjeX3+dLulr1EfetE7//NhBffO5Ee0ZiyuZLSiVczqZ3C6jrmhAfo9LPo9Ls6mcJuJZWbu8To/LaLAtrA2dEa3raNDIXFr3bh9SxO9RKlfUr921TXf96rUyRvrIV57Vg7ud7qpfu2lQf/i2TUrni/rS40f07RdGdFl/k952cZd6moK687FDuv+5YZWstKYtrA+9YY3eclGnCkWrfLGkaNCrxqD33L43AAAAAABcYIw92W/7debqq6+227Ztq3UZK2v7l6Vv/qb00Rek5tXn5JDWWlkruU7owrHWajKe1c7RmIZm03rbxV1qa/Cf8jjTiayeOz6n7cdmNTafVa5YUq5QVCTgVV9zUD1NQTWHfGrwe+TzGB2dTmn/REL7x+PaP5HQsZmUPC6jO24Y0G/esk6P7pvUx772nN60sUOFktVj+yb1v269SEenk7rryaPa0hPV0Gxa8+m8rlrdrH3jccUzTsAV9Lr1/htW66LuqL7w+GG9ODy/rN6WsE+rW0Nqax4yhSEAACAASURBVPCrMehVNOCESNGgRx63S3vHYnpxOKbx+Yx6m4Na3RJSY8ir+XRe86m8GkNeXTfYousGW9UZDahkrayksM8tY156La21SuaKCnrdyzqdrLV6cXheD+6eUDTg0c9d2afmsO9l37NCsSSP++wmRa21OjiZUDpXkjGSx23UEvKpJew762NVg7V2yesIAAAAAKgNY8wz1tqrT3odgVGd2Pc96e73Sh98UOq/ptbVnFOZfFHZQmlJ58/dTx3T/7jvRRkj/eXPXqqfv6ZfknT/c8P6n/ft0HWDLfrYmzfokr5G5Qol/ejglA5PJfXOy3rUWg63rLXaemhGe8di8nnc8riNZpM5HZlO6shUSrOpnObTecXSeSVzL633FPF7tKU3qp6moEbm0jo6nVI8U1BjuTtpPJbRdDK37HlEAx6t74xoVUtIw7Np7R2Paz6drxwzEvAoGvQqEvBoeDatkfmMXEYqWcnvcenWS3u0qiVUqSvoc6s55JXf49aukZiePT6rmWROb1jfrnde3qPL+pp0eDqpgxMJTcazyhZKyhdL6mkK6vo1LdrS06iH90zonx87pOeOzy2r1xiprzmon7+qX++7dpXaI37lCiUdmEjIGGldR0NlbHA2mdPusZg8Lpdawk7Y1Bj0LgvCSiWrodm09ozFZCVdM9CiljMIwiRpPp3Xp769Sz/YM6HP/9JVunaw5YzuV02JbEFDsylt6IgsC1oBAAAA4LWGwOh8MPKcdOcbpdu+Im2+tdbVVMW3nh9Rg9+jN23qWHJ5qWTP+S/r+WJJ8UxB2UJRnZHAaY9vrdWBiYSePjKjeKZQCX2GZlPaP57Q8ZmUepuDlfAoky8qli4olnHCqVgmr8agV2/e3Kk3b+7UeDyjLz95tDwiWFQk4FFj0Kt0rqi5dF7FktXq1pAu729Sc8in/9o5ptH5zJKafB6X/G6XE4qlnJBqoa5VLSF94MYB9TeHVLJW+aLVTDKryUROzx6b1Q/3T8nrdkYED08llS863/M+t0vrOxs0n85raDa97HUwRmoKehUNelWyVoWi1Xw6XxlRXLCpK6K+5pCstSpZq5DPCc6aQk4H2kBrWIlsQZ/45k6NxzJqj/gVSxf0xV+5RjesbVUqV9B/PH1cO4bnlSuWVCxabeyK6D1X9am//Pp+98VRPbBjTB0Rvy7ubdSGzogCXpdcxmg2ldOTB6f1xIEpzaXzuqK/WdcMNKsj6j/hfSkonimoI+LXuo4Greto0GBbWAGvW7lCSXc/dVSffeiAZpI5dUb9+qktXVrVEtKesbj2jsXVEvbp1ku79VMXd8nndunAREL7J+KaiGU1lcgqky/ptmv6dXFvY+W12TE8r0f2Tmh4LqORubRawj6996o+Xb+mVS6XUTyT1+7RuDZ0NqgpdOrgzVqr6WROByYSOjad0vBcWiNzaYX9Hl090KxrBlrUGQ28/DeCpOMzKY3HMrqkr1F+j/u0t7XWqlCyy9Yjs9ZqLpVX2O+Rz+NcVypZxbMFBbyuJcctFEt6YXheg63hM+qyO1t0rAEAAACvHIHR+SA2Kn1mk/TTn5Gu+WCtq8EKyBVKklT5BVtyfsnOFUsKeN1LLtt2dFaHpxIabHOCjcVdPNOJrH58ZEbPHp/Tpb1NeuvFXadd/PvQZEJf3npUR6aS2tQd1ebuqKy12jUa0+7RuCIBjy7tbdRFPVFJ0kwyp9lkTjPJnGZSOcUzBbmNkdtlFPZ7tLEroo1dkUqH19ZD05pO5ORySUZGqVxB8+m85lJ5FUov/fuyrqNBf/Pey9TTFNQv/stWHZ9N6fbrVuu+Z4c1k8yptykofzkEOjiZkCRds7pF+yfimk3l1dMYUDxTWLL+1gKXkS7ta1Jbg0/bj81p5iQdYkGvW2G/W9PJXGXdLWOk/uaQiiWr4bm0bljTqlsv69Zj+yb16L5JZfIltTX4takroqMzSR2fScvrNiqWrBY9Nfk9LhkjZfIl3Xppt376km7d/fQx/XD/lCSpNexTd1NAx6ZTimUK6m9xFpTfOx6Xtc644y9et0q/fMOADk0l9b2dY3r68IyyhZKKJatEtlDpZluou73Br3imoPSi3RKNkdzGqL8lpA2dzteOx+VSqRw4/ejAlI5Mpyo1XzvYoot7G9VY7oybTuS0c2Reu0fjmk3mlMoXVSxZbehs0I3r2nRRd1Tbj83pkb0TlVAz5HPL63YpnsmrZKWA16Ub1rTq9evbNTSb1jefH9FUIquwz61fuXFAv3bTGjWFvIpnC8rkimqP+JcEPqPzaT1/fE5Ds2mNzGWUzBZkjGSMUa5QUirnrKs2lchqPJbVTDKrBr9HrQ1+9TQFdNs1q/T2i7vkcbsq3Ynz6byuHmhRb1NQpZLV3vG4fnxkRqWSVVvEr9awX+0Rn1rDfjWUOwQPTyc1m8xpVUtIa9ob1BzyKpkrKpbOK+B1L+usyxVK8rrNkueSLRQ1k8zJ53Yp5PMo4HUtG2t1vtaN1raHqxJ8pXNFTSez6m0K1kXQViiW9NCeCT28d1I3rmvVT17UteTfyJczHstoMp5VMut8LwS9bkUCXjWHveqKBlb0OR6eSmoiltHVAy1swAAAAM5bBEbng2Je+tN26Y1/KL3pj2pdDfCqlUpW4/GMDk8lNZfK65ZNHZVgbDqR1e3/+pT2jMV188Z2/dYt63XV6ubKfYfn0vo/247rOy+Mam17g95/w2q9bm2rrJWOz6Z0cDKhXMHpaAr63LpqdbOi5V33rLU6PJXUfDpfXrvKWcdq4ZfQTL6oQ5NJHZhM6OBEQgcmE4ql8/rVmwZ184b2yi+Y6Zyz62DbohHI547P6b92jsvncWljZ0QbOhvU3RRU2OdWPFvQnY8e0hceP6x03glCPnjToH7h2lWVccxMvqj/2jmmr28flpF05apmbeyK6D93jOpbL4yqWE6hQj63bljTqkjAI5fLKORza01bg9Z2NGiwNayuxoB8HpfyxZJ2jcT0zNFZzaXzsuUOs6PTSe0dj+vIVFIl6wRqYZ9H1wy26PXr29TdGNRTh6f1owPTOjCZqDyuMdJga1ibu6PqiPoV8rnlNkbPHp+rBFgNfo9ev75NV65qViZf1Hw6r1yxVBnpHJpN69F9kzo8lZTXbXTLpg791JYuPbRnQt95cVRel/M+5IpOgNrbFNQbNrSppzGoH+yZWDJeGfK5FQl4ZK1k5XTFBX1uhXxutYZ96moMqCXsUyJT0HQypx3D8zoynVJfc1BXr27Ww3snlwRtvU1BZfLFk46cvpyFjr4FrWGf1nY0qFSyOj6b0ngsK6/bqL3Br6aQT9PJ7LJNAfwelzZ2RbSpKyJrpccPTFWCt/aIX9evaVV7g1/5YkmFUklBr0fRoEdhn0dTiaxG5jOajGeUyhWVzhWVzheVyReVyjnBns/jks/tbErg87jkdb903u0yGplLVx5vdWtIt17arTdv7lRHNKDGoLOO2xP7p/T4gSkdnExoNpnTbCqvS/oa9btv2aDr17RKcjo2Xxye1yN7J/XI3gntGY3rujUteuvFXbp2oEXxbEGzyZzGY1kNzTodccWSVVuDX20NPnndLhVKVrFMXt96bkQj8xn53C7lik5Ae+ul3WrwO3ty+Dwu9TYF1dccVNDnBL5T8axeGJrXEwemdGgqecr3rMHv0aauiNZ3RtTTGFBnNCArq50jMe0YntdEPKt8saR80aox6FV/S0j9zUFt6IxoS09Um7qjy9auk6S9Y3H9w0P79Z0XR2Wt1N0Y0M9e2auB1rAzEj2dUmckoBvWturawRaFfW5lCiVlyu9XJl/SdCKr7cfmtO3IjA5OJpTMFZXJFZ318vxuhX0euV1GVpKRE4j/7JW9ur68O2ixZBXP5OV2mcr7fmKdmXxRk3Fnh1Tne8hWvh6jQa+agl65XEYT8Yx2jjjr+r1xY7u6G52NN2aSOX35yaPaPxFXb3NQ/c0hXdQT1WV9TUsCsky+WPl+LpXD9+MzKc2l8lrb0aBNXZHKrqgrxVrncVvCPoV8p97PpViy2j8R13PH5jSVyOqy/iZduapZYf+53wNmoVu5MehVxxl2gL4SxZLVocmE+ltCS/74dHQ6qX3jCXU3BtTXHFRj0Lvka2R4Lq0nDkypvzmk69e0nNNwdWg2pcf2TWlTd0SX9zXJ5TLlgDyp47MpXTfYsuR9yhdLslZnFRafSqlktWcsrqaQVz1Np95E5lTyxZKOzaR0aDKpaMCjawfP7WsDAPWIwOh88Vdrpc3vkN7xd7WuBFhxiWxBY/MZretoqHUp59RELKMXh+d147q2JT+8v5xj0yl964URbeqKnPV9T6VUspXunFOx1ipdDn4iAW/lF/UTZfJFHZlOak1bwxn9UD80m1LE71Vj6KVfFPeNx/W1Hx+X1+1SW4NPLmMqwVU8W9ClfY36qS1dev36NvU3h9QU8p7VD+qlktWDu8d152OHtG88rp/Y3KmfvqRbXY0BbTsyox8fmZXf49Lr1rXp+jUtCnjdmk7kNJXIlj9yimfy6mkKarAtrOaQT8dmkjo06YSe0aBH0YBXiWxBByYSOjCRkNvldHT1NgWVLZQ0Gc9qNpVTa9in3uagOiIB5YulSlfU3rG4do/GVChZ3biuVTeta5fLSD86OK2nDk8rmS3K6zbyuF1K54pKlDvqfB6XuhsD6owEFPK7FfQ6HwGfWyGvWy6X04GVK5aUXzgtlsqXWRWKJXVFAxpoCysS8OihPRN64sDUkhBsQXvEr4t7ompt8KvB79EDO0Y1HsvqdWudsOKZo7NK5YpyGemKVc3a3B3R4/tf6l5bzOMy6m4KyOtyaTKRrWxgsOCmdW36petX65ZNHXri4JS+svWoHt03qWLJGTUsnqxAOWHidYMtunFdm1a1hBT2Ox1cmXxJsXReU8mc9o/HtWc0rgOTiSVdhw1+jy7qjqqvOVgJ0+ZSeR2bSenodFKxzMm7GI0xchkpX7QK+9z65dcNaHN3VN/YPqTH9k2qZJ3dRHubghqPZZQtd5Wezpr2sC7qjioS8FS+5xfe92L5+zdftNp60PkeaWvwy2Wk6WRuyWvj97h0WX+Trhlwwo8nDkzpx4dnK0HOybiMs/Pq4vfEGOc96W8J6Rvbh5TJl9TfEtR4ecMLSWoOefWGDe0qFK12jsyf9H0/UU9jQH0tIfU1O92VC6PC2UKpEmJ1Nwa0sSuiwbawjs+m9eLQnPaNJ5TMFZQpdzsuBF2R8h8BfB6XpuJZ7RqNKZ4pqMHv0c9d2av3XbtKU4msHtozoScPTiueKShXLCmeySuTX/qauF1GGzsj2twd1ebuiJpDPqVyBSVzRaWyzmk6X1RXNKB1HQ1a3RqSyxgVilbxbF4HJxLaN55QLJPXYFtY6zsiGp1P695nhrRnLC6fx6UP3jSo37h5rXKFku7bPqzv7xpXX3NQ15XXI5yMZ3V4KlkJHI9MJZUvlnRJb6MuX9WkgMetfeNx7RuPqyHg1ZaeqNZ3NOi543N6YMeYJuNZ+T0uXbemVRs6GvT4gSntGYsveZ4Nfo/6moPqbQpqeC695PqB1pDec1WfAl63xuYzGotlNDaf0eh8RvFMXt2NTmi7ujWsTV3Oa5UvlbT10LSePjyjfLGk/uaQOqMBbT00racOz1SO3d0Y0DUDLXr2+KyOzzjj7wGvSzdv6NBge1jbj87q+aE5FYpW68oBY1ej84eYkN+jrmhAq1tDWtUaUsTvkTFGpZLV7rGYfrh/SgcmEmrwe9Tg92gsltGj+yY1Gc/K4zK67Zp+/dYt61WyVt98fkQP7hpXe8Svy/qbdFF3VFZSKlvQVCKrHcMxvTA8r/3j8WXd0XfcsFr9LSHtHXM2c0nnipUAdiGMdbuMehqDWtUa0qqWkFa3htXbFJTHZTQyn9bhqaRG5zOaS+U0l8prNpXXXCqnWCavLT3O/32X9jXqsX2T+vetR7X10Iwu6WvU69e1aXN3VDOpXOV9vmagRVt6os4fAuYz2jMa0/GZlEbLHZetYZ9WtYTU3RhUrlhSIltQetFyAgGvS33NTp0ul9Gx6ZSOz6Q0mchWRviNcf5dCXjdiga8agl71R7x66rVLUv+CPbI3gkdn0lrU3dEW3oal3XfzqfzOjCRUF9zsDI6XypZvTA8r92jMV3UHdWWnuiSTVrm03ltPTStJw9Oqznk0y9c16+OSKDymNuOzMrncakz6ldnNHDWPy8dnkrq+7vGFPZ7dHFPozZ2RZTMFspf7wVd3t+koG/5Ma21Go9lNR7LlDcAKml1a0h9zaHK8/rRwWl9d8eovC6jpvIGNE0hr5pDPucj7HweOskfIzL5oozRSf8AIDlh5pHyH0RjmbxCPo+uHWh52WU2iiVneQGnG375bYslq92jMY3NZyq7TUeDzvIZQe/yOhc7Np3SnrGY2iJ+9TYFK/9HrVTIWipZFa2Vx2Ve1WMs/J+yEn8sOF8RGJ0vPvc6qXlA+oW7a10JAFTNwhpjZ7qA+vnubNZdKhRLSuWLlV+UzqWpRFbbjsw4O0Sm8/K5XbphbZs2dDYseaxMvqh/33pUX3j8sBqDXl072KJrB1t049q2yrpU1jqjfrtGYmoKedUS9qs94ldXNLCkGyVbcH5Ic7uMPC7Xy45yZfJFjcyldXw2rVyhpNYGnzPi2Rg8q26EbKGoiVhWJWvV3xw65Q/Y1lqNxTLaNRLTvvGEcoWSs1OmdcZQS9aqKeTVz1/dv2TdsYl4RslsUX3NQXndLmXyRT13fE7PHJ1VoWgV8DrdcQGPW36vS9GgV5f1NZ3x13wmX9SDu8f1/V3j8ntc6ogE1Bz2VcaaJ+NZbT82q50jMRVLVpu6Inr9+jat74hUQmMjJxCSpFg6r+mkswHD6tawLu6Jqjns07dfGNU3tg9pPJbRz1zeq//2hjVa3xlRqeS8Lj8+MqNH907qhwem5Pe4tKXHGXNeCJqNMepuDGhVS0jRgFcHJuPaPRrXgYmEhmfTOj6bUjpfrOxgGvC6ZGRkZXVsxunUW9AVDWhzd0TRRb+0xNJ5zaWdUelcwfmFLRp0ApRNXRFtPzan77wwWgm3FkZv2yN++T0uhX3OpheX9TWpLeLXs8fm9OPDM3p+aE67R+OaSmSXvO6m3J3p97hO25kY8Ttr943MpysB2GV9jXr3Fb16YWhe33h2WNGAR6lcUYWS1ebuqCZOssFGg9+jgbaQBlrDcruMXhia1+FyF11L2Kd1HQ2KZwqVUCPgdelNGzv0xg3t2jse12P7JnVoKqlrVrfoJ7d06opVzZqMZzQ0m170kVJL2Kc3bezQTevbtGcspv94+rieLoc8Aa9L3Y1BdUUD6m4MqCHg0eh8RsdnUjoynVwWuK3vaFDY79HQbEpTiZzWtIX17it69ZNburRrdF7ffXFMzx6b1eX9TXrjxg6tbgnpoT0TemDHqKYSOW3pieqq1c0KeN3aMxrTnrG4phO5k4adHpdRQ8CjUslWgt3OqL8SskYCXr1+fZveuKFdLw7P6z+ePiYjo3zJCSYv7o0qli7o2MzykLMl7NMl5fH8de0NWtMe1sHJpO760ZElO/J2RPyKBr1a+BfEGFUeY3g2vSQodhlVxqNPfB5NIa+aysHB7tGY8kVb6bTsiPj1po0d2jk6rx3DsZN+zTX4PXIZLQm4F7pcp5O5MwqsT8bncZU7tq2y+ZLS+eKSAM3tMrpqdbP6moL6/u7xZX8EiAacEfGmkFcTsayG515aI7O/JaiNnVE9d3xuyfda2OfW2o4Gp7M7W9BYLFMZcc/kS/K5Xbr10m6lckU9um9yySj+wmN2RgOKBr2aTTldqIlFyxdEAl6taQ9rsC2s/eOJk+6wvFjQ69abNrXr2oEWzabymohndGQqpd1jMc2l8stuv66jQdcNtujJg9M6NJU86Xuz7HV2uypBUqFU0kT8pT+ouMud5f3NIa3raFBn1K8dw87mOCd+//U2BXXbNf1a1RLSU4dn9PThaU3GsyqUbKWDdoExUsjrVrg8xt/W4FPJWj1/fH7J67XYwhqoV61u1uX9TSpZaaLcPbz18HQlBD6Ry0i9zUG9+4o+vfeqPoV8bj1xcFpbD03L73FpbXuD1rQ5/85lCiWlcwVNJnKajGU0n87L7XLJ6zFK54raP+6sGTqVeOnfS7/Hpc5oQO0Rv4rljuVEOej0ul0K+dy6uKdRV6xu1pq2sMbmnX8HD00ltGskpoOTCZWsFAl41N0YUMk6GwDNp/PqjAa0qSuitR0Nyuad9WbzxZIu7WvSdYMt2tgV0Xw6r5lkToWi1WX9Taf+YjqPEBidL/7tXVI2Ln3oB7WuBAAAnGcW1vhaGKV9JUolq2yhdNK/sK+0uVROh6eS6m0KvuIxrulEVt/dMaaexoBet7btrJ7HZDyrVK6gsN8ZBV287lg6V9TBSWfjC8kJAkI+t9a0hyvrZaVzRR2aSijodWtN+0vdszuG5/X5Rw+qKxrQbdf0a31npDKitXs0pq7GgAZaw2pr8C0LhudSORXKI50LsoWiDk8l1d8cWvYX8nyxtGyjgjMxEcvI53EtG11brFhygr09ozEZ4+yU2npCXafqjjhRqWSVL5VOuflCvlhSMlvQ8Fza6YCZTZV3vi2oaK2uXt2sm9a1Vb5OFn6fWfzYx2dS+tITR9QY9OpdV/RodWtYkjNuuXcsLp/HKOTzqCl06jXPrLXaMRxTKlfQxq7IaTeoKJWsJhPZcrdi6v9v796D5DrLO4//nr7MRZqRRvLI8ugyEtgCW7bjgRJZHMgWizEYNolJnErsTW2olKu8oWA3W7kUJrUXUoEK2UrCLgubLUOMvbtJiIuE2JX1Al7bARKILZmVjSRf0MqSJVn3mdFlRnPrefLH+3b36Z7untFY6tM98/1UdZ1Ln8vTp99+T5/nvOccvTY8ronpgrZetVJvXhdaHPWtCC15k+s6NzGtp186qZ0Hh/UT1/br9u3rS9/hcHz677qekIQ/NzGtZw4M69lXh1XwkHy84Zpebe1fqbUrOkqX/506P6ljZyfUmQ+J0mSLlrHJGR0eCa2KCrPh4Smb13bXbK3j7hqPD2k5Mjyu7/7otJ5++aReGx7X7dvX68NDG7V9wyq9fPy89r5+VkdHLmp4fFojY1Nas7JDNwz0atvVvTp0Zky7Do7oxePndPPG1brthqt1y6Y+7X39nHYeHNarp8e0siOnnq6cNqzu0ruu69fbBtfoyEj4Dr/23BH1duX0/hvX67Yb1iuXsVJrn5PnJnTi3KTOXpzW2pUd6u/pUG9XvpQgHx6b0oFTYzpw+oKuWdWln75lgz5084AKs649R8/qlRMXtKo7F09EmJ566aS+ufdE6ZLe/p4ObVyzQjfE1nUb+7rVlQ9PZ95z9Ky+/copPXNgWDdtXKVfvnWrPnjzNerMZTVTmNXZi9MaGQ+Xd4+MFVuXhXuEjo6F/lzWdHVvV+n3Pz41owsTMzo0PK79Jy/o+NkJvfWaXr1j61oNbe7TVfHzvTY8rkd2Htbf7Q/3y+ztDLceGFy7otRSOZ8J3WzGNDld0NhUoXQZ/5mxcFn20OY+vWPrWm25aqUuTMyUTiKdmwj3I33p+Dn94NBIRQKsv6dDbxsMv8GbN63W6PiUXh+d0JkLUyq4a3bW9fyRUf3d/tMVl+b3duY0M+tzkn5FZtKqrnzpZEhHNqNrr+7RW9b36JpV4aFFGTNdmJyJ3/2kclnTqu68ejpycpUf0vP8kbNzTgJs7OvWDQO92j6wSt0dOZ04N6FjZy8qG1uEFZ90/fLx8zp4Zkzd+az6VnTI5TWTY2/uX6mnfvM9deuEdkLCqF18/aPSgb+VfuPFtCMBAAAAgNRNF2aVNbvsT1GupzAbkm5rV3YsqDXrlXxi63zLPjw8rnMT07r+mlVX7AEMs7OuQ8Pj6sxl1N/TueAWvkdHL+rR3UflLr3run7dvHG1TNLxcxM6GFtPduaz6spntK6nU2tXdlRcnvhGuIckz5GRcQ30dWtg9aVfvph08vyEnn11WIfOjGvNio54mWZXxT1Y2xkJo3bx9O9J3/596d+dlHLL49IMAAAAAACQjkYJo8uTwqu90gfN7KSZ7UmMW2tmT5jZj2J3aaTkLpe+QUkunT2cdiQAAAAAAGAZu2IJI0kPSbqjatz9kp50922SnozDKOobDN3R19KNAwAAAAAALGtXLGHk7t+RNFw1+k5JD8f+hyV9+Eqtvy2t2RK6JIwAAAAAAECKrmQLo1rWu/ux2H9c0vomr7+19W6QLCuNHko7EgAAAAAAsIw1O2FU4uFu23XvuG1m95nZLjPbderUqSZGlqJsTlq9iRZGAAAAAAAgVc1OGJ0wswFJit2T9SZ09wfcfYe771i3bl3TAkxd36A0QgsjAAAAAACQnmYnjB6T9JHY/xFJjzZ5/a1vzRZaGAEAAAAAgFRdsYSRmf25pO9LequZHTGzeyV9VtLtZvYjSe+Lw0jq2yJdOC5NT6QdCQAAAAAAWKZyV2rB7n5Pnbduu1LrXBL6BkP37GGpf1u6sQAAAAAAgGUptZteo46+LaHLk9IAAAAAAEBKSBi1mmILI258DQAAAAAAUkLCqNX0DkiZPDe+BgAAAAAAqSFh1GoyGalvM5ekAQAAAACA1JAwakV9W2hhBAAAAAAAUkPCqBX1DZIwAgAAAAAAqSFh1Ir6BqWxU9LUWNqRAAAAAACAZYiEUStaszV0Rw+nGgYAAAAAAFieSBi1or7B0OXG1wAAAAAAIAUkjFpR35bQ5T5GAAAAAAAgBSSMWlHP1VKuixZGAAAAAAAgFSSMWpFZuCxt5GDakQAAAAAAgGWIhFGrWne9dHxP2lEAAAAAAIBliIRRq9owJI28Kl0cTTsSAAAAAACwzJAwalUDt4Tu8RfSjQMAAAAAACw7JIxa1cBQ6B57Pt04AAAAAADAskPCqFWt7JdWbZJe3512pfElZQAAE3JJREFUJAAAAAAAYJkhYdTKNgzRwggAAAAAADQdCaNWNnCLdGa/NHk+7UgAAAAAAMAyQsKolQ0MSXLp+A/TjgQAAAAAACwjJIxaWfFJadzHCAAAAAAANBEJo1bWu17qHeA+RgAAAAAAoKlIGLW6gVukY7QwAgAAAAAAzUPCqNUNDEmnX5GmxtKOBAAAAAAALBMkjFrdwC2Sz0rH96QdCQAAAAAAWCZIGLW6DUOhy32MAAAAAABAk5AwanW9A9KqTdKBp9OOBAAAAAAALBMkjFqdmXTjh6X9/1e6OJp2NAAAAAAAYBkgYdQObvo5qTAlvfS/044EAAAAAAAsAySM2sGGt0trtkp7/jLtSAAAAAAAwDJAwqgdmEk33SUd+Ftp7HTa0QAAAAAAgCWOhFG7uOkuyQvSvkfTjgQAAAAAACxxJIzaxdXbpXXXS3v+Ku1IAAAAAADAEkfCqF0UL0s79PfSudfTjgYAAAAAACxhJIzayU13SXJp55fTjgQAAAAAACxhJIzayVXXSjf/gvS9L0gjB9OOBgAAAAAALFEkjNrN+z4lZbLSt/592pEAAAAAAIAlioRRu1m9UfrJX5defEx69TtpRwMAAAAAAJYgEkbt6NaPS32D0v/5hFSYSTsaAAAAAACwxJAwakf5bun9n5FO7pMe/03JPe2IAAAAAADAEkLCqF1t/xnp3b8uPfcV6alPpx0NAAAAAABYQnJpB4A34Lb/II2fkb77B9KKtdKtH0s7IgAAAAAAsASQMGpnZtJPfU66OCJ987el4Vel939aynelHRkAAAAAAGhjXJLW7jJZ6a4/CTfC3vkl6cu3SadeSTsqAAAAAADQxkgYLQW5DukDn5H+xSPS+WPSf3+39OTvSpMX0o4MAAAAAAC0IRJGS8lbPiB99HvS9jvDfY2+sEN67iFpajztyAAAAAAAQBsxT+GR7GZ2h6T/Iikr6cvu/tlG0+/YscN37drVlNiWjNeekb5xv/T6D6TO1dLQPdKNPysNDDW+x9GJvdILj0gHvyuNnZLGzoTpr7tdeusd0pvfI3WvWXxchRmpMCnlV4R7MAEAAAAAgFSY2XPuvqPme81OGJlZVtIrkm6XdETSTkn3uPu+evOQMFokd+m170u7HpT2PSoVpqRMXhr4MWnttVLvemlFf3jS2rmj0ol90qkXJctKg7dKqzeG98dOSfufCDfXlqR110ubf1zqf4vUc43Uc7XUsVLKdkiZnDQ9Lk2ely4OS6OvSSMHww25Rw5KZ49IXgjTdq8NyafuNeEpbyvWSivXhXXmu6VcZ5gu1yllO8P9mgpT0vRFaWZSmplIvCarxk+GS/U6e6XOVdKKq0KcK9eFZedXSNl8SGDNToflVvTHYcuE5WQ7Q788bNewgcvbuaJfIRmWyYftkc2F/mw+LiOhlDSzyzhsYVtlsmH9xZdlpcwlNip0l2ZnwvYsTIX+5DKz8TOS/AMAAACAttNqCaNbJX3K3T8Qhz8pSe7+e/XmIWF0GYwPS6/9g3T4GenocyGRc+FESK5kO6VVG6Q1W6Trfyq0RFrZXzn/bEE6sjO0PDr8bHhNjC5s3Sv6pTVby6/OHuniaEgoXRyRxkdC//gZaex0SCgtRrZDynWVE0yFyXAfp5mLi1veUmSZkDjK5mMSKxcTfbG/2AJsZkKamQr9PruA5SaWaZmYv7LYH5NYFoeL/XPGJeYpSdRPxeRVoZjUmw4JvtmY2LNMSF7lOsvlINcdEn6WCZ/DY8LPZ+PnSvQX3zcL8+e7w+epOa1XJgrLG6LBYGKgIsFWPU+d91punktd1gLXs9B5Zgvx+y+Uy4XPxu+9+N3HspDJaU6yt/j9FcuDFvCd1op53s9hMcGarUyySuW4i58j2V8s15n4Gy3OV/rtxt9aaRnFhHdVEtwylUnrmsuI6yu94nCpvBfK/dX1R6Zq2RXb2su/l+T2rxhuNG2tbvz+ZCEBXkqGZ0O3uN3r/bepLl/Vw8kY55SLRJ1WUbdlKsfPGZcYn/yc1XVPxfBsYj11XrK581S/pPK2qejOk+QvbueG31GiX6r67FbZn6znk9PNToeTPcnXzEQoT7muWBd3hd9zvqs8LttRVS97+fNWb9M50xSHE9PWnSax3NLvMV/7xEkmq9J+bbZQPqE1PRH+h8xMhvez+crfYMVvPFu5f6lVDueUl+p6y2r01jnJVGtcdZ3ms/FzjMc6JX53mWz5P0Wpv2r8nN9Eg3JXfTKurkb7C9V5b4HzVO/z5p2n3ngv18vJet29cX2cycbZa9QHC6k3iuWjeOLQ4neRyST6q743qareL4T++biXy/jsTOJzdCTKeNUJy2J/xb5mujxciF154jdWrLdyqn/Std73UbVfn3f/kjjp26jenbOfqfqvmhxXsS9ayLhay0p+3hr/red0L8Gl5AFq7i8bbM/KmRvUNfMMV5f16v/jpWnrbY8FbKdG9WCthgHF/s4e6a0fnLOp2lGjhFGu2cFI2ijpcGL4iKR/kkIcy8uKtdL1HwqvIndpaiy0DpqvgslkpcF3hldx3slz0vkT5cTTzGSo+PMrww+oa7XUNxha+SzU7GxIRBWXV5iqbN0yJxmQGC7u+KoVpkMi6sIJafx0/FM6EZIhxVZR1Qdk2c6wU3eP65+cWymV+qU5FaHPzj0ILMSdYN2dyeUang3bsbTjrzoILCR3ztPlbVtsgVY64I6Jt1xXSLrkulR5cDld3rnPTpeXW33QV+/PbvLgr/ogpN7OoyLBVTzgzZbnK0xXtjIr/pkpfUc1DuIq/gRkEn+OL4bPlfzTW/OAqN4Bar2dfoM/A6nPowbjE2V3Qeupmv9yz5M8WMt3h3rGMvF7n5QmzpXLwOyMlExKSnX+eFUd0Mwb80K2feL3WPpjnDxwKB48VCVfLBOmKf62SvVIoj7xQmLe6haNHeU6LDlPzWXkarwSBxjFgwxZmL7iQKgqUVXa1skDxfn+uOkSpi3Wvx63Y7E+iomtCjUOHup9f9XzzUl2myoSznMSyNUHbJei1sFJ8o9yjSTQgpYRW8WWtlOhwWe+lFhrJAFqJTMWui7Lhla/+URSqFSXXywnXK64xOepVdeX9uszl77o4v8VeWVSt51kO8qJutliErmwiPIOAEvEVdctmYRRI2kkjBbEzO6TdJ8kDQ4OphzNEmUWEjuLnbdrdXite8vliymTCcmtyymbl1YNhBcAAFdaveRSvcTQJS870eKrmNBa6LwLThxVJeoWHWutkwUx/uLJmoUsJ3kSoDBdI7GTTPaofvKnXiuwS/lMyRMn1S0j3EOCtdgaKtdZe/ml5SSTwjOqmays+C5qlaFEUrO8gvJ6ag43mCY5rtjqtt5JOSkmbBMJpGKSsuLkUTKRmNge87bcrNpm1bHPGd/ovQblvm5Li4WeVKlSqwWRVJX8n6lK6CdacFW0Zqwus5ka0yTKdilRPFs7wZd8r1heky1hrUYrxFqtXYonbzPZ8onHYlkunowsnkgsnSwpqNxStVZLvXhoWkxyJ1u/LvgEU1Wc1Sd152t1Ul1PzTk54JXLK62n1roT0yx0XM3Wccm6tPpkbFXMl62lXFKN7V293ebbnqV+1RhW/fdrlfOarb18nm71dJq7rjnrri43Vf0L2X8tAWkkjI5K2pwY3hTHVXD3ByQ9IIVL0poTGgAAwBtUOtC/xPvGLXjZWYXnhixi3mwT//pVJGMWEW9yOfmuxg/taJbiNszmJL2BeJLLyXdftvBSkckolPWWPQ/dOjJZvaFy06qy+WVz8AwsN1fgn8y8dkraZmZvMrMOSXdLeiyFOAAAAAAAAFBD008FuPuMmX1c0jcVTjc96O57mx0HAAAAAAAAakul7ai7Py7p8TTWDQAAAAAAgMbSuCQNAAAAAAAALYyEEQAAAAAAACqQMAIAAAAAAEAFEkYAAAAAAACoQMIIAAAAAAAAFUgYAQAAAAAAoAIJIwAAAAAAAFQwd087hnmZ2SlJh9KO4zLpl3Q67SDQdig3WCzKDhaDcoPFouxgMSg3WCzKDhaDclNpi7uvq/VGWySMlhIz2+XuO9KOA+2FcoPFouxgMSg3WCzKDhaDcoPFouxgMSg3C8claQAAAAAAAKhAwggAAAAAAAAVSBg13wNpB4C2RLnBYlF2sBiUGywWZQeLQbnBYlF2sBiUmwXiHkYAAAAAAACoQAsjAAAAAAAAVCBh1CRmdoeZvWxm+83s/rTjQWszs4Nm9kMz221mu+K4tWb2hJn9KHbXpB0n0mVmD5rZSTPbkxhXs5xY8PlYB71gZm9PL3KkrU7Z+ZSZHY31zm4z+1DivU/GsvOymX0gnaiRNjPbbGZPm9k+M9trZr8Wx1PvoK4G5YY6Bw2ZWZeZPWtmz8ey8ztx/JvM7JlYRv7CzDri+M44vD++vzXN+JGOBuXmITN7NVHnDMXx7KsaIGHUBGaWlfRFSR+UtF3SPWa2Pd2o0Ab+mbsPJR75eL+kJ919m6Qn4zCWt4ck3VE1rl45+aCkbfF1n6Q/blKMaE0PaW7ZkaTPxXpnyN0fl6S4v7pb0o1xnv8W92tYfmYk/Ya7b5f0Tkkfi+WDegeN1Cs3EnUOGpuU9F53v0XSkKQ7zOydkn5foexcJ2lE0r1x+nsljcTxn4vTYfmpV24k6bcSdc7uOI59VQMkjJrjxyXtd/cD7j4l6auS7kw5JrSfOyU9HPsflvThFGNBC3D370garhpdr5zcKel/ePAPkvrMbKA5kaLV1Ck79dwp6avuPunur0rar7BfwzLj7sfc/Qex/7ykFyVtFPUOGmhQbuqhzoEkKdYdF+JgPr5c0nslfS2Or65zinXR1yTdZmbWpHDRIhqUm3rYVzVAwqg5Nko6nBg+osY7SsAlfcvMnjOz++K49e5+LPYfl7Q+ndDQ4uqVE+ohLMTHY3PsBxOXvVJ2MEe81ONtkp4R9Q4WqKrcSNQ5mIeZZc1st6STkp6Q9P8ljbr7TJwkWT5KZSe+f1bSVc2NGK2guty4e7HO+Uyscz5nZp1xHHVOAySMgNb0bnd/u0ITyY+Z2T9Nvunh8YY84hANUU5wif5Y0rUKzbePSfrDdMNBqzKzHkl/Kenfuvu55HvUO6inRrmhzsG83L3g7kOSNim0NLs+5ZDQBqrLjZndJOmTCuXnHZLWSvpEiiG2DRJGzXFU0ubE8KY4DqjJ3Y/G7klJX1fYQZ4oNo+M3ZPpRYgWVq+cUA+hIXc/Ef9gzUr6ksqXgFB2UGJmeYWD/j9197+Ko6l30FCtckOdg0vh7qOSnpZ0q8IlQ7n4VrJ8lMpOfH+1pDNNDhUtJFFu7oiXx7q7T0r6iqhzFoSEUXPslLQt3tG/Q+FGfo+lHBNalJmtNLPeYr+k90vao1BmPhIn+4ikR9OJEC2uXjl5TNIvxydBvFPS2cQlJEDxQL/oZxXqHSmUnbvj02fepHBTyGebHR/SF+8F8ieSXnT3P0q8Rb2DuuqVG+oczMfM1plZX+zvlnS7wj2wnpb083Gy6jqnWBf9vKSnYqtHLCN1ys1LiRMbpnDfq2Sdw76qjtz8k+CNcvcZM/u4pG9Kykp60N33phwWWtd6SV+P9+jLSfozd/+Gme2U9IiZ3SvpkKRfSDFGtAAz+3NJ75HUb2ZHJP1HSZ9V7XLyuKQPKdw8dFzSrzQ9YLSMOmXnPfERsy7poKR/JUnuvtfMHpG0T+FpRx9z90IacSN175L0LyX9MN4bQpJ+W9Q7aKxeubmHOgfzGJD0cHxKXkbSI+7+N2a2T9JXzezTkv6fQkJSsfs/zWy/woMd7k4jaKSuXrl5yszWSTJJuyX9apyefVUDRtIVAAAAAAAASVySBgAAAAAAgAokjAAAAAAAAFCBhBEAAAAAAAAqkDACAAAAAABABRJGAAAAAAAAqEDCCAAAIDKzgpntTrzuv4zL3mpmey7X8gAAAK6kXNoBAAAAtJCL7j6UdhAAAABpo4URAADAPMzsoJn9JzP7oZk9a2bXxfFbzewpM3vBzJ40s8E4fr2Zfd3Mno+vn4iLyprZl8xsr5l9y8y64/T/xsz2xeV8NaWPCQAAUELCCAAAoKy76pK0X0y8d9bdb5b0BUn/OY77r5Iedvcfk/Snkj4fx39e0rfd/RZJb5e0N47fJumL7n6jpFFJd8Xx90t6W1zOr16pDwcAALBQ5u5pxwAAANASzOyCu/fUGH9Q0nvd/YCZ5SUdd/erzOy0pAF3n47jj7l7v5mdkrTJ3ScTy9gq6Ql33xaHPyEp7+6fNrNvSLog6a8l/bW7X7jCHxUAAKAhWhgBAAAsjNfpvxSTif6CyveT/OeSvqjQGmmnmXGfSQAAkCoSRgAAAAvzi4nu92P/9yTdHft/SdJ3Y/+Tkj4qSWaWNbPV9RZqZhlJm939aUmfkLRa0pxWTgAAAM3E2SsAAICybjPbnRj+hrvfH/vXmNkLCq2E7onj/rWkr5jZb0k6JelX4vhfk/SAmd2r0JLoo5KO1VlnVtL/ikklk/R5dx+9bJ8IAABgEbiHEQAAwDziPYx2uPvptGMBAABoBi5JAwAAAAAAQAVaGAEAAAAAAKACLYwAAAAAAABQgYQRAAAAAAAAKpAwAgAAAAAAQAUSRgAAAAAAAKhAwggAAAAAAAAVSBgBAAAAAACgwj8CSJG43ABzryoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE3aIczLPsqC",
        "outputId": "da8d3785-cf01-4cb3-a653-60a707864a03"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o.squeeze(), y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.2243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmmle83YPtXu"
      },
      "source": [
        "# R square\n",
        "o=model(x_val.cuda())\n",
        "r2(o.squeeze(), y_val.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Cy98Z0Q0FL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JI9nSfvQ3At"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waOtjVGCQ4Pm"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 500\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYoqclW8Q5i0"
      },
      "source": [
        "y6 = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7p5xxLYQ7Uv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Fk0AcwQ8dq"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o.squeeze(), y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5wizxreQwJZ"
      },
      "source": [
        "# R square\n",
        "o=model(x_val.cuda())\n",
        "r2(o.squeeze(), y_val.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKe6PkjOQ9DM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}