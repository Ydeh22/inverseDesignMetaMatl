{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[r] Adj & reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXjV+L5nYvIeGZHbXAdPXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taechanha/inverseDesignMetaMatl/blob/main/%5Br%5D_Adj_%26_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d-CtwD3G6RH",
        "outputId": "fb7db910-1f44-4c5e-89f7-a5dc75db5a40"
      },
      "source": [
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "###################### FROM HERE ######################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = open(\"/content/dataset_2.txt\", 'r')\n",
        "data = []\n",
        "length = 0\n",
        "for i in f:\n",
        "  new = []\n",
        "  new.append(i)\n",
        "  data.append(new)\n",
        "\n",
        "  # length of dataset\n",
        "  length += 1\n",
        "\n",
        "f.close()\n",
        "\n",
        "# create dataset from data\n",
        "dataset = []\n",
        "for i in range(length):\n",
        "  new = []\n",
        "  for j in data[i][0].split(','):\n",
        "    new.append(float(j))\n",
        "  dataset.append(new)\n",
        "\n",
        "\n",
        "# trim out label from dataset\n",
        "# Ex Ey Ez\n",
        "label = []\n",
        "new = []\n",
        "for line in dataset:\n",
        "  tmp = []\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.append(line.pop(-1))\n",
        "  tmp.reverse()\n",
        "  label.append(tmp)\n",
        "\n",
        "print(\"label.shape: \", np.array(label).shape)\n",
        "\n",
        "\n",
        "# create edge_index\n",
        "edge_mat = np.zeros(shape=(27,27))\n",
        "edge_index = []\n",
        "\n",
        "for e in range(len(dataset)):\n",
        "  for i in range(0, 27):\n",
        "    for j in range(i+1, 27):\n",
        "      edge_mat[i][j] = dataset[e].pop(0)\n",
        "  edge_index.append(edge_mat + edge_mat.T)\n",
        "\n",
        "\n",
        "# whole dataset to Tensor & train/test split\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "edge_index = torch.FloatTensor(edge_index)\n",
        "label = torch.FloatTensor(label)\n",
        "\n",
        "split = int(length * 0.8)\n",
        "\n",
        "x_train = edge_index[:split]\n",
        "y_train = label[:split]\n",
        "x_val = edge_index[split:]\n",
        "y_val = label[split:]\n",
        "\n",
        "print(\"x_train, y_train, x_val shape: \", x_train.shape, y_train.shape, x_val.shape)\n",
        "\n",
        "# create torch dataset\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, label, dataset, transform=None, target_transform=None):\n",
        "        self.labels = label\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return data, label\n",
        "\n",
        "# create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_data   = CustomDataset(label=y_train, dataset=x_train)\n",
        "test_data       = CustomDataset(label=y_val, dataset=x_val)\n",
        "train_loader    = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_loader     = DataLoader(test_data, batch_size=4, shuffle=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label.shape:  (2000, 3)\n",
            "x_train, y_train, x_val shape:  torch.Size([1600, 27, 27]) torch.Size([1600, 3]) torch.Size([400, 27, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRxva3oEI8X_"
      },
      "source": [
        "epochs = 100"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIbPgPpLJXZX",
        "outputId": "972352b4-46a6-4609-c681-de4c9de4eb54"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 3)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc3(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc4(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ1KoOdUJhoG",
        "outputId": "9c3c23b4-fc77-4432-a3f6-20764bfeb592"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2190"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rx-pOccJllZ"
      },
      "source": [
        "def train(model, train_losses, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        o = model(x)\n",
        "        loss = loss_function(o, y)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    print('====> Epoch: {} loss: {:.4f}'.format(e, train_loss / len(train_loader)))\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "def test(model, val_losses):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda().float(), y.cuda()       \n",
        "            o = model(x)\n",
        "            loss = loss_function(o, y)\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))\n",
        "    val_losses.append(test_loss / len(test_loader))\n",
        "\n",
        "def test_pred(model):\n",
        "    y_pred = []\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda().float(), y.cuda()       \n",
        "            o = model(x)\n",
        "            y_pred.append(o)\n",
        "\n",
        "            loss = loss_function(o, y)\n",
        "            test_loss += loss.item()\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDV5B9gDJmRl",
        "outputId": "1b36e80e-c236-42a9-cfb0-d59bf0e7c67c"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 500\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 116.9155\n",
            "====> Test set loss: 110.5995\n",
            "====> Epoch: 2 loss: 106.7596\n",
            "====> Test set loss: 100.9539\n",
            "====> Epoch: 3 loss: 97.4907\n",
            "====> Test set loss: 92.1510\n",
            "====> Epoch: 4 loss: 89.0011\n",
            "====> Test set loss: 84.0994\n",
            "====> Epoch: 5 loss: 81.2463\n",
            "====> Test set loss: 76.7608\n",
            "====> Epoch: 6 loss: 74.1801\n",
            "====> Test set loss: 70.0423\n",
            "====> Epoch: 7 loss: 67.7673\n",
            "====> Test set loss: 63.9809\n",
            "====> Epoch: 8 loss: 61.9523\n",
            "====> Test set loss: 58.4803\n",
            "====> Epoch: 9 loss: 56.7057\n",
            "====> Test set loss: 53.5061\n",
            "====> Epoch: 10 loss: 51.9814\n",
            "====> Test set loss: 49.0696\n",
            "====> Epoch: 11 loss: 47.7658\n",
            "====> Test set loss: 45.1286\n",
            "====> Epoch: 12 loss: 44.0204\n",
            "====> Test set loss: 41.6122\n",
            "====> Epoch: 13 loss: 40.7088\n",
            "====> Test set loss: 38.5228\n",
            "====> Epoch: 14 loss: 37.7894\n",
            "====> Test set loss: 35.7932\n",
            "====> Epoch: 15 loss: 35.2459\n",
            "====> Test set loss: 33.4295\n",
            "====> Epoch: 16 loss: 33.0457\n",
            "====> Test set loss: 31.3920\n",
            "====> Epoch: 17 loss: 31.1545\n",
            "====> Test set loss: 29.6604\n",
            "====> Epoch: 18 loss: 29.5344\n",
            "====> Test set loss: 28.1927\n",
            "====> Epoch: 19 loss: 28.1594\n",
            "====> Test set loss: 26.9431\n",
            "====> Epoch: 20 loss: 26.9938\n",
            "====> Test set loss: 25.8943\n",
            "====> Epoch: 21 loss: 26.0147\n",
            "====> Test set loss: 25.0225\n",
            "====> Epoch: 22 loss: 25.1868\n",
            "====> Test set loss: 24.2879\n",
            "====> Epoch: 23 loss: 24.4911\n",
            "====> Test set loss: 23.6931\n",
            "====> Epoch: 24 loss: 23.9059\n",
            "====> Test set loss: 23.1743\n",
            "====> Epoch: 25 loss: 23.4080\n",
            "====> Test set loss: 22.7561\n",
            "====> Epoch: 26 loss: 22.9841\n",
            "====> Test set loss: 22.4067\n",
            "====> Epoch: 27 loss: 22.6160\n",
            "====> Test set loss: 22.1053\n",
            "====> Epoch: 28 loss: 22.2942\n",
            "====> Test set loss: 21.8474\n",
            "====> Epoch: 29 loss: 22.0054\n",
            "====> Test set loss: 21.6283\n",
            "====> Epoch: 30 loss: 21.7456\n",
            "====> Test set loss: 21.4255\n",
            "====> Epoch: 31 loss: 21.5090\n",
            "====> Test set loss: 21.2531\n",
            "====> Epoch: 32 loss: 21.2882\n",
            "====> Test set loss: 21.0901\n",
            "====> Epoch: 33 loss: 21.0789\n",
            "====> Test set loss: 20.9404\n",
            "====> Epoch: 34 loss: 20.8817\n",
            "====> Test set loss: 20.8034\n",
            "====> Epoch: 35 loss: 20.6935\n",
            "====> Test set loss: 20.6766\n",
            "====> Epoch: 36 loss: 20.5122\n",
            "====> Test set loss: 20.5531\n",
            "====> Epoch: 37 loss: 20.3356\n",
            "====> Test set loss: 20.4347\n",
            "====> Epoch: 38 loss: 20.1636\n",
            "====> Test set loss: 20.3219\n",
            "====> Epoch: 39 loss: 19.9971\n",
            "====> Test set loss: 20.2129\n",
            "====> Epoch: 40 loss: 19.8337\n",
            "====> Test set loss: 20.1073\n",
            "====> Epoch: 41 loss: 19.6735\n",
            "====> Test set loss: 20.0046\n",
            "====> Epoch: 42 loss: 19.5161\n",
            "====> Test set loss: 19.9027\n",
            "====> Epoch: 43 loss: 19.3594\n",
            "====> Test set loss: 19.8042\n",
            "====> Epoch: 44 loss: 19.2067\n",
            "====> Test set loss: 19.7055\n",
            "====> Epoch: 45 loss: 19.0559\n",
            "====> Test set loss: 19.6116\n",
            "====> Epoch: 46 loss: 18.9070\n",
            "====> Test set loss: 19.5181\n",
            "====> Epoch: 47 loss: 18.7597\n",
            "====> Test set loss: 19.4275\n",
            "====> Epoch: 48 loss: 18.6157\n",
            "====> Test set loss: 19.3373\n",
            "====> Epoch: 49 loss: 18.4740\n",
            "====> Test set loss: 19.2474\n",
            "====> Epoch: 50 loss: 18.3337\n",
            "====> Test set loss: 19.1601\n",
            "====> Epoch: 51 loss: 18.1952\n",
            "====> Test set loss: 19.0745\n",
            "====> Epoch: 52 loss: 18.0591\n",
            "====> Test set loss: 18.9920\n",
            "====> Epoch: 53 loss: 17.9243\n",
            "====> Test set loss: 18.9091\n",
            "====> Epoch: 54 loss: 17.7919\n",
            "====> Test set loss: 18.8264\n",
            "====> Epoch: 55 loss: 17.6604\n",
            "====> Test set loss: 18.7452\n",
            "====> Epoch: 56 loss: 17.5309\n",
            "====> Test set loss: 18.6676\n",
            "====> Epoch: 57 loss: 17.4034\n",
            "====> Test set loss: 18.5883\n",
            "====> Epoch: 58 loss: 17.2776\n",
            "====> Test set loss: 18.5109\n",
            "====> Epoch: 59 loss: 17.1536\n",
            "====> Test set loss: 18.4356\n",
            "====> Epoch: 60 loss: 17.0302\n",
            "====> Test set loss: 18.3609\n",
            "====> Epoch: 61 loss: 16.9093\n",
            "====> Test set loss: 18.2861\n",
            "====> Epoch: 62 loss: 16.7902\n",
            "====> Test set loss: 18.2131\n",
            "====> Epoch: 63 loss: 16.6728\n",
            "====> Test set loss: 18.1412\n",
            "====> Epoch: 64 loss: 16.5572\n",
            "====> Test set loss: 18.0720\n",
            "====> Epoch: 65 loss: 16.4432\n",
            "====> Test set loss: 18.0048\n",
            "====> Epoch: 66 loss: 16.3307\n",
            "====> Test set loss: 17.9345\n",
            "====> Epoch: 67 loss: 16.2194\n",
            "====> Test set loss: 17.8670\n",
            "====> Epoch: 68 loss: 16.1097\n",
            "====> Test set loss: 17.8009\n",
            "====> Epoch: 69 loss: 16.0024\n",
            "====> Test set loss: 17.7349\n",
            "====> Epoch: 70 loss: 15.8965\n",
            "====> Test set loss: 17.6717\n",
            "====> Epoch: 71 loss: 15.7919\n",
            "====> Test set loss: 17.6064\n",
            "====> Epoch: 72 loss: 15.6893\n",
            "====> Test set loss: 17.5463\n",
            "====> Epoch: 73 loss: 15.5867\n",
            "====> Test set loss: 17.4846\n",
            "====> Epoch: 74 loss: 15.4867\n",
            "====> Test set loss: 17.4245\n",
            "====> Epoch: 75 loss: 15.3881\n",
            "====> Test set loss: 17.3651\n",
            "====> Epoch: 76 loss: 15.2905\n",
            "====> Test set loss: 17.3053\n",
            "====> Epoch: 77 loss: 15.1942\n",
            "====> Test set loss: 17.2472\n",
            "====> Epoch: 78 loss: 15.1000\n",
            "====> Test set loss: 17.1894\n",
            "====> Epoch: 79 loss: 15.0056\n",
            "====> Test set loss: 17.1338\n",
            "====> Epoch: 80 loss: 14.9136\n",
            "====> Test set loss: 17.0778\n",
            "====> Epoch: 81 loss: 14.8224\n",
            "====> Test set loss: 17.0239\n",
            "====> Epoch: 82 loss: 14.7325\n",
            "====> Test set loss: 16.9690\n",
            "====> Epoch: 83 loss: 14.6442\n",
            "====> Test set loss: 16.9171\n",
            "====> Epoch: 84 loss: 14.5573\n",
            "====> Test set loss: 16.8653\n",
            "====> Epoch: 85 loss: 14.4710\n",
            "====> Test set loss: 16.8136\n",
            "====> Epoch: 86 loss: 14.3876\n",
            "====> Test set loss: 16.7641\n",
            "====> Epoch: 87 loss: 14.3037\n",
            "====> Test set loss: 16.7128\n",
            "====> Epoch: 88 loss: 14.2219\n",
            "====> Test set loss: 16.6659\n",
            "====> Epoch: 89 loss: 14.1403\n",
            "====> Test set loss: 16.6175\n",
            "====> Epoch: 90 loss: 14.0599\n",
            "====> Test set loss: 16.5695\n",
            "====> Epoch: 91 loss: 13.9817\n",
            "====> Test set loss: 16.5223\n",
            "====> Epoch: 92 loss: 13.9039\n",
            "====> Test set loss: 16.4754\n",
            "====> Epoch: 93 loss: 13.8273\n",
            "====> Test set loss: 16.4305\n",
            "====> Epoch: 94 loss: 13.7520\n",
            "====> Test set loss: 16.3851\n",
            "====> Epoch: 95 loss: 13.6773\n",
            "====> Test set loss: 16.3411\n",
            "====> Epoch: 96 loss: 13.6041\n",
            "====> Test set loss: 16.2982\n",
            "====> Epoch: 97 loss: 13.5313\n",
            "====> Test set loss: 16.2552\n",
            "====> Epoch: 98 loss: 13.4600\n",
            "====> Test set loss: 16.2134\n",
            "====> Epoch: 99 loss: 13.3900\n",
            "====> Test set loss: 16.1720\n",
            "====> Epoch: 100 loss: 13.3211\n",
            "====> Test set loss: 16.1309\n",
            "====> Epoch: 101 loss: 13.2523\n",
            "====> Test set loss: 16.0920\n",
            "====> Epoch: 102 loss: 13.1852\n",
            "====> Test set loss: 16.0516\n",
            "====> Epoch: 103 loss: 13.1190\n",
            "====> Test set loss: 16.0127\n",
            "====> Epoch: 104 loss: 13.0531\n",
            "====> Test set loss: 15.9738\n",
            "====> Epoch: 105 loss: 12.9889\n",
            "====> Test set loss: 15.9357\n",
            "====> Epoch: 106 loss: 12.9254\n",
            "====> Test set loss: 15.8990\n",
            "====> Epoch: 107 loss: 12.8636\n",
            "====> Test set loss: 15.8615\n",
            "====> Epoch: 108 loss: 12.8010\n",
            "====> Test set loss: 15.8252\n",
            "====> Epoch: 109 loss: 12.7408\n",
            "====> Test set loss: 15.7905\n",
            "====> Epoch: 110 loss: 12.6803\n",
            "====> Test set loss: 15.7545\n",
            "====> Epoch: 111 loss: 12.6211\n",
            "====> Test set loss: 15.7209\n",
            "====> Epoch: 112 loss: 12.5629\n",
            "====> Test set loss: 15.6859\n",
            "====> Epoch: 113 loss: 12.5053\n",
            "====> Test set loss: 15.6532\n",
            "====> Epoch: 114 loss: 12.4478\n",
            "====> Test set loss: 15.6189\n",
            "====> Epoch: 115 loss: 12.3927\n",
            "====> Test set loss: 15.5869\n",
            "====> Epoch: 116 loss: 12.3370\n",
            "====> Test set loss: 15.5541\n",
            "====> Epoch: 117 loss: 12.2822\n",
            "====> Test set loss: 15.5224\n",
            "====> Epoch: 118 loss: 12.2281\n",
            "====> Test set loss: 15.4923\n",
            "====> Epoch: 119 loss: 12.1749\n",
            "====> Test set loss: 15.4607\n",
            "====> Epoch: 120 loss: 12.1229\n",
            "====> Test set loss: 15.4314\n",
            "====> Epoch: 121 loss: 12.0719\n",
            "====> Test set loss: 15.4015\n",
            "====> Epoch: 122 loss: 12.0206\n",
            "====> Test set loss: 15.3722\n",
            "====> Epoch: 123 loss: 11.9709\n",
            "====> Test set loss: 15.3445\n",
            "====> Epoch: 124 loss: 11.9216\n",
            "====> Test set loss: 15.3145\n",
            "====> Epoch: 125 loss: 11.8730\n",
            "====> Test set loss: 15.2861\n",
            "====> Epoch: 126 loss: 11.8247\n",
            "====> Test set loss: 15.2580\n",
            "====> Epoch: 127 loss: 11.7776\n",
            "====> Test set loss: 15.2324\n",
            "====> Epoch: 128 loss: 11.7301\n",
            "====> Test set loss: 15.2045\n",
            "====> Epoch: 129 loss: 11.6845\n",
            "====> Test set loss: 15.1772\n",
            "====> Epoch: 130 loss: 11.6389\n",
            "====> Test set loss: 15.1519\n",
            "====> Epoch: 131 loss: 11.5934\n",
            "====> Test set loss: 15.1258\n",
            "====> Epoch: 132 loss: 11.5494\n",
            "====> Test set loss: 15.1022\n",
            "====> Epoch: 133 loss: 11.5056\n",
            "====> Test set loss: 15.0746\n",
            "====> Epoch: 134 loss: 11.4621\n",
            "====> Test set loss: 15.0505\n",
            "====> Epoch: 135 loss: 11.4193\n",
            "====> Test set loss: 15.0257\n",
            "====> Epoch: 136 loss: 11.3773\n",
            "====> Test set loss: 15.0022\n",
            "====> Epoch: 137 loss: 11.3355\n",
            "====> Test set loss: 14.9780\n",
            "====> Epoch: 138 loss: 11.2963\n",
            "====> Test set loss: 14.9556\n",
            "====> Epoch: 139 loss: 11.2550\n",
            "====> Test set loss: 14.9314\n",
            "====> Epoch: 140 loss: 11.2149\n",
            "====> Test set loss: 14.9106\n",
            "====> Epoch: 141 loss: 11.1753\n",
            "====> Test set loss: 14.8864\n",
            "====> Epoch: 142 loss: 11.1362\n",
            "====> Test set loss: 14.8644\n",
            "====> Epoch: 143 loss: 11.0983\n",
            "====> Test set loss: 14.8428\n",
            "====> Epoch: 144 loss: 11.0599\n",
            "====> Test set loss: 14.8223\n",
            "====> Epoch: 145 loss: 11.0231\n",
            "====> Test set loss: 14.8008\n",
            "====> Epoch: 146 loss: 10.9856\n",
            "====> Test set loss: 14.7797\n",
            "====> Epoch: 147 loss: 10.9499\n",
            "====> Test set loss: 14.7587\n",
            "====> Epoch: 148 loss: 10.9143\n",
            "====> Test set loss: 14.7377\n",
            "====> Epoch: 149 loss: 10.8783\n",
            "====> Test set loss: 14.7168\n",
            "====> Epoch: 150 loss: 10.8431\n",
            "====> Test set loss: 14.6977\n",
            "====> Epoch: 151 loss: 10.8084\n",
            "====> Test set loss: 14.6787\n",
            "====> Epoch: 152 loss: 10.7753\n",
            "====> Test set loss: 14.6592\n",
            "====> Epoch: 153 loss: 10.7411\n",
            "====> Test set loss: 14.6393\n",
            "====> Epoch: 154 loss: 10.7076\n",
            "====> Test set loss: 14.6207\n",
            "====> Epoch: 155 loss: 10.6750\n",
            "====> Test set loss: 14.6033\n",
            "====> Epoch: 156 loss: 10.6429\n",
            "====> Test set loss: 14.5842\n",
            "====> Epoch: 157 loss: 10.6106\n",
            "====> Test set loss: 14.5665\n",
            "====> Epoch: 158 loss: 10.5794\n",
            "====> Test set loss: 14.5485\n",
            "====> Epoch: 159 loss: 10.5476\n",
            "====> Test set loss: 14.5301\n",
            "====> Epoch: 160 loss: 10.5170\n",
            "====> Test set loss: 14.5130\n",
            "====> Epoch: 161 loss: 10.4868\n",
            "====> Test set loss: 14.4970\n",
            "====> Epoch: 162 loss: 10.4562\n",
            "====> Test set loss: 14.4794\n",
            "====> Epoch: 163 loss: 10.4268\n",
            "====> Test set loss: 14.4636\n",
            "====> Epoch: 164 loss: 10.3975\n",
            "====> Test set loss: 14.4468\n",
            "====> Epoch: 165 loss: 10.3692\n",
            "====> Test set loss: 14.4306\n",
            "====> Epoch: 166 loss: 10.3398\n",
            "====> Test set loss: 14.4142\n",
            "====> Epoch: 167 loss: 10.3118\n",
            "====> Test set loss: 14.3974\n",
            "====> Epoch: 168 loss: 10.2838\n",
            "====> Test set loss: 14.3822\n",
            "====> Epoch: 169 loss: 10.2560\n",
            "====> Test set loss: 14.3668\n",
            "====> Epoch: 170 loss: 10.2284\n",
            "====> Test set loss: 14.3516\n",
            "====> Epoch: 171 loss: 10.2013\n",
            "====> Test set loss: 14.3369\n",
            "====> Epoch: 172 loss: 10.1745\n",
            "====> Test set loss: 14.3203\n",
            "====> Epoch: 173 loss: 10.1484\n",
            "====> Test set loss: 14.3068\n",
            "====> Epoch: 174 loss: 10.1223\n",
            "====> Test set loss: 14.2918\n",
            "====> Epoch: 175 loss: 10.0961\n",
            "====> Test set loss: 14.2760\n",
            "====> Epoch: 176 loss: 10.0708\n",
            "====> Test set loss: 14.2639\n",
            "====> Epoch: 177 loss: 10.0460\n",
            "====> Test set loss: 14.2503\n",
            "====> Epoch: 178 loss: 10.0209\n",
            "====> Test set loss: 14.2346\n",
            "====> Epoch: 179 loss: 9.9960\n",
            "====> Test set loss: 14.2217\n",
            "====> Epoch: 180 loss: 9.9732\n",
            "====> Test set loss: 14.2077\n",
            "====> Epoch: 181 loss: 9.9484\n",
            "====> Test set loss: 14.1953\n",
            "====> Epoch: 182 loss: 9.9243\n",
            "====> Test set loss: 14.1817\n",
            "====> Epoch: 183 loss: 9.9014\n",
            "====> Test set loss: 14.1682\n",
            "====> Epoch: 184 loss: 9.8780\n",
            "====> Test set loss: 14.1554\n",
            "====> Epoch: 185 loss: 9.8548\n",
            "====> Test set loss: 14.1424\n",
            "====> Epoch: 186 loss: 9.8317\n",
            "====> Test set loss: 14.1297\n",
            "====> Epoch: 187 loss: 9.8099\n",
            "====> Test set loss: 14.1176\n",
            "====> Epoch: 188 loss: 9.7875\n",
            "====> Test set loss: 14.1063\n",
            "====> Epoch: 189 loss: 9.7656\n",
            "====> Test set loss: 14.0930\n",
            "====> Epoch: 190 loss: 9.7437\n",
            "====> Test set loss: 14.0809\n",
            "====> Epoch: 191 loss: 9.7226\n",
            "====> Test set loss: 14.0690\n",
            "====> Epoch: 192 loss: 9.7005\n",
            "====> Test set loss: 14.0566\n",
            "====> Epoch: 193 loss: 9.6796\n",
            "====> Test set loss: 14.0449\n",
            "====> Epoch: 194 loss: 9.6592\n",
            "====> Test set loss: 14.0333\n",
            "====> Epoch: 195 loss: 9.6382\n",
            "====> Test set loss: 14.0222\n",
            "====> Epoch: 196 loss: 9.6177\n",
            "====> Test set loss: 14.0108\n",
            "====> Epoch: 197 loss: 9.5981\n",
            "====> Test set loss: 13.9991\n",
            "====> Epoch: 198 loss: 9.5773\n",
            "====> Test set loss: 13.9879\n",
            "====> Epoch: 199 loss: 9.5574\n",
            "====> Test set loss: 13.9771\n",
            "====> Epoch: 200 loss: 9.5384\n",
            "====> Test set loss: 13.9654\n",
            "====> Epoch: 201 loss: 9.5181\n",
            "====> Test set loss: 13.9541\n",
            "====> Epoch: 202 loss: 9.4986\n",
            "====> Test set loss: 13.9427\n",
            "====> Epoch: 203 loss: 9.4796\n",
            "====> Test set loss: 13.9330\n",
            "====> Epoch: 204 loss: 9.4608\n",
            "====> Test set loss: 13.9225\n",
            "====> Epoch: 205 loss: 9.4417\n",
            "====> Test set loss: 13.9126\n",
            "====> Epoch: 206 loss: 9.4234\n",
            "====> Test set loss: 13.9014\n",
            "====> Epoch: 207 loss: 9.4050\n",
            "====> Test set loss: 13.8906\n",
            "====> Epoch: 208 loss: 9.3873\n",
            "====> Test set loss: 13.8801\n",
            "====> Epoch: 209 loss: 9.3688\n",
            "====> Test set loss: 13.8702\n",
            "====> Epoch: 210 loss: 9.3515\n",
            "====> Test set loss: 13.8615\n",
            "====> Epoch: 211 loss: 9.3334\n",
            "====> Test set loss: 13.8508\n",
            "====> Epoch: 212 loss: 9.3158\n",
            "====> Test set loss: 13.8402\n",
            "====> Epoch: 213 loss: 9.2993\n",
            "====> Test set loss: 13.8296\n",
            "====> Epoch: 214 loss: 9.2814\n",
            "====> Test set loss: 13.8211\n",
            "====> Epoch: 215 loss: 9.2649\n",
            "====> Test set loss: 13.8110\n",
            "====> Epoch: 216 loss: 9.2487\n",
            "====> Test set loss: 13.8015\n",
            "====> Epoch: 217 loss: 9.2322\n",
            "====> Test set loss: 13.7917\n",
            "====> Epoch: 218 loss: 9.2149\n",
            "====> Test set loss: 13.7825\n",
            "====> Epoch: 219 loss: 9.1996\n",
            "====> Test set loss: 13.7723\n",
            "====> Epoch: 220 loss: 9.1824\n",
            "====> Test set loss: 13.7640\n",
            "====> Epoch: 221 loss: 9.1665\n",
            "====> Test set loss: 13.7552\n",
            "====> Epoch: 222 loss: 9.1507\n",
            "====> Test set loss: 13.7451\n",
            "====> Epoch: 223 loss: 9.1353\n",
            "====> Test set loss: 13.7369\n",
            "====> Epoch: 224 loss: 9.1192\n",
            "====> Test set loss: 13.7277\n",
            "====> Epoch: 225 loss: 9.1036\n",
            "====> Test set loss: 13.7177\n",
            "====> Epoch: 226 loss: 9.0882\n",
            "====> Test set loss: 13.7100\n",
            "====> Epoch: 227 loss: 9.0730\n",
            "====> Test set loss: 13.6993\n",
            "====> Epoch: 228 loss: 9.0573\n",
            "====> Test set loss: 13.6903\n",
            "====> Epoch: 229 loss: 9.0431\n",
            "====> Test set loss: 13.6831\n",
            "====> Epoch: 230 loss: 9.0278\n",
            "====> Test set loss: 13.6744\n",
            "====> Epoch: 231 loss: 9.0128\n",
            "====> Test set loss: 13.6647\n",
            "====> Epoch: 232 loss: 8.9979\n",
            "====> Test set loss: 13.6551\n",
            "====> Epoch: 233 loss: 8.9838\n",
            "====> Test set loss: 13.6481\n",
            "====> Epoch: 234 loss: 8.9698\n",
            "====> Test set loss: 13.6396\n",
            "====> Epoch: 235 loss: 8.9550\n",
            "====> Test set loss: 13.6305\n",
            "====> Epoch: 236 loss: 8.9414\n",
            "====> Test set loss: 13.6223\n",
            "====> Epoch: 237 loss: 8.9271\n",
            "====> Test set loss: 13.6146\n",
            "====> Epoch: 238 loss: 8.9129\n",
            "====> Test set loss: 13.6057\n",
            "====> Epoch: 239 loss: 8.8999\n",
            "====> Test set loss: 13.5986\n",
            "====> Epoch: 240 loss: 8.8858\n",
            "====> Test set loss: 13.5908\n",
            "====> Epoch: 241 loss: 8.8727\n",
            "====> Test set loss: 13.5817\n",
            "====> Epoch: 242 loss: 8.8596\n",
            "====> Test set loss: 13.5739\n",
            "====> Epoch: 243 loss: 8.8457\n",
            "====> Test set loss: 13.5662\n",
            "====> Epoch: 244 loss: 8.8324\n",
            "====> Test set loss: 13.5580\n",
            "====> Epoch: 245 loss: 8.8197\n",
            "====> Test set loss: 13.5506\n",
            "====> Epoch: 246 loss: 8.8063\n",
            "====> Test set loss: 13.5424\n",
            "====> Epoch: 247 loss: 8.7939\n",
            "====> Test set loss: 13.5334\n",
            "====> Epoch: 248 loss: 8.7807\n",
            "====> Test set loss: 13.5279\n",
            "====> Epoch: 249 loss: 8.7678\n",
            "====> Test set loss: 13.5178\n",
            "====> Epoch: 250 loss: 8.7555\n",
            "====> Test set loss: 13.5106\n",
            "====> Epoch: 251 loss: 8.7429\n",
            "====> Test set loss: 13.5033\n",
            "====> Epoch: 252 loss: 8.7302\n",
            "====> Test set loss: 13.4964\n",
            "====> Epoch: 253 loss: 8.7188\n",
            "====> Test set loss: 13.4888\n",
            "====> Epoch: 254 loss: 8.7060\n",
            "====> Test set loss: 13.4809\n",
            "====> Epoch: 255 loss: 8.6940\n",
            "====> Test set loss: 13.4733\n",
            "====> Epoch: 256 loss: 8.6831\n",
            "====> Test set loss: 13.4659\n",
            "====> Epoch: 257 loss: 8.6704\n",
            "====> Test set loss: 13.4585\n",
            "====> Epoch: 258 loss: 8.6579\n",
            "====> Test set loss: 13.4508\n",
            "====> Epoch: 259 loss: 8.6463\n",
            "====> Test set loss: 13.4444\n",
            "====> Epoch: 260 loss: 8.6344\n",
            "====> Test set loss: 13.4351\n",
            "====> Epoch: 261 loss: 8.6231\n",
            "====> Test set loss: 13.4288\n",
            "====> Epoch: 262 loss: 8.6114\n",
            "====> Test set loss: 13.4209\n",
            "====> Epoch: 263 loss: 8.6004\n",
            "====> Test set loss: 13.4130\n",
            "====> Epoch: 264 loss: 8.5882\n",
            "====> Test set loss: 13.4069\n",
            "====> Epoch: 265 loss: 8.5771\n",
            "====> Test set loss: 13.3986\n",
            "====> Epoch: 266 loss: 8.5657\n",
            "====> Test set loss: 13.3916\n",
            "====> Epoch: 267 loss: 8.5546\n",
            "====> Test set loss: 13.3853\n",
            "====> Epoch: 268 loss: 8.5434\n",
            "====> Test set loss: 13.3773\n",
            "====> Epoch: 269 loss: 8.5320\n",
            "====> Test set loss: 13.3704\n",
            "====> Epoch: 270 loss: 8.5216\n",
            "====> Test set loss: 13.3642\n",
            "====> Epoch: 271 loss: 8.5103\n",
            "====> Test set loss: 13.3561\n",
            "====> Epoch: 272 loss: 8.4993\n",
            "====> Test set loss: 13.3488\n",
            "====> Epoch: 273 loss: 8.4886\n",
            "====> Test set loss: 13.3427\n",
            "====> Epoch: 274 loss: 8.4785\n",
            "====> Test set loss: 13.3357\n",
            "====> Epoch: 275 loss: 8.4676\n",
            "====> Test set loss: 13.3284\n",
            "====> Epoch: 276 loss: 8.4568\n",
            "====> Test set loss: 13.3222\n",
            "====> Epoch: 277 loss: 8.4461\n",
            "====> Test set loss: 13.3158\n",
            "====> Epoch: 278 loss: 8.4359\n",
            "====> Test set loss: 13.3081\n",
            "====> Epoch: 279 loss: 8.4255\n",
            "====> Test set loss: 13.3010\n",
            "====> Epoch: 280 loss: 8.4147\n",
            "====> Test set loss: 13.2944\n",
            "====> Epoch: 281 loss: 8.4044\n",
            "====> Test set loss: 13.2856\n",
            "====> Epoch: 282 loss: 8.3944\n",
            "====> Test set loss: 13.2794\n",
            "====> Epoch: 283 loss: 8.3842\n",
            "====> Test set loss: 13.2736\n",
            "====> Epoch: 284 loss: 8.3740\n",
            "====> Test set loss: 13.2662\n",
            "====> Epoch: 285 loss: 8.3640\n",
            "====> Test set loss: 13.2594\n",
            "====> Epoch: 286 loss: 8.3543\n",
            "====> Test set loss: 13.2524\n",
            "====> Epoch: 287 loss: 8.3445\n",
            "====> Test set loss: 13.2450\n",
            "====> Epoch: 288 loss: 8.3345\n",
            "====> Test set loss: 13.2395\n",
            "====> Epoch: 289 loss: 8.3246\n",
            "====> Test set loss: 13.2323\n",
            "====> Epoch: 290 loss: 8.3152\n",
            "====> Test set loss: 13.2260\n",
            "====> Epoch: 291 loss: 8.3049\n",
            "====> Test set loss: 13.2184\n",
            "====> Epoch: 292 loss: 8.2958\n",
            "====> Test set loss: 13.2113\n",
            "====> Epoch: 293 loss: 8.2862\n",
            "====> Test set loss: 13.2053\n",
            "====> Epoch: 294 loss: 8.2764\n",
            "====> Test set loss: 13.1983\n",
            "====> Epoch: 295 loss: 8.2667\n",
            "====> Test set loss: 13.1919\n",
            "====> Epoch: 296 loss: 8.2574\n",
            "====> Test set loss: 13.1858\n",
            "====> Epoch: 297 loss: 8.2485\n",
            "====> Test set loss: 13.1783\n",
            "====> Epoch: 298 loss: 8.2387\n",
            "====> Test set loss: 13.1719\n",
            "====> Epoch: 299 loss: 8.2295\n",
            "====> Test set loss: 13.1656\n",
            "====> Epoch: 300 loss: 8.2208\n",
            "====> Test set loss: 13.1595\n",
            "====> Epoch: 301 loss: 8.2109\n",
            "====> Test set loss: 13.1520\n",
            "====> Epoch: 302 loss: 8.2023\n",
            "====> Test set loss: 13.1450\n",
            "====> Epoch: 303 loss: 8.1930\n",
            "====> Test set loss: 13.1386\n",
            "====> Epoch: 304 loss: 8.1844\n",
            "====> Test set loss: 13.1326\n",
            "====> Epoch: 305 loss: 8.1751\n",
            "====> Test set loss: 13.1264\n",
            "====> Epoch: 306 loss: 8.1653\n",
            "====> Test set loss: 13.1190\n",
            "====> Epoch: 307 loss: 8.1573\n",
            "====> Test set loss: 13.1122\n",
            "====> Epoch: 308 loss: 8.1481\n",
            "====> Test set loss: 13.1057\n",
            "====> Epoch: 309 loss: 8.1386\n",
            "====> Test set loss: 13.0995\n",
            "====> Epoch: 310 loss: 8.1306\n",
            "====> Test set loss: 13.0938\n",
            "====> Epoch: 311 loss: 8.1216\n",
            "====> Test set loss: 13.0867\n",
            "====> Epoch: 312 loss: 8.1133\n",
            "====> Test set loss: 13.0801\n",
            "====> Epoch: 313 loss: 8.1048\n",
            "====> Test set loss: 13.0747\n",
            "====> Epoch: 314 loss: 8.0962\n",
            "====> Test set loss: 13.0675\n",
            "====> Epoch: 315 loss: 8.0876\n",
            "====> Test set loss: 13.0609\n",
            "====> Epoch: 316 loss: 8.0796\n",
            "====> Test set loss: 13.0550\n",
            "====> Epoch: 317 loss: 8.0710\n",
            "====> Test set loss: 13.0486\n",
            "====> Epoch: 318 loss: 8.0625\n",
            "====> Test set loss: 13.0422\n",
            "====> Epoch: 319 loss: 8.0540\n",
            "====> Test set loss: 13.0345\n",
            "====> Epoch: 320 loss: 8.0459\n",
            "====> Test set loss: 13.0288\n",
            "====> Epoch: 321 loss: 8.0376\n",
            "====> Test set loss: 13.0223\n",
            "====> Epoch: 322 loss: 8.0291\n",
            "====> Test set loss: 13.0160\n",
            "====> Epoch: 323 loss: 8.0210\n",
            "====> Test set loss: 13.0090\n",
            "====> Epoch: 324 loss: 8.0133\n",
            "====> Test set loss: 13.0033\n",
            "====> Epoch: 325 loss: 8.0052\n",
            "====> Test set loss: 12.9961\n",
            "====> Epoch: 326 loss: 7.9968\n",
            "====> Test set loss: 12.9895\n",
            "====> Epoch: 327 loss: 7.9885\n",
            "====> Test set loss: 12.9841\n",
            "====> Epoch: 328 loss: 7.9805\n",
            "====> Test set loss: 12.9774\n",
            "====> Epoch: 329 loss: 7.9727\n",
            "====> Test set loss: 12.9717\n",
            "====> Epoch: 330 loss: 7.9647\n",
            "====> Test set loss: 12.9649\n",
            "====> Epoch: 331 loss: 7.9567\n",
            "====> Test set loss: 12.9585\n",
            "====> Epoch: 332 loss: 7.9488\n",
            "====> Test set loss: 12.9529\n",
            "====> Epoch: 333 loss: 7.9409\n",
            "====> Test set loss: 12.9464\n",
            "====> Epoch: 334 loss: 7.9335\n",
            "====> Test set loss: 12.9404\n",
            "====> Epoch: 335 loss: 7.9254\n",
            "====> Test set loss: 12.9330\n",
            "====> Epoch: 336 loss: 7.9173\n",
            "====> Test set loss: 12.9261\n",
            "====> Epoch: 337 loss: 7.9093\n",
            "====> Test set loss: 12.9217\n",
            "====> Epoch: 338 loss: 7.9022\n",
            "====> Test set loss: 12.9147\n",
            "====> Epoch: 339 loss: 7.8948\n",
            "====> Test set loss: 12.9076\n",
            "====> Epoch: 340 loss: 7.8870\n",
            "====> Test set loss: 12.9017\n",
            "====> Epoch: 341 loss: 7.8797\n",
            "====> Test set loss: 12.8956\n",
            "====> Epoch: 342 loss: 7.8718\n",
            "====> Test set loss: 12.8890\n",
            "====> Epoch: 343 loss: 7.8642\n",
            "====> Test set loss: 12.8830\n",
            "====> Epoch: 344 loss: 7.8569\n",
            "====> Test set loss: 12.8757\n",
            "====> Epoch: 345 loss: 7.8491\n",
            "====> Test set loss: 12.8707\n",
            "====> Epoch: 346 loss: 7.8421\n",
            "====> Test set loss: 12.8648\n",
            "====> Epoch: 347 loss: 7.8347\n",
            "====> Test set loss: 12.8571\n",
            "====> Epoch: 348 loss: 7.8270\n",
            "====> Test set loss: 12.8517\n",
            "====> Epoch: 349 loss: 7.8199\n",
            "====> Test set loss: 12.8456\n",
            "====> Epoch: 350 loss: 7.8127\n",
            "====> Test set loss: 12.8393\n",
            "====> Epoch: 351 loss: 7.8051\n",
            "====> Test set loss: 12.8332\n",
            "====> Epoch: 352 loss: 7.7981\n",
            "====> Test set loss: 12.8273\n",
            "====> Epoch: 353 loss: 7.7907\n",
            "====> Test set loss: 12.8210\n",
            "====> Epoch: 354 loss: 7.7835\n",
            "====> Test set loss: 12.8148\n",
            "====> Epoch: 355 loss: 7.7769\n",
            "====> Test set loss: 12.8094\n",
            "====> Epoch: 356 loss: 7.7697\n",
            "====> Test set loss: 12.8016\n",
            "====> Epoch: 357 loss: 7.7628\n",
            "====> Test set loss: 12.7966\n",
            "====> Epoch: 358 loss: 7.7558\n",
            "====> Test set loss: 12.7907\n",
            "====> Epoch: 359 loss: 7.7486\n",
            "====> Test set loss: 12.7851\n",
            "====> Epoch: 360 loss: 7.7419\n",
            "====> Test set loss: 12.7776\n",
            "====> Epoch: 361 loss: 7.7348\n",
            "====> Test set loss: 12.7722\n",
            "====> Epoch: 362 loss: 7.7280\n",
            "====> Test set loss: 12.7660\n",
            "====> Epoch: 363 loss: 7.7211\n",
            "====> Test set loss: 12.7592\n",
            "====> Epoch: 364 loss: 7.7136\n",
            "====> Test set loss: 12.7540\n",
            "====> Epoch: 365 loss: 7.7074\n",
            "====> Test set loss: 12.7480\n",
            "====> Epoch: 366 loss: 7.7004\n",
            "====> Test set loss: 12.7415\n",
            "====> Epoch: 367 loss: 7.6934\n",
            "====> Test set loss: 12.7349\n",
            "====> Epoch: 368 loss: 7.6869\n",
            "====> Test set loss: 12.7290\n",
            "====> Epoch: 369 loss: 7.6795\n",
            "====> Test set loss: 12.7224\n",
            "====> Epoch: 370 loss: 7.6734\n",
            "====> Test set loss: 12.7167\n",
            "====> Epoch: 371 loss: 7.6666\n",
            "====> Test set loss: 12.7111\n",
            "====> Epoch: 372 loss: 7.6596\n",
            "====> Test set loss: 12.7040\n",
            "====> Epoch: 373 loss: 7.6533\n",
            "====> Test set loss: 12.6986\n",
            "====> Epoch: 374 loss: 7.6462\n",
            "====> Test set loss: 12.6934\n",
            "====> Epoch: 375 loss: 7.6403\n",
            "====> Test set loss: 12.6871\n",
            "====> Epoch: 376 loss: 7.6339\n",
            "====> Test set loss: 12.6797\n",
            "====> Epoch: 377 loss: 7.6268\n",
            "====> Test set loss: 12.6742\n",
            "====> Epoch: 378 loss: 7.6200\n",
            "====> Test set loss: 12.6687\n",
            "====> Epoch: 379 loss: 7.6137\n",
            "====> Test set loss: 12.6622\n",
            "====> Epoch: 380 loss: 7.6070\n",
            "====> Test set loss: 12.6573\n",
            "====> Epoch: 381 loss: 7.6007\n",
            "====> Test set loss: 12.6501\n",
            "====> Epoch: 382 loss: 7.5946\n",
            "====> Test set loss: 12.6444\n",
            "====> Epoch: 383 loss: 7.5882\n",
            "====> Test set loss: 12.6376\n",
            "====> Epoch: 384 loss: 7.5819\n",
            "====> Test set loss: 12.6320\n",
            "====> Epoch: 385 loss: 7.5752\n",
            "====> Test set loss: 12.6260\n",
            "====> Epoch: 386 loss: 7.5689\n",
            "====> Test set loss: 12.6190\n",
            "====> Epoch: 387 loss: 7.5629\n",
            "====> Test set loss: 12.6149\n",
            "====> Epoch: 388 loss: 7.5564\n",
            "====> Test set loss: 12.6073\n",
            "====> Epoch: 389 loss: 7.5504\n",
            "====> Test set loss: 12.6012\n",
            "====> Epoch: 390 loss: 7.5435\n",
            "====> Test set loss: 12.5952\n",
            "====> Epoch: 391 loss: 7.5372\n",
            "====> Test set loss: 12.5892\n",
            "====> Epoch: 392 loss: 7.5308\n",
            "====> Test set loss: 12.5831\n",
            "====> Epoch: 393 loss: 7.5248\n",
            "====> Test set loss: 12.5767\n",
            "====> Epoch: 394 loss: 7.5186\n",
            "====> Test set loss: 12.5714\n",
            "====> Epoch: 395 loss: 7.5124\n",
            "====> Test set loss: 12.5655\n",
            "====> Epoch: 396 loss: 7.5060\n",
            "====> Test set loss: 12.5587\n",
            "====> Epoch: 397 loss: 7.5001\n",
            "====> Test set loss: 12.5538\n",
            "====> Epoch: 398 loss: 7.4936\n",
            "====> Test set loss: 12.5467\n",
            "====> Epoch: 399 loss: 7.4881\n",
            "====> Test set loss: 12.5409\n",
            "====> Epoch: 400 loss: 7.4824\n",
            "====> Test set loss: 12.5345\n",
            "====> Epoch: 401 loss: 7.4757\n",
            "====> Test set loss: 12.5282\n",
            "====> Epoch: 402 loss: 7.4697\n",
            "====> Test set loss: 12.5225\n",
            "====> Epoch: 403 loss: 7.4638\n",
            "====> Test set loss: 12.5155\n",
            "====> Epoch: 404 loss: 7.4579\n",
            "====> Test set loss: 12.5100\n",
            "====> Epoch: 405 loss: 7.4512\n",
            "====> Test set loss: 12.5042\n",
            "====> Epoch: 406 loss: 7.4453\n",
            "====> Test set loss: 12.4975\n",
            "====> Epoch: 407 loss: 7.4399\n",
            "====> Test set loss: 12.4921\n",
            "====> Epoch: 408 loss: 7.4337\n",
            "====> Test set loss: 12.4862\n",
            "====> Epoch: 409 loss: 7.4275\n",
            "====> Test set loss: 12.4793\n",
            "====> Epoch: 410 loss: 7.4215\n",
            "====> Test set loss: 12.4736\n",
            "====> Epoch: 411 loss: 7.4156\n",
            "====> Test set loss: 12.4671\n",
            "====> Epoch: 412 loss: 7.4097\n",
            "====> Test set loss: 12.4607\n",
            "====> Epoch: 413 loss: 7.4032\n",
            "====> Test set loss: 12.4542\n",
            "====> Epoch: 414 loss: 7.3980\n",
            "====> Test set loss: 12.4494\n",
            "====> Epoch: 415 loss: 7.3920\n",
            "====> Test set loss: 12.4423\n",
            "====> Epoch: 416 loss: 7.3860\n",
            "====> Test set loss: 12.4361\n",
            "====> Epoch: 417 loss: 7.3802\n",
            "====> Test set loss: 12.4305\n",
            "====> Epoch: 418 loss: 7.3738\n",
            "====> Test set loss: 12.4239\n",
            "====> Epoch: 419 loss: 7.3679\n",
            "====> Test set loss: 12.4186\n",
            "====> Epoch: 420 loss: 7.3625\n",
            "====> Test set loss: 12.4132\n",
            "====> Epoch: 421 loss: 7.3576\n",
            "====> Test set loss: 12.4067\n",
            "====> Epoch: 422 loss: 7.3510\n",
            "====> Test set loss: 12.4004\n",
            "====> Epoch: 423 loss: 7.3455\n",
            "====> Test set loss: 12.3942\n",
            "====> Epoch: 424 loss: 7.3395\n",
            "====> Test set loss: 12.3876\n",
            "====> Epoch: 425 loss: 7.3346\n",
            "====> Test set loss: 12.3829\n",
            "====> Epoch: 426 loss: 7.3290\n",
            "====> Test set loss: 12.3757\n",
            "====> Epoch: 427 loss: 7.3231\n",
            "====> Test set loss: 12.3690\n",
            "====> Epoch: 428 loss: 7.3175\n",
            "====> Test set loss: 12.3632\n",
            "====> Epoch: 429 loss: 7.3117\n",
            "====> Test set loss: 12.3581\n",
            "====> Epoch: 430 loss: 7.3062\n",
            "====> Test set loss: 12.3510\n",
            "====> Epoch: 431 loss: 7.3005\n",
            "====> Test set loss: 12.3458\n",
            "====> Epoch: 432 loss: 7.2950\n",
            "====> Test set loss: 12.3398\n",
            "====> Epoch: 433 loss: 7.2896\n",
            "====> Test set loss: 12.3325\n",
            "====> Epoch: 434 loss: 7.2841\n",
            "====> Test set loss: 12.3284\n",
            "====> Epoch: 435 loss: 7.2783\n",
            "====> Test set loss: 12.3220\n",
            "====> Epoch: 436 loss: 7.2727\n",
            "====> Test set loss: 12.3149\n",
            "====> Epoch: 437 loss: 7.2671\n",
            "====> Test set loss: 12.3096\n",
            "====> Epoch: 438 loss: 7.2617\n",
            "====> Test set loss: 12.3038\n",
            "====> Epoch: 439 loss: 7.2562\n",
            "====> Test set loss: 12.2968\n",
            "====> Epoch: 440 loss: 7.2509\n",
            "====> Test set loss: 12.2916\n",
            "====> Epoch: 441 loss: 7.2451\n",
            "====> Test set loss: 12.2863\n",
            "====> Epoch: 442 loss: 7.2399\n",
            "====> Test set loss: 12.2794\n",
            "====> Epoch: 443 loss: 7.2344\n",
            "====> Test set loss: 12.2735\n",
            "====> Epoch: 444 loss: 7.2291\n",
            "====> Test set loss: 12.2676\n",
            "====> Epoch: 445 loss: 7.2237\n",
            "====> Test set loss: 12.2614\n",
            "====> Epoch: 446 loss: 7.2190\n",
            "====> Test set loss: 12.2556\n",
            "====> Epoch: 447 loss: 7.2139\n",
            "====> Test set loss: 12.2499\n",
            "====> Epoch: 448 loss: 7.2078\n",
            "====> Test set loss: 12.2438\n",
            "====> Epoch: 449 loss: 7.2023\n",
            "====> Test set loss: 12.2373\n",
            "====> Epoch: 450 loss: 7.1970\n",
            "====> Test set loss: 12.2326\n",
            "====> Epoch: 451 loss: 7.1927\n",
            "====> Test set loss: 12.2262\n",
            "====> Epoch: 452 loss: 7.1868\n",
            "====> Test set loss: 12.2191\n",
            "====> Epoch: 453 loss: 7.1814\n",
            "====> Test set loss: 12.2144\n",
            "====> Epoch: 454 loss: 7.1763\n",
            "====> Test set loss: 12.2090\n",
            "====> Epoch: 455 loss: 7.1710\n",
            "====> Test set loss: 12.2030\n",
            "====> Epoch: 456 loss: 7.1661\n",
            "====> Test set loss: 12.1973\n",
            "====> Epoch: 457 loss: 7.1610\n",
            "====> Test set loss: 12.1911\n",
            "====> Epoch: 458 loss: 7.1559\n",
            "====> Test set loss: 12.1853\n",
            "====> Epoch: 459 loss: 7.1506\n",
            "====> Test set loss: 12.1788\n",
            "====> Epoch: 460 loss: 7.1455\n",
            "====> Test set loss: 12.1731\n",
            "====> Epoch: 461 loss: 7.1408\n",
            "====> Test set loss: 12.1657\n",
            "====> Epoch: 462 loss: 7.1351\n",
            "====> Test set loss: 12.1612\n",
            "====> Epoch: 463 loss: 7.1301\n",
            "====> Test set loss: 12.1551\n",
            "====> Epoch: 464 loss: 7.1255\n",
            "====> Test set loss: 12.1498\n",
            "====> Epoch: 465 loss: 7.1204\n",
            "====> Test set loss: 12.1438\n",
            "====> Epoch: 466 loss: 7.1151\n",
            "====> Test set loss: 12.1378\n",
            "====> Epoch: 467 loss: 7.1102\n",
            "====> Test set loss: 12.1318\n",
            "====> Epoch: 468 loss: 7.1052\n",
            "====> Test set loss: 12.1255\n",
            "====> Epoch: 469 loss: 7.1001\n",
            "====> Test set loss: 12.1206\n",
            "====> Epoch: 470 loss: 7.0947\n",
            "====> Test set loss: 12.1145\n",
            "====> Epoch: 471 loss: 7.0902\n",
            "====> Test set loss: 12.1087\n",
            "====> Epoch: 472 loss: 7.0853\n",
            "====> Test set loss: 12.1020\n",
            "====> Epoch: 473 loss: 7.0805\n",
            "====> Test set loss: 12.0969\n",
            "====> Epoch: 474 loss: 7.0752\n",
            "====> Test set loss: 12.0904\n",
            "====> Epoch: 475 loss: 7.0701\n",
            "====> Test set loss: 12.0846\n",
            "====> Epoch: 476 loss: 7.0658\n",
            "====> Test set loss: 12.0796\n",
            "====> Epoch: 477 loss: 7.0607\n",
            "====> Test set loss: 12.0735\n",
            "====> Epoch: 478 loss: 7.0557\n",
            "====> Test set loss: 12.0672\n",
            "====> Epoch: 479 loss: 7.0510\n",
            "====> Test set loss: 12.0617\n",
            "====> Epoch: 480 loss: 7.0460\n",
            "====> Test set loss: 12.0555\n",
            "====> Epoch: 481 loss: 7.0408\n",
            "====> Test set loss: 12.0498\n",
            "====> Epoch: 482 loss: 7.0360\n",
            "====> Test set loss: 12.0439\n",
            "====> Epoch: 483 loss: 7.0315\n",
            "====> Test set loss: 12.0369\n",
            "====> Epoch: 484 loss: 7.0264\n",
            "====> Test set loss: 12.0318\n",
            "====> Epoch: 485 loss: 7.0221\n",
            "====> Test set loss: 12.0265\n",
            "====> Epoch: 486 loss: 7.0172\n",
            "====> Test set loss: 12.0209\n",
            "====> Epoch: 487 loss: 7.0120\n",
            "====> Test set loss: 12.0146\n",
            "====> Epoch: 488 loss: 7.0074\n",
            "====> Test set loss: 12.0095\n",
            "====> Epoch: 489 loss: 7.0024\n",
            "====> Test set loss: 12.0032\n",
            "====> Epoch: 490 loss: 6.9980\n",
            "====> Test set loss: 11.9985\n",
            "====> Epoch: 491 loss: 6.9929\n",
            "====> Test set loss: 11.9911\n",
            "====> Epoch: 492 loss: 6.9888\n",
            "====> Test set loss: 11.9851\n",
            "====> Epoch: 493 loss: 6.9831\n",
            "====> Test set loss: 11.9791\n",
            "====> Epoch: 494 loss: 6.9785\n",
            "====> Test set loss: 11.9736\n",
            "====> Epoch: 495 loss: 6.9739\n",
            "====> Test set loss: 11.9689\n",
            "====> Epoch: 496 loss: 6.9687\n",
            "====> Test set loss: 11.9629\n",
            "====> Epoch: 497 loss: 6.9642\n",
            "====> Test set loss: 11.9558\n",
            "====> Epoch: 498 loss: 6.9601\n",
            "====> Test set loss: 11.9491\n",
            "====> Epoch: 499 loss: 6.9551\n",
            "====> Test set loss: 11.9436\n",
            "====> Epoch: 500 loss: 6.9502\n",
            "====> Test set loss: 11.9382\n",
            "====> Test set loss: 11.9382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZjBUzDfJ3RK"
      },
      "source": [
        "y1 = y_pred"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "ZPRIRfrYJqth",
        "outputId": "a6e84cb7-c128-4620-b72a-8f2066779b4a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1ec1286f10>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee6T2qONJtaE",
        "outputId": "128a20e2-9dfe-4c7d-b2b8-24aa2569f15d"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o, y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.3363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5ApAlnVJ1mS"
      },
      "source": [
        "#"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dpFvYAFJ81R",
        "outputId": "74c8ffe8-42be-4d13-e0bc-0ff59f3ea745"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 6)\n",
        "        self.fc2 = nn.Linear(6, 3)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc3(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc4(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=6, bias=True)\n",
            "  (fc2): Linear(in_features=6, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrWkhmk7KV1_",
        "outputId": "d617555e-7984-48d7-d126-84c0bee01ea1"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4401"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8JGCdACKXiv",
        "outputId": "1356147d-85c7-4a01-821e-85ca43ea4273"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 374\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 125.6342\n",
            "====> Test set loss: 123.4495\n",
            "====> Epoch: 2 loss: 122.4277\n",
            "====> Test set loss: 119.7642\n",
            "====> Epoch: 3 loss: 117.9503\n",
            "====> Test set loss: 114.5056\n",
            "====> Epoch: 4 loss: 111.9588\n",
            "====> Test set loss: 107.8023\n",
            "====> Epoch: 5 loss: 104.4401\n",
            "====> Test set loss: 99.3194\n",
            "====> Epoch: 6 loss: 95.0937\n",
            "====> Test set loss: 89.2300\n",
            "====> Epoch: 7 loss: 84.5811\n",
            "====> Test set loss: 78.4780\n",
            "====> Epoch: 8 loss: 73.9604\n",
            "====> Test set loss: 68.0834\n",
            "====> Epoch: 9 loss: 64.0086\n",
            "====> Test set loss: 58.6566\n",
            "====> Epoch: 10 loss: 55.1662\n",
            "====> Test set loss: 50.4672\n",
            "====> Epoch: 11 loss: 47.6541\n",
            "====> Test set loss: 43.5740\n",
            "====> Epoch: 12 loss: 41.5151\n",
            "====> Test set loss: 38.1249\n",
            "====> Epoch: 13 loss: 36.6122\n",
            "====> Test set loss: 33.6825\n",
            "====> Epoch: 14 loss: 32.4703\n",
            "====> Test set loss: 29.8127\n",
            "====> Epoch: 15 loss: 28.9340\n",
            "====> Test set loss: 26.7783\n",
            "====> Epoch: 16 loss: 26.3514\n",
            "====> Test set loss: 24.7123\n",
            "====> Epoch: 17 loss: 24.6318\n",
            "====> Test set loss: 23.4209\n",
            "====> Epoch: 18 loss: 23.5337\n",
            "====> Test set loss: 22.6430\n",
            "====> Epoch: 19 loss: 22.8373\n",
            "====> Test set loss: 22.1809\n",
            "====> Epoch: 20 loss: 22.3502\n",
            "====> Test set loss: 21.8610\n",
            "====> Epoch: 21 loss: 21.9681\n",
            "====> Test set loss: 21.6261\n",
            "====> Epoch: 22 loss: 21.6356\n",
            "====> Test set loss: 21.4205\n",
            "====> Epoch: 23 loss: 21.3198\n",
            "====> Test set loss: 21.2231\n",
            "====> Epoch: 24 loss: 21.0065\n",
            "====> Test set loss: 21.0250\n",
            "====> Epoch: 25 loss: 20.6912\n",
            "====> Test set loss: 20.8204\n",
            "====> Epoch: 26 loss: 20.3575\n",
            "====> Test set loss: 20.6145\n",
            "====> Epoch: 27 loss: 20.0246\n",
            "====> Test set loss: 20.4097\n",
            "====> Epoch: 28 loss: 19.6857\n",
            "====> Test set loss: 20.2060\n",
            "====> Epoch: 29 loss: 19.3443\n",
            "====> Test set loss: 20.0004\n",
            "====> Epoch: 30 loss: 19.0082\n",
            "====> Test set loss: 19.7905\n",
            "====> Epoch: 31 loss: 18.6697\n",
            "====> Test set loss: 19.5867\n",
            "====> Epoch: 32 loss: 18.3384\n",
            "====> Test set loss: 19.3848\n",
            "====> Epoch: 33 loss: 18.0149\n",
            "====> Test set loss: 19.1849\n",
            "====> Epoch: 34 loss: 17.6934\n",
            "====> Test set loss: 18.9836\n",
            "====> Epoch: 35 loss: 17.3682\n",
            "====> Test set loss: 18.7800\n",
            "====> Epoch: 36 loss: 17.0491\n",
            "====> Test set loss: 18.5866\n",
            "====> Epoch: 37 loss: 16.7354\n",
            "====> Test set loss: 18.3913\n",
            "====> Epoch: 38 loss: 16.4212\n",
            "====> Test set loss: 18.2014\n",
            "====> Epoch: 39 loss: 16.1160\n",
            "====> Test set loss: 18.0256\n",
            "====> Epoch: 40 loss: 15.8183\n",
            "====> Test set loss: 17.8471\n",
            "====> Epoch: 41 loss: 15.5286\n",
            "====> Test set loss: 17.6706\n",
            "====> Epoch: 42 loss: 15.2396\n",
            "====> Test set loss: 17.4977\n",
            "====> Epoch: 43 loss: 14.9586\n",
            "====> Test set loss: 17.3264\n",
            "====> Epoch: 44 loss: 14.6833\n",
            "====> Test set loss: 17.1643\n",
            "====> Epoch: 45 loss: 14.4164\n",
            "====> Test set loss: 16.9990\n",
            "====> Epoch: 46 loss: 14.1564\n",
            "====> Test set loss: 16.8459\n",
            "====> Epoch: 47 loss: 13.9030\n",
            "====> Test set loss: 16.6969\n",
            "====> Epoch: 48 loss: 13.6542\n",
            "====> Test set loss: 16.5531\n",
            "====> Epoch: 49 loss: 13.4183\n",
            "====> Test set loss: 16.4123\n",
            "====> Epoch: 50 loss: 13.1850\n",
            "====> Test set loss: 16.2741\n",
            "====> Epoch: 51 loss: 12.9615\n",
            "====> Test set loss: 16.1446\n",
            "====> Epoch: 52 loss: 12.7489\n",
            "====> Test set loss: 16.0131\n",
            "====> Epoch: 53 loss: 12.5363\n",
            "====> Test set loss: 15.8888\n",
            "====> Epoch: 54 loss: 12.3316\n",
            "====> Test set loss: 15.7718\n",
            "====> Epoch: 55 loss: 12.1383\n",
            "====> Test set loss: 15.6513\n",
            "====> Epoch: 56 loss: 11.9458\n",
            "====> Test set loss: 15.5384\n",
            "====> Epoch: 57 loss: 11.7631\n",
            "====> Test set loss: 15.4334\n",
            "====> Epoch: 58 loss: 11.5880\n",
            "====> Test set loss: 15.3292\n",
            "====> Epoch: 59 loss: 11.4174\n",
            "====> Test set loss: 15.2276\n",
            "====> Epoch: 60 loss: 11.2468\n",
            "====> Test set loss: 15.1286\n",
            "====> Epoch: 61 loss: 11.0875\n",
            "====> Test set loss: 15.0325\n",
            "====> Epoch: 62 loss: 10.9332\n",
            "====> Test set loss: 14.9456\n",
            "====> Epoch: 63 loss: 10.7842\n",
            "====> Test set loss: 14.8564\n",
            "====> Epoch: 64 loss: 10.6437\n",
            "====> Test set loss: 14.7734\n",
            "====> Epoch: 65 loss: 10.5013\n",
            "====> Test set loss: 14.6888\n",
            "====> Epoch: 66 loss: 10.3636\n",
            "====> Test set loss: 14.6109\n",
            "====> Epoch: 67 loss: 10.2320\n",
            "====> Test set loss: 14.5327\n",
            "====> Epoch: 68 loss: 10.1060\n",
            "====> Test set loss: 14.4600\n",
            "====> Epoch: 69 loss: 9.9857\n",
            "====> Test set loss: 14.3894\n",
            "====> Epoch: 70 loss: 9.8637\n",
            "====> Test set loss: 14.3203\n",
            "====> Epoch: 71 loss: 9.7513\n",
            "====> Test set loss: 14.2413\n",
            "====> Epoch: 72 loss: 9.6394\n",
            "====> Test set loss: 14.1793\n",
            "====> Epoch: 73 loss: 9.5324\n",
            "====> Test set loss: 14.1096\n",
            "====> Epoch: 74 loss: 9.4289\n",
            "====> Test set loss: 14.0501\n",
            "====> Epoch: 75 loss: 9.3267\n",
            "====> Test set loss: 13.9897\n",
            "====> Epoch: 76 loss: 9.2329\n",
            "====> Test set loss: 13.9217\n",
            "====> Epoch: 77 loss: 9.1389\n",
            "====> Test set loss: 13.8643\n",
            "====> Epoch: 78 loss: 9.0472\n",
            "====> Test set loss: 13.8094\n",
            "====> Epoch: 79 loss: 8.9590\n",
            "====> Test set loss: 13.7554\n",
            "====> Epoch: 80 loss: 8.8742\n",
            "====> Test set loss: 13.6971\n",
            "====> Epoch: 81 loss: 8.7973\n",
            "====> Test set loss: 13.6447\n",
            "====> Epoch: 82 loss: 8.7121\n",
            "====> Test set loss: 13.5878\n",
            "====> Epoch: 83 loss: 8.6375\n",
            "====> Test set loss: 13.5373\n",
            "====> Epoch: 84 loss: 8.5602\n",
            "====> Test set loss: 13.4898\n",
            "====> Epoch: 85 loss: 8.4865\n",
            "====> Test set loss: 13.4429\n",
            "====> Epoch: 86 loss: 8.4171\n",
            "====> Test set loss: 13.3933\n",
            "====> Epoch: 87 loss: 8.3503\n",
            "====> Test set loss: 13.3396\n",
            "====> Epoch: 88 loss: 8.2825\n",
            "====> Test set loss: 13.2878\n",
            "====> Epoch: 89 loss: 8.2183\n",
            "====> Test set loss: 13.2433\n",
            "====> Epoch: 90 loss: 8.1520\n",
            "====> Test set loss: 13.2065\n",
            "====> Epoch: 91 loss: 8.0941\n",
            "====> Test set loss: 13.1520\n",
            "====> Epoch: 92 loss: 8.0324\n",
            "====> Test set loss: 13.1073\n",
            "====> Epoch: 93 loss: 7.9727\n",
            "====> Test set loss: 13.0611\n",
            "====> Epoch: 94 loss: 7.9152\n",
            "====> Test set loss: 13.0195\n",
            "====> Epoch: 95 loss: 7.8595\n",
            "====> Test set loss: 12.9727\n",
            "====> Epoch: 96 loss: 7.8018\n",
            "====> Test set loss: 12.9350\n",
            "====> Epoch: 97 loss: 7.7506\n",
            "====> Test set loss: 12.8900\n",
            "====> Epoch: 98 loss: 7.6982\n",
            "====> Test set loss: 12.8403\n",
            "====> Epoch: 99 loss: 7.6471\n",
            "====> Test set loss: 12.7961\n",
            "====> Epoch: 100 loss: 7.5958\n",
            "====> Test set loss: 12.7557\n",
            "====> Epoch: 101 loss: 7.5477\n",
            "====> Test set loss: 12.7177\n",
            "====> Epoch: 102 loss: 7.4996\n",
            "====> Test set loss: 12.6701\n",
            "====> Epoch: 103 loss: 7.4503\n",
            "====> Test set loss: 12.6280\n",
            "====> Epoch: 104 loss: 7.4070\n",
            "====> Test set loss: 12.5822\n",
            "====> Epoch: 105 loss: 7.3599\n",
            "====> Test set loss: 12.5448\n",
            "====> Epoch: 106 loss: 7.3178\n",
            "====> Test set loss: 12.5045\n",
            "====> Epoch: 107 loss: 7.2692\n",
            "====> Test set loss: 12.4606\n",
            "====> Epoch: 108 loss: 7.2311\n",
            "====> Test set loss: 12.4244\n",
            "====> Epoch: 109 loss: 7.1875\n",
            "====> Test set loss: 12.3835\n",
            "====> Epoch: 110 loss: 7.1486\n",
            "====> Test set loss: 12.3436\n",
            "====> Epoch: 111 loss: 7.1035\n",
            "====> Test set loss: 12.3036\n",
            "====> Epoch: 112 loss: 7.0666\n",
            "====> Test set loss: 12.2618\n",
            "====> Epoch: 113 loss: 7.0245\n",
            "====> Test set loss: 12.2220\n",
            "====> Epoch: 114 loss: 6.9888\n",
            "====> Test set loss: 12.1806\n",
            "====> Epoch: 115 loss: 6.9502\n",
            "====> Test set loss: 12.1443\n",
            "====> Epoch: 116 loss: 6.9104\n",
            "====> Test set loss: 12.1046\n",
            "====> Epoch: 117 loss: 6.8754\n",
            "====> Test set loss: 12.0635\n",
            "====> Epoch: 118 loss: 6.8365\n",
            "====> Test set loss: 12.0313\n",
            "====> Epoch: 119 loss: 6.8030\n",
            "====> Test set loss: 11.9876\n",
            "====> Epoch: 120 loss: 6.7690\n",
            "====> Test set loss: 11.9498\n",
            "====> Epoch: 121 loss: 6.7318\n",
            "====> Test set loss: 11.9095\n",
            "====> Epoch: 122 loss: 6.6981\n",
            "====> Test set loss: 11.8731\n",
            "====> Epoch: 123 loss: 6.6616\n",
            "====> Test set loss: 11.8456\n",
            "====> Epoch: 124 loss: 6.6314\n",
            "====> Test set loss: 11.8023\n",
            "====> Epoch: 125 loss: 6.5964\n",
            "====> Test set loss: 11.7605\n",
            "====> Epoch: 126 loss: 6.5643\n",
            "====> Test set loss: 11.7236\n",
            "====> Epoch: 127 loss: 6.5295\n",
            "====> Test set loss: 11.6863\n",
            "====> Epoch: 128 loss: 6.4955\n",
            "====> Test set loss: 11.6470\n",
            "====> Epoch: 129 loss: 6.4689\n",
            "====> Test set loss: 11.6115\n",
            "====> Epoch: 130 loss: 6.4370\n",
            "====> Test set loss: 11.5725\n",
            "====> Epoch: 131 loss: 6.4035\n",
            "====> Test set loss: 11.5323\n",
            "====> Epoch: 132 loss: 6.3751\n",
            "====> Test set loss: 11.5025\n",
            "====> Epoch: 133 loss: 6.3436\n",
            "====> Test set loss: 11.4571\n",
            "====> Epoch: 134 loss: 6.3148\n",
            "====> Test set loss: 11.4242\n",
            "====> Epoch: 135 loss: 6.2854\n",
            "====> Test set loss: 11.3924\n",
            "====> Epoch: 136 loss: 6.2580\n",
            "====> Test set loss: 11.3511\n",
            "====> Epoch: 137 loss: 6.2293\n",
            "====> Test set loss: 11.3212\n",
            "====> Epoch: 138 loss: 6.2003\n",
            "====> Test set loss: 11.2869\n",
            "====> Epoch: 139 loss: 6.1730\n",
            "====> Test set loss: 11.2525\n",
            "====> Epoch: 140 loss: 6.1448\n",
            "====> Test set loss: 11.2197\n",
            "====> Epoch: 141 loss: 6.1213\n",
            "====> Test set loss: 11.1768\n",
            "====> Epoch: 142 loss: 6.0914\n",
            "====> Test set loss: 11.1459\n",
            "====> Epoch: 143 loss: 6.0657\n",
            "====> Test set loss: 11.1123\n",
            "====> Epoch: 144 loss: 6.0384\n",
            "====> Test set loss: 11.0720\n",
            "====> Epoch: 145 loss: 6.0098\n",
            "====> Test set loss: 11.0432\n",
            "====> Epoch: 146 loss: 5.9873\n",
            "====> Test set loss: 11.0032\n",
            "====> Epoch: 147 loss: 5.9604\n",
            "====> Test set loss: 10.9726\n",
            "====> Epoch: 148 loss: 5.9330\n",
            "====> Test set loss: 10.9479\n",
            "====> Epoch: 149 loss: 5.9120\n",
            "====> Test set loss: 10.9088\n",
            "====> Epoch: 150 loss: 5.8857\n",
            "====> Test set loss: 10.8787\n",
            "====> Epoch: 151 loss: 5.8613\n",
            "====> Test set loss: 10.8453\n",
            "====> Epoch: 152 loss: 5.8373\n",
            "====> Test set loss: 10.8122\n",
            "====> Epoch: 153 loss: 5.8133\n",
            "====> Test set loss: 10.7803\n",
            "====> Epoch: 154 loss: 5.7939\n",
            "====> Test set loss: 10.7436\n",
            "====> Epoch: 155 loss: 5.7674\n",
            "====> Test set loss: 10.7183\n",
            "====> Epoch: 156 loss: 5.7439\n",
            "====> Test set loss: 10.6836\n",
            "====> Epoch: 157 loss: 5.7258\n",
            "====> Test set loss: 10.6489\n",
            "====> Epoch: 158 loss: 5.7020\n",
            "====> Test set loss: 10.6220\n",
            "====> Epoch: 159 loss: 5.6813\n",
            "====> Test set loss: 10.5900\n",
            "====> Epoch: 160 loss: 5.6596\n",
            "====> Test set loss: 10.5671\n",
            "====> Epoch: 161 loss: 5.6374\n",
            "====> Test set loss: 10.5298\n",
            "====> Epoch: 162 loss: 5.6174\n",
            "====> Test set loss: 10.5039\n",
            "====> Epoch: 163 loss: 5.5975\n",
            "====> Test set loss: 10.4781\n",
            "====> Epoch: 164 loss: 5.5769\n",
            "====> Test set loss: 10.4436\n",
            "====> Epoch: 165 loss: 5.5568\n",
            "====> Test set loss: 10.4191\n",
            "====> Epoch: 166 loss: 5.5355\n",
            "====> Test set loss: 10.3926\n",
            "====> Epoch: 167 loss: 5.5162\n",
            "====> Test set loss: 10.3672\n",
            "====> Epoch: 168 loss: 5.4950\n",
            "====> Test set loss: 10.3356\n",
            "====> Epoch: 169 loss: 5.4812\n",
            "====> Test set loss: 10.3071\n",
            "====> Epoch: 170 loss: 5.4602\n",
            "====> Test set loss: 10.2795\n",
            "====> Epoch: 171 loss: 5.4449\n",
            "====> Test set loss: 10.2598\n",
            "====> Epoch: 172 loss: 5.4217\n",
            "====> Test set loss: 10.2415\n",
            "====> Epoch: 173 loss: 5.4098\n",
            "====> Test set loss: 10.2163\n",
            "====> Epoch: 174 loss: 5.3908\n",
            "====> Test set loss: 10.1830\n",
            "====> Epoch: 175 loss: 5.3715\n",
            "====> Test set loss: 10.1717\n",
            "====> Epoch: 176 loss: 5.3554\n",
            "====> Test set loss: 10.1311\n",
            "====> Epoch: 177 loss: 5.3388\n",
            "====> Test set loss: 10.1142\n",
            "====> Epoch: 178 loss: 5.3226\n",
            "====> Test set loss: 10.0862\n",
            "====> Epoch: 179 loss: 5.3054\n",
            "====> Test set loss: 10.0609\n",
            "====> Epoch: 180 loss: 5.2902\n",
            "====> Test set loss: 10.0394\n",
            "====> Epoch: 181 loss: 5.2704\n",
            "====> Test set loss: 10.0177\n",
            "====> Epoch: 182 loss: 5.2620\n",
            "====> Test set loss: 9.9915\n",
            "====> Epoch: 183 loss: 5.2446\n",
            "====> Test set loss: 9.9723\n",
            "====> Epoch: 184 loss: 5.2255\n",
            "====> Test set loss: 9.9482\n",
            "====> Epoch: 185 loss: 5.2141\n",
            "====> Test set loss: 9.9267\n",
            "====> Epoch: 186 loss: 5.1974\n",
            "====> Test set loss: 9.9079\n",
            "====> Epoch: 187 loss: 5.1865\n",
            "====> Test set loss: 9.8787\n",
            "====> Epoch: 188 loss: 5.1726\n",
            "====> Test set loss: 9.8539\n",
            "====> Epoch: 189 loss: 5.1589\n",
            "====> Test set loss: 9.8332\n",
            "====> Epoch: 190 loss: 5.1432\n",
            "====> Test set loss: 9.8170\n",
            "====> Epoch: 191 loss: 5.1295\n",
            "====> Test set loss: 9.7975\n",
            "====> Epoch: 192 loss: 5.1173\n",
            "====> Test set loss: 9.7810\n",
            "====> Epoch: 193 loss: 5.1066\n",
            "====> Test set loss: 9.7503\n",
            "====> Epoch: 194 loss: 5.0909\n",
            "====> Test set loss: 9.7413\n",
            "====> Epoch: 195 loss: 5.0807\n",
            "====> Test set loss: 9.7180\n",
            "====> Epoch: 196 loss: 5.0656\n",
            "====> Test set loss: 9.6993\n",
            "====> Epoch: 197 loss: 5.0541\n",
            "====> Test set loss: 9.6808\n",
            "====> Epoch: 198 loss: 5.0423\n",
            "====> Test set loss: 9.6629\n",
            "====> Epoch: 199 loss: 5.0329\n",
            "====> Test set loss: 9.6456\n",
            "====> Epoch: 200 loss: 5.0202\n",
            "====> Test set loss: 9.6164\n",
            "====> Epoch: 201 loss: 5.0094\n",
            "====> Test set loss: 9.5995\n",
            "====> Epoch: 202 loss: 5.0010\n",
            "====> Test set loss: 9.5845\n",
            "====> Epoch: 203 loss: 4.9872\n",
            "====> Test set loss: 9.5760\n",
            "====> Epoch: 204 loss: 4.9776\n",
            "====> Test set loss: 9.5563\n",
            "====> Epoch: 205 loss: 4.9644\n",
            "====> Test set loss: 9.5345\n",
            "====> Epoch: 206 loss: 4.9541\n",
            "====> Test set loss: 9.5241\n",
            "====> Epoch: 207 loss: 4.9467\n",
            "====> Test set loss: 9.5078\n",
            "====> Epoch: 208 loss: 4.9349\n",
            "====> Test set loss: 9.4936\n",
            "====> Epoch: 209 loss: 4.9266\n",
            "====> Test set loss: 9.4776\n",
            "====> Epoch: 210 loss: 4.9169\n",
            "====> Test set loss: 9.4544\n",
            "====> Epoch: 211 loss: 4.9048\n",
            "====> Test set loss: 9.4559\n",
            "====> Epoch: 212 loss: 4.8964\n",
            "====> Test set loss: 9.4331\n",
            "====> Epoch: 213 loss: 4.8876\n",
            "====> Test set loss: 9.4156\n",
            "====> Epoch: 214 loss: 4.8816\n",
            "====> Test set loss: 9.4043\n",
            "====> Epoch: 215 loss: 4.8708\n",
            "====> Test set loss: 9.3974\n",
            "====> Epoch: 216 loss: 4.8640\n",
            "====> Test set loss: 9.3833\n",
            "====> Epoch: 217 loss: 4.8523\n",
            "====> Test set loss: 9.3716\n",
            "====> Epoch: 218 loss: 4.8429\n",
            "====> Test set loss: 9.3568\n",
            "====> Epoch: 219 loss: 4.8359\n",
            "====> Test set loss: 9.3536\n",
            "====> Epoch: 220 loss: 4.8285\n",
            "====> Test set loss: 9.3349\n",
            "====> Epoch: 221 loss: 4.8184\n",
            "====> Test set loss: 9.3217\n",
            "====> Epoch: 222 loss: 4.8114\n",
            "====> Test set loss: 9.3109\n",
            "====> Epoch: 223 loss: 4.8009\n",
            "====> Test set loss: 9.2944\n",
            "====> Epoch: 224 loss: 4.8001\n",
            "====> Test set loss: 9.2837\n",
            "====> Epoch: 225 loss: 4.7883\n",
            "====> Test set loss: 9.2693\n",
            "====> Epoch: 226 loss: 4.7836\n",
            "====> Test set loss: 9.2559\n",
            "====> Epoch: 227 loss: 4.7721\n",
            "====> Test set loss: 9.2509\n",
            "====> Epoch: 228 loss: 4.7680\n",
            "====> Test set loss: 9.2376\n",
            "====> Epoch: 229 loss: 4.7620\n",
            "====> Test set loss: 9.2313\n",
            "====> Epoch: 230 loss: 4.7522\n",
            "====> Test set loss: 9.2297\n",
            "====> Epoch: 231 loss: 4.7476\n",
            "====> Test set loss: 9.2100\n",
            "====> Epoch: 232 loss: 4.7358\n",
            "====> Test set loss: 9.2195\n",
            "====> Epoch: 233 loss: 4.7359\n",
            "====> Test set loss: 9.1875\n",
            "====> Epoch: 234 loss: 4.7279\n",
            "====> Test set loss: 9.1775\n",
            "====> Epoch: 235 loss: 4.7209\n",
            "====> Test set loss: 9.1732\n",
            "====> Epoch: 236 loss: 4.7145\n",
            "====> Test set loss: 9.1705\n",
            "====> Epoch: 237 loss: 4.7092\n",
            "====> Test set loss: 9.1593\n",
            "====> Epoch: 238 loss: 4.7029\n",
            "====> Test set loss: 9.1488\n",
            "====> Epoch: 239 loss: 4.6968\n",
            "====> Test set loss: 9.1478\n",
            "====> Epoch: 240 loss: 4.6928\n",
            "====> Test set loss: 9.1350\n",
            "====> Epoch: 241 loss: 4.6854\n",
            "====> Test set loss: 9.1223\n",
            "====> Epoch: 242 loss: 4.6801\n",
            "====> Test set loss: 9.1198\n",
            "====> Epoch: 243 loss: 4.6726\n",
            "====> Test set loss: 9.1092\n",
            "====> Epoch: 244 loss: 4.6729\n",
            "====> Test set loss: 9.1033\n",
            "====> Epoch: 245 loss: 4.6614\n",
            "====> Test set loss: 9.0998\n",
            "====> Epoch: 246 loss: 4.6572\n",
            "====> Test set loss: 9.1040\n",
            "====> Epoch: 247 loss: 4.6516\n",
            "====> Test set loss: 9.0938\n",
            "====> Epoch: 248 loss: 4.6466\n",
            "====> Test set loss: 9.0821\n",
            "====> Epoch: 249 loss: 4.6435\n",
            "====> Test set loss: 9.0692\n",
            "====> Epoch: 250 loss: 4.6376\n",
            "====> Test set loss: 9.0731\n",
            "====> Epoch: 251 loss: 4.6327\n",
            "====> Test set loss: 9.0572\n",
            "====> Epoch: 252 loss: 4.6254\n",
            "====> Test set loss: 9.0563\n",
            "====> Epoch: 253 loss: 4.6239\n",
            "====> Test set loss: 9.0436\n",
            "====> Epoch: 254 loss: 4.6177\n",
            "====> Test set loss: 9.0419\n",
            "====> Epoch: 255 loss: 4.6140\n",
            "====> Test set loss: 9.0340\n",
            "====> Epoch: 256 loss: 4.6097\n",
            "====> Test set loss: 9.0322\n",
            "====> Epoch: 257 loss: 4.6035\n",
            "====> Test set loss: 9.0278\n",
            "====> Epoch: 258 loss: 4.6018\n",
            "====> Test set loss: 9.0169\n",
            "====> Epoch: 259 loss: 4.5945\n",
            "====> Test set loss: 9.0062\n",
            "====> Epoch: 260 loss: 4.5904\n",
            "====> Test set loss: 9.0052\n",
            "====> Epoch: 261 loss: 4.5862\n",
            "====> Test set loss: 8.9985\n",
            "====> Epoch: 262 loss: 4.5829\n",
            "====> Test set loss: 8.9933\n",
            "====> Epoch: 263 loss: 4.5773\n",
            "====> Test set loss: 8.9881\n",
            "====> Epoch: 264 loss: 4.5725\n",
            "====> Test set loss: 8.9800\n",
            "====> Epoch: 265 loss: 4.5687\n",
            "====> Test set loss: 8.9739\n",
            "====> Epoch: 266 loss: 4.5652\n",
            "====> Test set loss: 8.9640\n",
            "====> Epoch: 267 loss: 4.5585\n",
            "====> Test set loss: 8.9710\n",
            "====> Epoch: 268 loss: 4.5574\n",
            "====> Test set loss: 8.9685\n",
            "====> Epoch: 269 loss: 4.5546\n",
            "====> Test set loss: 8.9527\n",
            "====> Epoch: 270 loss: 4.5483\n",
            "====> Test set loss: 8.9584\n",
            "====> Epoch: 271 loss: 4.5472\n",
            "====> Test set loss: 8.9453\n",
            "====> Epoch: 272 loss: 4.5435\n",
            "====> Test set loss: 8.9386\n",
            "====> Epoch: 273 loss: 4.5370\n",
            "====> Test set loss: 8.9429\n",
            "====> Epoch: 274 loss: 4.5345\n",
            "====> Test set loss: 8.9399\n",
            "====> Epoch: 275 loss: 4.5302\n",
            "====> Test set loss: 8.9303\n",
            "====> Epoch: 276 loss: 4.5297\n",
            "====> Test set loss: 8.9320\n",
            "====> Epoch: 277 loss: 4.5251\n",
            "====> Test set loss: 8.9166\n",
            "====> Epoch: 278 loss: 4.5215\n",
            "====> Test set loss: 8.9210\n",
            "====> Epoch: 279 loss: 4.5171\n",
            "====> Test set loss: 8.9224\n",
            "====> Epoch: 280 loss: 4.5148\n",
            "====> Test set loss: 8.9091\n",
            "====> Epoch: 281 loss: 4.5120\n",
            "====> Test set loss: 8.9064\n",
            "====> Epoch: 282 loss: 4.5065\n",
            "====> Test set loss: 8.9067\n",
            "====> Epoch: 283 loss: 4.5051\n",
            "====> Test set loss: 8.9043\n",
            "====> Epoch: 284 loss: 4.5025\n",
            "====> Test set loss: 8.8995\n",
            "====> Epoch: 285 loss: 4.4972\n",
            "====> Test set loss: 8.8996\n",
            "====> Epoch: 286 loss: 4.4954\n",
            "====> Test set loss: 8.9050\n",
            "====> Epoch: 287 loss: 4.4951\n",
            "====> Test set loss: 8.8999\n",
            "====> Epoch: 288 loss: 4.4903\n",
            "====> Test set loss: 8.8956\n",
            "====> Epoch: 289 loss: 4.4871\n",
            "====> Test set loss: 8.8960\n",
            "====> Epoch: 290 loss: 4.4827\n",
            "====> Test set loss: 8.8925\n",
            "====> Epoch: 291 loss: 4.4827\n",
            "====> Test set loss: 8.8865\n",
            "====> Epoch: 292 loss: 4.4783\n",
            "====> Test set loss: 8.8871\n",
            "====> Epoch: 293 loss: 4.4751\n",
            "====> Test set loss: 8.8836\n",
            "====> Epoch: 294 loss: 4.4740\n",
            "====> Test set loss: 8.8850\n",
            "====> Epoch: 295 loss: 4.4695\n",
            "====> Test set loss: 8.8784\n",
            "====> Epoch: 296 loss: 4.4651\n",
            "====> Test set loss: 8.8729\n",
            "====> Epoch: 297 loss: 4.4651\n",
            "====> Test set loss: 8.8726\n",
            "====> Epoch: 298 loss: 4.4620\n",
            "====> Test set loss: 8.8717\n",
            "====> Epoch: 299 loss: 4.4588\n",
            "====> Test set loss: 8.8708\n",
            "====> Epoch: 300 loss: 4.4570\n",
            "====> Test set loss: 8.8691\n",
            "====> Epoch: 301 loss: 4.4541\n",
            "====> Test set loss: 8.8644\n",
            "====> Epoch: 302 loss: 4.4520\n",
            "====> Test set loss: 8.8703\n",
            "====> Epoch: 303 loss: 4.4486\n",
            "====> Test set loss: 8.8645\n",
            "====> Epoch: 304 loss: 4.4467\n",
            "====> Test set loss: 8.8632\n",
            "====> Epoch: 305 loss: 4.4445\n",
            "====> Test set loss: 8.8631\n",
            "====> Epoch: 306 loss: 4.4400\n",
            "====> Test set loss: 8.8720\n",
            "====> Epoch: 307 loss: 4.4376\n",
            "====> Test set loss: 8.8703\n",
            "====> Epoch: 308 loss: 4.4358\n",
            "====> Test set loss: 8.8582\n",
            "====> Epoch: 309 loss: 4.4321\n",
            "====> Test set loss: 8.8600\n",
            "====> Epoch: 310 loss: 4.4352\n",
            "====> Test set loss: 8.8567\n",
            "====> Epoch: 311 loss: 4.4295\n",
            "====> Test set loss: 8.8558\n",
            "====> Epoch: 312 loss: 4.4261\n",
            "====> Test set loss: 8.8549\n",
            "====> Epoch: 313 loss: 4.4251\n",
            "====> Test set loss: 8.8559\n",
            "====> Epoch: 314 loss: 4.4221\n",
            "====> Test set loss: 8.8582\n",
            "====> Epoch: 315 loss: 4.4196\n",
            "====> Test set loss: 8.8503\n",
            "====> Epoch: 316 loss: 4.4176\n",
            "====> Test set loss: 8.8516\n",
            "====> Epoch: 317 loss: 4.4138\n",
            "====> Test set loss: 8.8523\n",
            "====> Epoch: 318 loss: 4.4124\n",
            "====> Test set loss: 8.8542\n",
            "====> Epoch: 319 loss: 4.4119\n",
            "====> Test set loss: 8.8552\n",
            "====> Epoch: 320 loss: 4.4079\n",
            "====> Test set loss: 8.8583\n",
            "====> Epoch: 321 loss: 4.4049\n",
            "====> Test set loss: 8.8492\n",
            "====> Epoch: 322 loss: 4.4025\n",
            "====> Test set loss: 8.8579\n",
            "====> Epoch: 323 loss: 4.4019\n",
            "====> Test set loss: 8.8472\n",
            "====> Epoch: 324 loss: 4.3980\n",
            "====> Test set loss: 8.8468\n",
            "====> Epoch: 325 loss: 4.3975\n",
            "====> Test set loss: 8.8479\n",
            "====> Epoch: 326 loss: 4.3948\n",
            "====> Test set loss: 8.8445\n",
            "====> Epoch: 327 loss: 4.3920\n",
            "====> Test set loss: 8.8480\n",
            "====> Epoch: 328 loss: 4.3911\n",
            "====> Test set loss: 8.8438\n",
            "====> Epoch: 329 loss: 4.3884\n",
            "====> Test set loss: 8.8509\n",
            "====> Epoch: 330 loss: 4.3855\n",
            "====> Test set loss: 8.8407\n",
            "====> Epoch: 331 loss: 4.3841\n",
            "====> Test set loss: 8.8475\n",
            "====> Epoch: 332 loss: 4.3827\n",
            "====> Test set loss: 8.8433\n",
            "====> Epoch: 333 loss: 4.3800\n",
            "====> Test set loss: 8.8440\n",
            "====> Epoch: 334 loss: 4.3803\n",
            "====> Test set loss: 8.8408\n",
            "====> Epoch: 335 loss: 4.3746\n",
            "====> Test set loss: 8.8540\n",
            "====> Epoch: 336 loss: 4.3773\n",
            "====> Test set loss: 8.8420\n",
            "====> Epoch: 337 loss: 4.3738\n",
            "====> Test set loss: 8.8483\n",
            "====> Epoch: 338 loss: 4.3714\n",
            "====> Test set loss: 8.8494\n",
            "====> Epoch: 339 loss: 4.3683\n",
            "====> Test set loss: 8.8452\n",
            "====> Epoch: 340 loss: 4.3678\n",
            "====> Test set loss: 8.8438\n",
            "====> Epoch: 341 loss: 4.3649\n",
            "====> Test set loss: 8.8437\n",
            "====> Epoch: 342 loss: 4.3621\n",
            "====> Test set loss: 8.8420\n",
            "====> Epoch: 343 loss: 4.3610\n",
            "====> Test set loss: 8.8545\n",
            "====> Epoch: 344 loss: 4.3599\n",
            "====> Test set loss: 8.8459\n",
            "====> Epoch: 345 loss: 4.3575\n",
            "====> Test set loss: 8.8515\n",
            "====> Epoch: 346 loss: 4.3542\n",
            "====> Test set loss: 8.8477\n",
            "====> Epoch: 347 loss: 4.3551\n",
            "====> Test set loss: 8.8496\n",
            "====> Epoch: 348 loss: 4.3547\n",
            "====> Test set loss: 8.8532\n",
            "====> Epoch: 349 loss: 4.3500\n",
            "====> Test set loss: 8.8494\n",
            "====> Epoch: 350 loss: 4.3479\n",
            "====> Test set loss: 8.8489\n",
            "====> Epoch: 351 loss: 4.3466\n",
            "====> Test set loss: 8.8499\n",
            "====> Epoch: 352 loss: 4.3445\n",
            "====> Test set loss: 8.8543\n",
            "====> Epoch: 353 loss: 4.3439\n",
            "====> Test set loss: 8.8539\n",
            "====> Epoch: 354 loss: 4.3422\n",
            "====> Test set loss: 8.8505\n",
            "====> Epoch: 355 loss: 4.3390\n",
            "====> Test set loss: 8.8484\n",
            "====> Epoch: 356 loss: 4.3385\n",
            "====> Test set loss: 8.8514\n",
            "====> Epoch: 357 loss: 4.3361\n",
            "====> Test set loss: 8.8531\n",
            "====> Epoch: 358 loss: 4.3332\n",
            "====> Test set loss: 8.8481\n",
            "====> Epoch: 359 loss: 4.3324\n",
            "====> Test set loss: 8.8530\n",
            "====> Epoch: 360 loss: 4.3290\n",
            "====> Test set loss: 8.8550\n",
            "====> Epoch: 361 loss: 4.3272\n",
            "====> Test set loss: 8.8574\n",
            "====> Epoch: 362 loss: 4.3278\n",
            "====> Test set loss: 8.8560\n",
            "====> Epoch: 363 loss: 4.3258\n",
            "====> Test set loss: 8.8556\n",
            "====> Epoch: 364 loss: 4.3238\n",
            "====> Test set loss: 8.8609\n",
            "====> Epoch: 365 loss: 4.3218\n",
            "====> Test set loss: 8.8598\n",
            "====> Epoch: 366 loss: 4.3184\n",
            "====> Test set loss: 8.8569\n",
            "====> Epoch: 367 loss: 4.3195\n",
            "====> Test set loss: 8.8528\n",
            "====> Epoch: 368 loss: 4.3177\n",
            "====> Test set loss: 8.8578\n",
            "====> Epoch: 369 loss: 4.3160\n",
            "====> Test set loss: 8.8695\n",
            "====> Epoch: 370 loss: 4.3129\n",
            "====> Test set loss: 8.8718\n",
            "====> Epoch: 371 loss: 4.3128\n",
            "====> Test set loss: 8.8675\n",
            "====> Epoch: 372 loss: 4.3127\n",
            "====> Test set loss: 8.8614\n",
            "====> Epoch: 373 loss: 4.3091\n",
            "====> Test set loss: 8.8630\n",
            "====> Epoch: 374 loss: 4.3085\n",
            "====> Test set loss: 8.8636\n",
            "====> Test set loss: 8.8636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4GehNDjKiBe"
      },
      "source": [
        "y2 = y_pred"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "YddurwCnKjzn",
        "outputId": "d99fd882-ca96-4f8f-8c18-0467424cd901"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1ec08254d0>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFFQ7OxIKkhB",
        "outputId": "116f1864-4bd4-4025-bffd-cd6c44c51b66"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o, y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 2.8969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcM4bDTYKlbq"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHwuSnOJKlp9",
        "outputId": "b8e527f4-8533-4d02-cd28-8b7bd9915490"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 364)\n",
        "        self.fc2 = nn.Linear(364, 182)\n",
        "        self.fc3 = nn.Linear(182, 3)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc4(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=364, bias=True)\n",
            "  (fc2): Linear(in_features=364, out_features=182, bias=True)\n",
            "  (fc3): Linear(in_features=182, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hol0TR3YeG3o",
        "outputId": "509f2e48-2c08-4959-b2c0-58773e0708b4"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "332699"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K_FXedhK2cc",
        "outputId": "729c6a8c-b7ca-42fe-b381-8b095f11a11a"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 500\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 46.3396\n",
            "====> Test set loss: 24.4722\n",
            "====> Epoch: 2 loss: 23.2851\n",
            "====> Test set loss: 22.2340\n",
            "====> Epoch: 3 loss: 19.1627\n",
            "====> Test set loss: 20.3099\n",
            "====> Epoch: 4 loss: 15.2512\n",
            "====> Test set loss: 18.4406\n",
            "====> Epoch: 5 loss: 11.6777\n",
            "====> Test set loss: 16.6822\n",
            "====> Epoch: 6 loss: 8.8621\n",
            "====> Test set loss: 15.1082\n",
            "====> Epoch: 7 loss: 6.7733\n",
            "====> Test set loss: 14.0811\n",
            "====> Epoch: 8 loss: 5.1709\n",
            "====> Test set loss: 13.2579\n",
            "====> Epoch: 9 loss: 3.9367\n",
            "====> Test set loss: 12.4155\n",
            "====> Epoch: 10 loss: 2.9850\n",
            "====> Test set loss: 11.7953\n",
            "====> Epoch: 11 loss: 2.3326\n",
            "====> Test set loss: 11.3824\n",
            "====> Epoch: 12 loss: 1.8387\n",
            "====> Test set loss: 11.2013\n",
            "====> Epoch: 13 loss: 1.4767\n",
            "====> Test set loss: 11.0893\n",
            "====> Epoch: 14 loss: 1.1778\n",
            "====> Test set loss: 10.9172\n",
            "====> Epoch: 15 loss: 0.9351\n",
            "====> Test set loss: 10.9914\n",
            "====> Epoch: 16 loss: 0.7302\n",
            "====> Test set loss: 11.0685\n",
            "====> Epoch: 17 loss: 0.5585\n",
            "====> Test set loss: 11.0861\n",
            "====> Epoch: 18 loss: 0.4124\n",
            "====> Test set loss: 11.1051\n",
            "====> Epoch: 19 loss: 0.2893\n",
            "====> Test set loss: 11.1955\n",
            "====> Epoch: 20 loss: 0.2039\n",
            "====> Test set loss: 11.2597\n",
            "====> Epoch: 21 loss: 0.1405\n",
            "====> Test set loss: 11.3182\n",
            "====> Epoch: 22 loss: 0.1177\n",
            "====> Test set loss: 11.4749\n",
            "====> Epoch: 23 loss: 0.1445\n",
            "====> Test set loss: 11.3005\n",
            "====> Epoch: 24 loss: 0.2146\n",
            "====> Test set loss: 11.7107\n",
            "====> Epoch: 25 loss: 0.2291\n",
            "====> Test set loss: 11.4677\n",
            "====> Epoch: 26 loss: 0.1843\n",
            "====> Test set loss: 11.3796\n",
            "====> Epoch: 27 loss: 0.1332\n",
            "====> Test set loss: 11.3749\n",
            "====> Epoch: 28 loss: 0.1074\n",
            "====> Test set loss: 11.3492\n",
            "====> Epoch: 29 loss: 0.1002\n",
            "====> Test set loss: 11.4803\n",
            "====> Epoch: 30 loss: 0.1008\n",
            "====> Test set loss: 11.3470\n",
            "====> Epoch: 31 loss: 0.1400\n",
            "====> Test set loss: 11.2330\n",
            "====> Epoch: 32 loss: 0.1841\n",
            "====> Test set loss: 11.5978\n",
            "====> Epoch: 33 loss: 0.1730\n",
            "====> Test set loss: 11.4295\n",
            "====> Epoch: 34 loss: 0.1390\n",
            "====> Test set loss: 11.2977\n",
            "====> Epoch: 35 loss: 0.1201\n",
            "====> Test set loss: 11.3713\n",
            "====> Epoch: 36 loss: 0.1067\n",
            "====> Test set loss: 11.3265\n",
            "====> Epoch: 37 loss: 0.1072\n",
            "====> Test set loss: 11.5083\n",
            "====> Epoch: 38 loss: 0.1294\n",
            "====> Test set loss: 11.4594\n",
            "====> Epoch: 39 loss: 0.1341\n",
            "====> Test set loss: 11.3019\n",
            "====> Epoch: 40 loss: 0.1470\n",
            "====> Test set loss: 11.3948\n",
            "====> Epoch: 41 loss: 0.1369\n",
            "====> Test set loss: 11.3578\n",
            "====> Epoch: 42 loss: 0.1204\n",
            "====> Test set loss: 11.5190\n",
            "====> Epoch: 43 loss: 0.1072\n",
            "====> Test set loss: 11.2232\n",
            "====> Epoch: 44 loss: 0.0998\n",
            "====> Test set loss: 11.3651\n",
            "====> Epoch: 45 loss: 0.0989\n",
            "====> Test set loss: 11.1749\n",
            "====> Epoch: 46 loss: 0.1104\n",
            "====> Test set loss: 11.3073\n",
            "====> Epoch: 47 loss: 0.1436\n",
            "====> Test set loss: 11.3968\n",
            "====> Epoch: 48 loss: 0.1575\n",
            "====> Test set loss: 11.3945\n",
            "====> Epoch: 49 loss: 0.1411\n",
            "====> Test set loss: 11.3280\n",
            "====> Epoch: 50 loss: 0.1031\n",
            "====> Test set loss: 11.1992\n",
            "====> Epoch: 51 loss: 0.0886\n",
            "====> Test set loss: 11.2539\n",
            "====> Epoch: 52 loss: 0.0911\n",
            "====> Test set loss: 11.2885\n",
            "====> Epoch: 53 loss: 0.1095\n",
            "====> Test set loss: 11.3400\n",
            "====> Epoch: 54 loss: 0.1193\n",
            "====> Test set loss: 11.3775\n",
            "====> Epoch: 55 loss: 0.1291\n",
            "====> Test set loss: 11.1967\n",
            "====> Epoch: 56 loss: 0.1200\n",
            "====> Test set loss: 11.3243\n",
            "====> Epoch: 57 loss: 0.1144\n",
            "====> Test set loss: 11.2006\n",
            "====> Epoch: 58 loss: 0.0994\n",
            "====> Test set loss: 11.2937\n",
            "====> Epoch: 59 loss: 0.0998\n",
            "====> Test set loss: 11.2989\n",
            "====> Epoch: 60 loss: 0.1053\n",
            "====> Test set loss: 11.4295\n",
            "====> Epoch: 61 loss: 0.1008\n",
            "====> Test set loss: 11.4029\n",
            "====> Epoch: 62 loss: 0.1064\n",
            "====> Test set loss: 11.2324\n",
            "====> Epoch: 63 loss: 0.1099\n",
            "====> Test set loss: 11.1872\n",
            "====> Epoch: 64 loss: 0.1094\n",
            "====> Test set loss: 11.3027\n",
            "====> Epoch: 65 loss: 0.1005\n",
            "====> Test set loss: 11.2659\n",
            "====> Epoch: 66 loss: 0.0966\n",
            "====> Test set loss: 11.2959\n",
            "====> Epoch: 67 loss: 0.1096\n",
            "====> Test set loss: 11.2834\n",
            "====> Epoch: 68 loss: 0.1109\n",
            "====> Test set loss: 11.2913\n",
            "====> Epoch: 69 loss: 0.1136\n",
            "====> Test set loss: 11.2547\n",
            "====> Epoch: 70 loss: 0.0998\n",
            "====> Test set loss: 11.2195\n",
            "====> Epoch: 71 loss: 0.0867\n",
            "====> Test set loss: 11.2672\n",
            "====> Epoch: 72 loss: 0.0932\n",
            "====> Test set loss: 11.2730\n",
            "====> Epoch: 73 loss: 0.0974\n",
            "====> Test set loss: 11.2439\n",
            "====> Epoch: 74 loss: 0.1025\n",
            "====> Test set loss: 11.2627\n",
            "====> Epoch: 75 loss: 0.0987\n",
            "====> Test set loss: 11.2821\n",
            "====> Epoch: 76 loss: 0.1004\n",
            "====> Test set loss: 11.2420\n",
            "====> Epoch: 77 loss: 0.0986\n",
            "====> Test set loss: 11.3508\n",
            "====> Epoch: 78 loss: 0.0951\n",
            "====> Test set loss: 11.3093\n",
            "====> Epoch: 79 loss: 0.1025\n",
            "====> Test set loss: 11.3817\n",
            "====> Epoch: 80 loss: 0.0981\n",
            "====> Test set loss: 11.1539\n",
            "====> Epoch: 81 loss: 0.0945\n",
            "====> Test set loss: 11.1618\n",
            "====> Epoch: 82 loss: 0.0909\n",
            "====> Test set loss: 11.1998\n",
            "====> Epoch: 83 loss: 0.0983\n",
            "====> Test set loss: 11.2248\n",
            "====> Epoch: 84 loss: 0.0908\n",
            "====> Test set loss: 11.1452\n",
            "====> Epoch: 85 loss: 0.0911\n",
            "====> Test set loss: 11.2688\n",
            "====> Epoch: 86 loss: 0.0922\n",
            "====> Test set loss: 11.1749\n",
            "====> Epoch: 87 loss: 0.0937\n",
            "====> Test set loss: 11.1743\n",
            "====> Epoch: 88 loss: 0.0960\n",
            "====> Test set loss: 11.2902\n",
            "====> Epoch: 89 loss: 0.0914\n",
            "====> Test set loss: 11.1787\n",
            "====> Epoch: 90 loss: 0.1017\n",
            "====> Test set loss: 11.1820\n",
            "====> Epoch: 91 loss: 0.1029\n",
            "====> Test set loss: 11.2893\n",
            "====> Epoch: 92 loss: 0.0915\n",
            "====> Test set loss: 11.1039\n",
            "====> Epoch: 93 loss: 0.0792\n",
            "====> Test set loss: 11.2349\n",
            "====> Epoch: 94 loss: 0.0769\n",
            "====> Test set loss: 11.1241\n",
            "====> Epoch: 95 loss: 0.0799\n",
            "====> Test set loss: 11.1757\n",
            "====> Epoch: 96 loss: 0.0846\n",
            "====> Test set loss: 11.0939\n",
            "====> Epoch: 97 loss: 0.0973\n",
            "====> Test set loss: 11.2165\n",
            "====> Epoch: 98 loss: 0.1038\n",
            "====> Test set loss: 11.1554\n",
            "====> Epoch: 99 loss: 0.0875\n",
            "====> Test set loss: 11.0926\n",
            "====> Epoch: 100 loss: 0.0832\n",
            "====> Test set loss: 11.0558\n",
            "====> Epoch: 101 loss: 0.0731\n",
            "====> Test set loss: 11.2114\n",
            "====> Epoch: 102 loss: 0.0814\n",
            "====> Test set loss: 11.2444\n",
            "====> Epoch: 103 loss: 0.0902\n",
            "====> Test set loss: 11.1679\n",
            "====> Epoch: 104 loss: 0.0837\n",
            "====> Test set loss: 11.0655\n",
            "====> Epoch: 105 loss: 0.0791\n",
            "====> Test set loss: 11.1294\n",
            "====> Epoch: 106 loss: 0.0856\n",
            "====> Test set loss: 11.1813\n",
            "====> Epoch: 107 loss: 0.0846\n",
            "====> Test set loss: 11.1265\n",
            "====> Epoch: 108 loss: 0.0921\n",
            "====> Test set loss: 11.1416\n",
            "====> Epoch: 109 loss: 0.0886\n",
            "====> Test set loss: 11.1598\n",
            "====> Epoch: 110 loss: 0.0878\n",
            "====> Test set loss: 11.1190\n",
            "====> Epoch: 111 loss: 0.0829\n",
            "====> Test set loss: 11.1566\n",
            "====> Epoch: 112 loss: 0.0798\n",
            "====> Test set loss: 11.1061\n",
            "====> Epoch: 113 loss: 0.0723\n",
            "====> Test set loss: 11.0632\n",
            "====> Epoch: 114 loss: 0.0804\n",
            "====> Test set loss: 11.1365\n",
            "====> Epoch: 115 loss: 0.0830\n",
            "====> Test set loss: 11.1830\n",
            "====> Epoch: 116 loss: 0.0800\n",
            "====> Test set loss: 11.1730\n",
            "====> Epoch: 117 loss: 0.0857\n",
            "====> Test set loss: 11.1090\n",
            "====> Epoch: 118 loss: 0.0828\n",
            "====> Test set loss: 11.0151\n",
            "====> Epoch: 119 loss: 0.0786\n",
            "====> Test set loss: 11.0470\n",
            "====> Epoch: 120 loss: 0.0740\n",
            "====> Test set loss: 11.1098\n",
            "====> Epoch: 121 loss: 0.0681\n",
            "====> Test set loss: 11.1335\n",
            "====> Epoch: 122 loss: 0.0758\n",
            "====> Test set loss: 11.0982\n",
            "====> Epoch: 123 loss: 0.0910\n",
            "====> Test set loss: 11.0955\n",
            "====> Epoch: 124 loss: 0.0971\n",
            "====> Test set loss: 10.9758\n",
            "====> Epoch: 125 loss: 0.0789\n",
            "====> Test set loss: 11.0500\n",
            "====> Epoch: 126 loss: 0.0746\n",
            "====> Test set loss: 11.1235\n",
            "====> Epoch: 127 loss: 0.0697\n",
            "====> Test set loss: 11.1008\n",
            "====> Epoch: 128 loss: 0.0747\n",
            "====> Test set loss: 11.0924\n",
            "====> Epoch: 129 loss: 0.0785\n",
            "====> Test set loss: 11.1202\n",
            "====> Epoch: 130 loss: 0.0739\n",
            "====> Test set loss: 10.9997\n",
            "====> Epoch: 131 loss: 0.0777\n",
            "====> Test set loss: 11.1024\n",
            "====> Epoch: 132 loss: 0.0763\n",
            "====> Test set loss: 11.1123\n",
            "====> Epoch: 133 loss: 0.0780\n",
            "====> Test set loss: 11.0893\n",
            "====> Epoch: 134 loss: 0.0795\n",
            "====> Test set loss: 10.9370\n",
            "====> Epoch: 135 loss: 0.0746\n",
            "====> Test set loss: 10.9940\n",
            "====> Epoch: 136 loss: 0.0668\n",
            "====> Test set loss: 11.0490\n",
            "====> Epoch: 137 loss: 0.0645\n",
            "====> Test set loss: 10.9202\n",
            "====> Epoch: 138 loss: 0.0682\n",
            "====> Test set loss: 11.0237\n",
            "====> Epoch: 139 loss: 0.0825\n",
            "====> Test set loss: 10.9688\n",
            "====> Epoch: 140 loss: 0.0836\n",
            "====> Test set loss: 11.0484\n",
            "====> Epoch: 141 loss: 0.0787\n",
            "====> Test set loss: 10.9901\n",
            "====> Epoch: 142 loss: 0.0677\n",
            "====> Test set loss: 11.0990\n",
            "====> Epoch: 143 loss: 0.0594\n",
            "====> Test set loss: 11.0292\n",
            "====> Epoch: 144 loss: 0.0647\n",
            "====> Test set loss: 10.9685\n",
            "====> Epoch: 145 loss: 0.0740\n",
            "====> Test set loss: 10.9802\n",
            "====> Epoch: 146 loss: 0.0782\n",
            "====> Test set loss: 11.0651\n",
            "====> Epoch: 147 loss: 0.0767\n",
            "====> Test set loss: 10.9620\n",
            "====> Epoch: 148 loss: 0.0772\n",
            "====> Test set loss: 11.0376\n",
            "====> Epoch: 149 loss: 0.0726\n",
            "====> Test set loss: 11.0312\n",
            "====> Epoch: 150 loss: 0.0685\n",
            "====> Test set loss: 11.0120\n",
            "====> Epoch: 151 loss: 0.0652\n",
            "====> Test set loss: 10.9986\n",
            "====> Epoch: 152 loss: 0.0644\n",
            "====> Test set loss: 10.9650\n",
            "====> Epoch: 153 loss: 0.0626\n",
            "====> Test set loss: 10.9938\n",
            "====> Epoch: 154 loss: 0.0710\n",
            "====> Test set loss: 11.0628\n",
            "====> Epoch: 155 loss: 0.0760\n",
            "====> Test set loss: 11.2309\n",
            "====> Epoch: 156 loss: 0.0749\n",
            "====> Test set loss: 11.0511\n",
            "====> Epoch: 157 loss: 0.0659\n",
            "====> Test set loss: 11.0090\n",
            "====> Epoch: 158 loss: 0.0666\n",
            "====> Test set loss: 10.9881\n",
            "====> Epoch: 159 loss: 0.0658\n",
            "====> Test set loss: 10.9561\n",
            "====> Epoch: 160 loss: 0.0687\n",
            "====> Test set loss: 10.9598\n",
            "====> Epoch: 161 loss: 0.0686\n",
            "====> Test set loss: 10.8767\n",
            "====> Epoch: 162 loss: 0.0748\n",
            "====> Test set loss: 10.9432\n",
            "====> Epoch: 163 loss: 0.0714\n",
            "====> Test set loss: 10.9897\n",
            "====> Epoch: 164 loss: 0.0676\n",
            "====> Test set loss: 10.9829\n",
            "====> Epoch: 165 loss: 0.0664\n",
            "====> Test set loss: 11.0586\n",
            "====> Epoch: 166 loss: 0.0666\n",
            "====> Test set loss: 11.0111\n",
            "====> Epoch: 167 loss: 0.0710\n",
            "====> Test set loss: 10.9212\n",
            "====> Epoch: 168 loss: 0.0659\n",
            "====> Test set loss: 11.0407\n",
            "====> Epoch: 169 loss: 0.0680\n",
            "====> Test set loss: 10.8404\n",
            "====> Epoch: 170 loss: 0.0739\n",
            "====> Test set loss: 11.0531\n",
            "====> Epoch: 171 loss: 0.0670\n",
            "====> Test set loss: 10.9214\n",
            "====> Epoch: 172 loss: 0.0645\n",
            "====> Test set loss: 10.9051\n",
            "====> Epoch: 173 loss: 0.0604\n",
            "====> Test set loss: 10.9449\n",
            "====> Epoch: 174 loss: 0.0551\n",
            "====> Test set loss: 10.9913\n",
            "====> Epoch: 175 loss: 0.0620\n",
            "====> Test set loss: 10.9734\n",
            "====> Epoch: 176 loss: 0.0647\n",
            "====> Test set loss: 10.9510\n",
            "====> Epoch: 177 loss: 0.0800\n",
            "====> Test set loss: 11.0248\n",
            "====> Epoch: 178 loss: 0.0794\n",
            "====> Test set loss: 10.9161\n",
            "====> Epoch: 179 loss: 0.0640\n",
            "====> Test set loss: 10.9816\n",
            "====> Epoch: 180 loss: 0.0540\n",
            "====> Test set loss: 10.9681\n",
            "====> Epoch: 181 loss: 0.0514\n",
            "====> Test set loss: 10.9710\n",
            "====> Epoch: 182 loss: 0.0643\n",
            "====> Test set loss: 10.9356\n",
            "====> Epoch: 183 loss: 0.0805\n",
            "====> Test set loss: 10.9252\n",
            "====> Epoch: 184 loss: 0.0755\n",
            "====> Test set loss: 10.9542\n",
            "====> Epoch: 185 loss: 0.0671\n",
            "====> Test set loss: 10.9499\n",
            "====> Epoch: 186 loss: 0.0575\n",
            "====> Test set loss: 10.9560\n",
            "====> Epoch: 187 loss: 0.0530\n",
            "====> Test set loss: 11.0531\n",
            "====> Epoch: 188 loss: 0.0564\n",
            "====> Test set loss: 10.9043\n",
            "====> Epoch: 189 loss: 0.0707\n",
            "====> Test set loss: 11.0242\n",
            "====> Epoch: 190 loss: 0.0675\n",
            "====> Test set loss: 10.9677\n",
            "====> Epoch: 191 loss: 0.0671\n",
            "====> Test set loss: 10.8869\n",
            "====> Epoch: 192 loss: 0.0673\n",
            "====> Test set loss: 10.9871\n",
            "====> Epoch: 193 loss: 0.0612\n",
            "====> Test set loss: 10.9600\n",
            "====> Epoch: 194 loss: 0.0673\n",
            "====> Test set loss: 10.8697\n",
            "====> Epoch: 195 loss: 0.0625\n",
            "====> Test set loss: 10.9461\n",
            "====> Epoch: 196 loss: 0.0613\n",
            "====> Test set loss: 10.9362\n",
            "====> Epoch: 197 loss: 0.0587\n",
            "====> Test set loss: 10.9234\n",
            "====> Epoch: 198 loss: 0.0576\n",
            "====> Test set loss: 10.9604\n",
            "====> Epoch: 199 loss: 0.0617\n",
            "====> Test set loss: 11.0789\n",
            "====> Epoch: 200 loss: 0.0568\n",
            "====> Test set loss: 10.9720\n",
            "====> Epoch: 201 loss: 0.0574\n",
            "====> Test set loss: 10.9504\n",
            "====> Epoch: 202 loss: 0.0599\n",
            "====> Test set loss: 10.8471\n",
            "====> Epoch: 203 loss: 0.0601\n",
            "====> Test set loss: 10.8158\n",
            "====> Epoch: 204 loss: 0.0632\n",
            "====> Test set loss: 10.9182\n",
            "====> Epoch: 205 loss: 0.0673\n",
            "====> Test set loss: 10.8943\n",
            "====> Epoch: 206 loss: 0.0639\n",
            "====> Test set loss: 11.0298\n",
            "====> Epoch: 207 loss: 0.0552\n",
            "====> Test set loss: 10.8686\n",
            "====> Epoch: 208 loss: 0.0505\n",
            "====> Test set loss: 10.9281\n",
            "====> Epoch: 209 loss: 0.0648\n",
            "====> Test set loss: 10.9218\n",
            "====> Epoch: 210 loss: 0.0763\n",
            "====> Test set loss: 10.9514\n",
            "====> Epoch: 211 loss: 0.0704\n",
            "====> Test set loss: 10.9485\n",
            "====> Epoch: 212 loss: 0.0661\n",
            "====> Test set loss: 10.8496\n",
            "====> Epoch: 213 loss: 0.0531\n",
            "====> Test set loss: 10.8524\n",
            "====> Epoch: 214 loss: 0.0443\n",
            "====> Test set loss: 10.9553\n",
            "====> Epoch: 215 loss: 0.0482\n",
            "====> Test set loss: 10.9444\n",
            "====> Epoch: 216 loss: 0.0577\n",
            "====> Test set loss: 11.0587\n",
            "====> Epoch: 217 loss: 0.0615\n",
            "====> Test set loss: 10.9760\n",
            "====> Epoch: 218 loss: 0.0608\n",
            "====> Test set loss: 10.8670\n",
            "====> Epoch: 219 loss: 0.0586\n",
            "====> Test set loss: 10.9285\n",
            "====> Epoch: 220 loss: 0.0593\n",
            "====> Test set loss: 10.9915\n",
            "====> Epoch: 221 loss: 0.0574\n",
            "====> Test set loss: 10.8784\n",
            "====> Epoch: 222 loss: 0.0564\n",
            "====> Test set loss: 10.8923\n",
            "====> Epoch: 223 loss: 0.0514\n",
            "====> Test set loss: 10.8603\n",
            "====> Epoch: 224 loss: 0.0551\n",
            "====> Test set loss: 10.8659\n",
            "====> Epoch: 225 loss: 0.0539\n",
            "====> Test set loss: 10.8366\n",
            "====> Epoch: 226 loss: 0.0544\n",
            "====> Test set loss: 11.0074\n",
            "====> Epoch: 227 loss: 0.0572\n",
            "====> Test set loss: 10.8012\n",
            "====> Epoch: 228 loss: 0.0634\n",
            "====> Test set loss: 10.9549\n",
            "====> Epoch: 229 loss: 0.0596\n",
            "====> Test set loss: 11.0097\n",
            "====> Epoch: 230 loss: 0.0555\n",
            "====> Test set loss: 10.8034\n",
            "====> Epoch: 231 loss: 0.0511\n",
            "====> Test set loss: 10.9454\n",
            "====> Epoch: 232 loss: 0.0484\n",
            "====> Test set loss: 10.8540\n",
            "====> Epoch: 233 loss: 0.0545\n",
            "====> Test set loss: 10.8032\n",
            "====> Epoch: 234 loss: 0.0604\n",
            "====> Test set loss: 10.9219\n",
            "====> Epoch: 235 loss: 0.0619\n",
            "====> Test set loss: 10.8651\n",
            "====> Epoch: 236 loss: 0.0573\n",
            "====> Test set loss: 10.8665\n",
            "====> Epoch: 237 loss: 0.0508\n",
            "====> Test set loss: 10.9224\n",
            "====> Epoch: 238 loss: 0.0519\n",
            "====> Test set loss: 10.7519\n",
            "====> Epoch: 239 loss: 0.0571\n",
            "====> Test set loss: 10.8388\n",
            "====> Epoch: 240 loss: 0.0560\n",
            "====> Test set loss: 10.8851\n",
            "====> Epoch: 241 loss: 0.0504\n",
            "====> Test set loss: 10.9456\n",
            "====> Epoch: 242 loss: 0.0504\n",
            "====> Test set loss: 10.8347\n",
            "====> Epoch: 243 loss: 0.0489\n",
            "====> Test set loss: 10.7875\n",
            "====> Epoch: 244 loss: 0.0556\n",
            "====> Test set loss: 10.8089\n",
            "====> Epoch: 245 loss: 0.0593\n",
            "====> Test set loss: 10.9083\n",
            "====> Epoch: 246 loss: 0.0595\n",
            "====> Test set loss: 10.8156\n",
            "====> Epoch: 247 loss: 0.0529\n",
            "====> Test set loss: 10.8459\n",
            "====> Epoch: 248 loss: 0.0613\n",
            "====> Test set loss: 10.8978\n",
            "====> Epoch: 249 loss: 0.0565\n",
            "====> Test set loss: 10.9213\n",
            "====> Epoch: 250 loss: 0.0469\n",
            "====> Test set loss: 10.8731\n",
            "====> Epoch: 251 loss: 0.0429\n",
            "====> Test set loss: 10.7881\n",
            "====> Epoch: 252 loss: 0.0458\n",
            "====> Test set loss: 10.8900\n",
            "====> Epoch: 253 loss: 0.0525\n",
            "====> Test set loss: 10.8536\n",
            "====> Epoch: 254 loss: 0.0604\n",
            "====> Test set loss: 10.8187\n",
            "====> Epoch: 255 loss: 0.0608\n",
            "====> Test set loss: 10.9126\n",
            "====> Epoch: 256 loss: 0.0605\n",
            "====> Test set loss: 10.8871\n",
            "====> Epoch: 257 loss: 0.0546\n",
            "====> Test set loss: 10.8668\n",
            "====> Epoch: 258 loss: 0.0434\n",
            "====> Test set loss: 10.9503\n",
            "====> Epoch: 259 loss: 0.0409\n",
            "====> Test set loss: 10.8623\n",
            "====> Epoch: 260 loss: 0.0427\n",
            "====> Test set loss: 10.9264\n",
            "====> Epoch: 261 loss: 0.0536\n",
            "====> Test set loss: 10.7979\n",
            "====> Epoch: 262 loss: 0.0621\n",
            "====> Test set loss: 10.8841\n",
            "====> Epoch: 263 loss: 0.0660\n",
            "====> Test set loss: 10.8484\n",
            "====> Epoch: 264 loss: 0.0597\n",
            "====> Test set loss: 10.8197\n",
            "====> Epoch: 265 loss: 0.0577\n",
            "====> Test set loss: 10.8311\n",
            "====> Epoch: 266 loss: 0.0488\n",
            "====> Test set loss: 10.8534\n",
            "====> Epoch: 267 loss: 0.0420\n",
            "====> Test set loss: 10.8267\n",
            "====> Epoch: 268 loss: 0.0437\n",
            "====> Test set loss: 10.7896\n",
            "====> Epoch: 269 loss: 0.0492\n",
            "====> Test set loss: 10.9051\n",
            "====> Epoch: 270 loss: 0.0498\n",
            "====> Test set loss: 10.7759\n",
            "====> Epoch: 271 loss: 0.0541\n",
            "====> Test set loss: 10.9077\n",
            "====> Epoch: 272 loss: 0.0517\n",
            "====> Test set loss: 10.8220\n",
            "====> Epoch: 273 loss: 0.0540\n",
            "====> Test set loss: 10.7789\n",
            "====> Epoch: 274 loss: 0.0567\n",
            "====> Test set loss: 10.8316\n",
            "====> Epoch: 275 loss: 0.0570\n",
            "====> Test set loss: 10.8919\n",
            "====> Epoch: 276 loss: 0.0519\n",
            "====> Test set loss: 10.8690\n",
            "====> Epoch: 277 loss: 0.0485\n",
            "====> Test set loss: 10.8540\n",
            "====> Epoch: 278 loss: 0.0488\n",
            "====> Test set loss: 10.8561\n",
            "====> Epoch: 279 loss: 0.0501\n",
            "====> Test set loss: 10.8164\n",
            "====> Epoch: 280 loss: 0.0480\n",
            "====> Test set loss: 10.8619\n",
            "====> Epoch: 281 loss: 0.0502\n",
            "====> Test set loss: 10.9101\n",
            "====> Epoch: 282 loss: 0.0489\n",
            "====> Test set loss: 10.8340\n",
            "====> Epoch: 283 loss: 0.0472\n",
            "====> Test set loss: 10.7751\n",
            "====> Epoch: 284 loss: 0.0476\n",
            "====> Test set loss: 10.8288\n",
            "====> Epoch: 285 loss: 0.0491\n",
            "====> Test set loss: 10.8814\n",
            "====> Epoch: 286 loss: 0.0513\n",
            "====> Test set loss: 10.8634\n",
            "====> Epoch: 287 loss: 0.0522\n",
            "====> Test set loss: 10.7516\n",
            "====> Epoch: 288 loss: 0.0539\n",
            "====> Test set loss: 10.8417\n",
            "====> Epoch: 289 loss: 0.0500\n",
            "====> Test set loss: 10.7500\n",
            "====> Epoch: 290 loss: 0.0460\n",
            "====> Test set loss: 10.8361\n",
            "====> Epoch: 291 loss: 0.0454\n",
            "====> Test set loss: 10.8401\n",
            "====> Epoch: 292 loss: 0.0450\n",
            "====> Test set loss: 10.8589\n",
            "====> Epoch: 293 loss: 0.0467\n",
            "====> Test set loss: 10.7741\n",
            "====> Epoch: 294 loss: 0.0514\n",
            "====> Test set loss: 10.8242\n",
            "====> Epoch: 295 loss: 0.0497\n",
            "====> Test set loss: 10.7710\n",
            "====> Epoch: 296 loss: 0.0467\n",
            "====> Test set loss: 10.7370\n",
            "====> Epoch: 297 loss: 0.0568\n",
            "====> Test set loss: 10.7804\n",
            "====> Epoch: 298 loss: 0.0547\n",
            "====> Test set loss: 10.8946\n",
            "====> Epoch: 299 loss: 0.0506\n",
            "====> Test set loss: 10.8108\n",
            "====> Epoch: 300 loss: 0.0477\n",
            "====> Test set loss: 10.7732\n",
            "====> Epoch: 301 loss: 0.0443\n",
            "====> Test set loss: 10.9067\n",
            "====> Epoch: 302 loss: 0.0437\n",
            "====> Test set loss: 10.7756\n",
            "====> Epoch: 303 loss: 0.0493\n",
            "====> Test set loss: 10.8568\n",
            "====> Epoch: 304 loss: 0.0501\n",
            "====> Test set loss: 10.8312\n",
            "====> Epoch: 305 loss: 0.0545\n",
            "====> Test set loss: 10.8025\n",
            "====> Epoch: 306 loss: 0.0491\n",
            "====> Test set loss: 10.7306\n",
            "====> Epoch: 307 loss: 0.0446\n",
            "====> Test set loss: 10.7059\n",
            "====> Epoch: 308 loss: 0.0455\n",
            "====> Test set loss: 10.8938\n",
            "====> Epoch: 309 loss: 0.0506\n",
            "====> Test set loss: 10.7904\n",
            "====> Epoch: 310 loss: 0.0472\n",
            "====> Test set loss: 10.8782\n",
            "====> Epoch: 311 loss: 0.0472\n",
            "====> Test set loss: 10.6855\n",
            "====> Epoch: 312 loss: 0.0479\n",
            "====> Test set loss: 10.7544\n",
            "====> Epoch: 313 loss: 0.0463\n",
            "====> Test set loss: 10.9075\n",
            "====> Epoch: 314 loss: 0.0473\n",
            "====> Test set loss: 10.8304\n",
            "====> Epoch: 315 loss: 0.0488\n",
            "====> Test set loss: 10.7562\n",
            "====> Epoch: 316 loss: 0.0504\n",
            "====> Test set loss: 10.8171\n",
            "====> Epoch: 317 loss: 0.0497\n",
            "====> Test set loss: 10.8068\n",
            "====> Epoch: 318 loss: 0.0410\n",
            "====> Test set loss: 10.7736\n",
            "====> Epoch: 319 loss: 0.0427\n",
            "====> Test set loss: 10.7861\n",
            "====> Epoch: 320 loss: 0.0449\n",
            "====> Test set loss: 10.8172\n",
            "====> Epoch: 321 loss: 0.0434\n",
            "====> Test set loss: 10.7090\n",
            "====> Epoch: 322 loss: 0.0471\n",
            "====> Test set loss: 10.8318\n",
            "====> Epoch: 323 loss: 0.0463\n",
            "====> Test set loss: 10.7649\n",
            "====> Epoch: 324 loss: 0.0454\n",
            "====> Test set loss: 10.7859\n",
            "====> Epoch: 325 loss: 0.0419\n",
            "====> Test set loss: 10.7813\n",
            "====> Epoch: 326 loss: 0.0467\n",
            "====> Test set loss: 10.8476\n",
            "====> Epoch: 327 loss: 0.0494\n",
            "====> Test set loss: 10.9023\n",
            "====> Epoch: 328 loss: 0.0564\n",
            "====> Test set loss: 10.8148\n",
            "====> Epoch: 329 loss: 0.0518\n",
            "====> Test set loss: 10.7244\n",
            "====> Epoch: 330 loss: 0.0442\n",
            "====> Test set loss: 10.8381\n",
            "====> Epoch: 331 loss: 0.0364\n",
            "====> Test set loss: 10.7975\n",
            "====> Epoch: 332 loss: 0.0380\n",
            "====> Test set loss: 10.7649\n",
            "====> Epoch: 333 loss: 0.0388\n",
            "====> Test set loss: 10.8640\n",
            "====> Epoch: 334 loss: 0.0431\n",
            "====> Test set loss: 10.7725\n",
            "====> Epoch: 335 loss: 0.0476\n",
            "====> Test set loss: 10.8171\n",
            "====> Epoch: 336 loss: 0.0502\n",
            "====> Test set loss: 10.7932\n",
            "====> Epoch: 337 loss: 0.0471\n",
            "====> Test set loss: 10.7500\n",
            "====> Epoch: 338 loss: 0.0425\n",
            "====> Test set loss: 10.7994\n",
            "====> Epoch: 339 loss: 0.0454\n",
            "====> Test set loss: 10.7867\n",
            "====> Epoch: 340 loss: 0.0444\n",
            "====> Test set loss: 10.7103\n",
            "====> Epoch: 341 loss: 0.0459\n",
            "====> Test set loss: 10.6909\n",
            "====> Epoch: 342 loss: 0.0467\n",
            "====> Test set loss: 10.7261\n",
            "====> Epoch: 343 loss: 0.0473\n",
            "====> Test set loss: 10.7856\n",
            "====> Epoch: 344 loss: 0.0444\n",
            "====> Test set loss: 10.7256\n",
            "====> Epoch: 345 loss: 0.0490\n",
            "====> Test set loss: 10.7801\n",
            "====> Epoch: 346 loss: 0.0406\n",
            "====> Test set loss: 10.7943\n",
            "====> Epoch: 347 loss: 0.0387\n",
            "====> Test set loss: 10.8057\n",
            "====> Epoch: 348 loss: 0.0439\n",
            "====> Test set loss: 10.7984\n",
            "====> Epoch: 349 loss: 0.0428\n",
            "====> Test set loss: 10.8014\n",
            "====> Epoch: 350 loss: 0.0432\n",
            "====> Test set loss: 10.7442\n",
            "====> Epoch: 351 loss: 0.0431\n",
            "====> Test set loss: 10.7619\n",
            "====> Epoch: 352 loss: 0.0446\n",
            "====> Test set loss: 10.8619\n",
            "====> Epoch: 353 loss: 0.0437\n",
            "====> Test set loss: 10.7679\n",
            "====> Epoch: 354 loss: 0.0474\n",
            "====> Test set loss: 10.7603\n",
            "====> Epoch: 355 loss: 0.0467\n",
            "====> Test set loss: 10.7203\n",
            "====> Epoch: 356 loss: 0.0416\n",
            "====> Test set loss: 10.7334\n",
            "====> Epoch: 357 loss: 0.0408\n",
            "====> Test set loss: 10.7720\n",
            "====> Epoch: 358 loss: 0.0401\n",
            "====> Test set loss: 10.7472\n",
            "====> Epoch: 359 loss: 0.0408\n",
            "====> Test set loss: 10.7483\n",
            "====> Epoch: 360 loss: 0.0463\n",
            "====> Test set loss: 10.7425\n",
            "====> Epoch: 361 loss: 0.0464\n",
            "====> Test set loss: 10.7376\n",
            "====> Epoch: 362 loss: 0.0443\n",
            "====> Test set loss: 10.7571\n",
            "====> Epoch: 363 loss: 0.0427\n",
            "====> Test set loss: 10.7692\n",
            "====> Epoch: 364 loss: 0.0443\n",
            "====> Test set loss: 10.8176\n",
            "====> Epoch: 365 loss: 0.0440\n",
            "====> Test set loss: 10.7398\n",
            "====> Epoch: 366 loss: 0.0409\n",
            "====> Test set loss: 10.7503\n",
            "====> Epoch: 367 loss: 0.0363\n",
            "====> Test set loss: 10.6975\n",
            "====> Epoch: 368 loss: 0.0379\n",
            "====> Test set loss: 10.8039\n",
            "====> Epoch: 369 loss: 0.0434\n",
            "====> Test set loss: 10.8128\n",
            "====> Epoch: 370 loss: 0.0445\n",
            "====> Test set loss: 10.7721\n",
            "====> Epoch: 371 loss: 0.0426\n",
            "====> Test set loss: 10.6875\n",
            "====> Epoch: 372 loss: 0.0423\n",
            "====> Test set loss: 10.7568\n",
            "====> Epoch: 373 loss: 0.0419\n",
            "====> Test set loss: 10.8370\n",
            "====> Epoch: 374 loss: 0.0427\n",
            "====> Test set loss: 10.6422\n",
            "====> Epoch: 375 loss: 0.0405\n",
            "====> Test set loss: 10.7272\n",
            "====> Epoch: 376 loss: 0.0383\n",
            "====> Test set loss: 10.7568\n",
            "====> Epoch: 377 loss: 0.0446\n",
            "====> Test set loss: 10.6813\n",
            "====> Epoch: 378 loss: 0.0490\n",
            "====> Test set loss: 10.7392\n",
            "====> Epoch: 379 loss: 0.0430\n",
            "====> Test set loss: 10.8257\n",
            "====> Epoch: 380 loss: 0.0401\n",
            "====> Test set loss: 10.7489\n",
            "====> Epoch: 381 loss: 0.0350\n",
            "====> Test set loss: 10.8220\n",
            "====> Epoch: 382 loss: 0.0368\n",
            "====> Test set loss: 10.8087\n",
            "====> Epoch: 383 loss: 0.0404\n",
            "====> Test set loss: 10.7189\n",
            "====> Epoch: 384 loss: 0.0405\n",
            "====> Test set loss: 10.7395\n",
            "====> Epoch: 385 loss: 0.0419\n",
            "====> Test set loss: 10.6872\n",
            "====> Epoch: 386 loss: 0.0385\n",
            "====> Test set loss: 10.6724\n",
            "====> Epoch: 387 loss: 0.0390\n",
            "====> Test set loss: 10.7382\n",
            "====> Epoch: 388 loss: 0.0417\n",
            "====> Test set loss: 10.7238\n",
            "====> Epoch: 389 loss: 0.0458\n",
            "====> Test set loss: 10.7261\n",
            "====> Epoch: 390 loss: 0.0451\n",
            "====> Test set loss: 10.8344\n",
            "====> Epoch: 391 loss: 0.0426\n",
            "====> Test set loss: 10.7477\n",
            "====> Epoch: 392 loss: 0.0422\n",
            "====> Test set loss: 10.7841\n",
            "====> Epoch: 393 loss: 0.0374\n",
            "====> Test set loss: 10.7551\n",
            "====> Epoch: 394 loss: 0.0398\n",
            "====> Test set loss: 10.7318\n",
            "====> Epoch: 395 loss: 0.0387\n",
            "====> Test set loss: 10.6712\n",
            "====> Epoch: 396 loss: 0.0379\n",
            "====> Test set loss: 10.6631\n",
            "====> Epoch: 397 loss: 0.0401\n",
            "====> Test set loss: 10.6669\n",
            "====> Epoch: 398 loss: 0.0371\n",
            "====> Test set loss: 10.7570\n",
            "====> Epoch: 399 loss: 0.0392\n",
            "====> Test set loss: 10.6801\n",
            "====> Epoch: 400 loss: 0.0447\n",
            "====> Test set loss: 10.6871\n",
            "====> Epoch: 401 loss: 0.0418\n",
            "====> Test set loss: 10.7043\n",
            "====> Epoch: 402 loss: 0.0394\n",
            "====> Test set loss: 10.6946\n",
            "====> Epoch: 403 loss: 0.0353\n",
            "====> Test set loss: 10.6994\n",
            "====> Epoch: 404 loss: 0.0389\n",
            "====> Test set loss: 10.7159\n",
            "====> Epoch: 405 loss: 0.0416\n",
            "====> Test set loss: 10.7006\n",
            "====> Epoch: 406 loss: 0.0429\n",
            "====> Test set loss: 10.7495\n",
            "====> Epoch: 407 loss: 0.0426\n",
            "====> Test set loss: 10.7391\n",
            "====> Epoch: 408 loss: 0.0424\n",
            "====> Test set loss: 10.7798\n",
            "====> Epoch: 409 loss: 0.0445\n",
            "====> Test set loss: 10.7902\n",
            "====> Epoch: 410 loss: 0.0403\n",
            "====> Test set loss: 10.7108\n",
            "====> Epoch: 411 loss: 0.0347\n",
            "====> Test set loss: 10.6838\n",
            "====> Epoch: 412 loss: 0.0345\n",
            "====> Test set loss: 10.7600\n",
            "====> Epoch: 413 loss: 0.0384\n",
            "====> Test set loss: 10.7520\n",
            "====> Epoch: 414 loss: 0.0468\n",
            "====> Test set loss: 10.6804\n",
            "====> Epoch: 415 loss: 0.0420\n",
            "====> Test set loss: 10.7036\n",
            "====> Epoch: 416 loss: 0.0383\n",
            "====> Test set loss: 10.6379\n",
            "====> Epoch: 417 loss: 0.0361\n",
            "====> Test set loss: 10.7281\n",
            "====> Epoch: 418 loss: 0.0374\n",
            "====> Test set loss: 10.6749\n",
            "====> Epoch: 419 loss: 0.0388\n",
            "====> Test set loss: 10.6967\n",
            "====> Epoch: 420 loss: 0.0382\n",
            "====> Test set loss: 10.7341\n",
            "====> Epoch: 421 loss: 0.0374\n",
            "====> Test set loss: 10.8994\n",
            "====> Epoch: 422 loss: 0.0364\n",
            "====> Test set loss: 10.5805\n",
            "====> Epoch: 423 loss: 0.0404\n",
            "====> Test set loss: 10.7185\n",
            "====> Epoch: 424 loss: 0.0408\n",
            "====> Test set loss: 10.6561\n",
            "====> Epoch: 425 loss: 0.0388\n",
            "====> Test set loss: 10.7429\n",
            "====> Epoch: 426 loss: 0.0352\n",
            "====> Test set loss: 10.7057\n",
            "====> Epoch: 427 loss: 0.0352\n",
            "====> Test set loss: 10.7566\n",
            "====> Epoch: 428 loss: 0.0374\n",
            "====> Test set loss: 10.7000\n",
            "====> Epoch: 429 loss: 0.0388\n",
            "====> Test set loss: 10.6183\n",
            "====> Epoch: 430 loss: 0.0423\n",
            "====> Test set loss: 10.6083\n",
            "====> Epoch: 431 loss: 0.0391\n",
            "====> Test set loss: 10.7779\n",
            "====> Epoch: 432 loss: 0.0395\n",
            "====> Test set loss: 10.6262\n",
            "====> Epoch: 433 loss: 0.0377\n",
            "====> Test set loss: 10.7454\n",
            "====> Epoch: 434 loss: 0.0356\n",
            "====> Test set loss: 10.7000\n",
            "====> Epoch: 435 loss: 0.0406\n",
            "====> Test set loss: 10.6820\n",
            "====> Epoch: 436 loss: 0.0404\n",
            "====> Test set loss: 10.6751\n",
            "====> Epoch: 437 loss: 0.0370\n",
            "====> Test set loss: 10.7105\n",
            "====> Epoch: 438 loss: 0.0352\n",
            "====> Test set loss: 10.6832\n",
            "====> Epoch: 439 loss: 0.0351\n",
            "====> Test set loss: 10.7083\n",
            "====> Epoch: 440 loss: 0.0344\n",
            "====> Test set loss: 10.7147\n",
            "====> Epoch: 441 loss: 0.0391\n",
            "====> Test set loss: 10.7672\n",
            "====> Epoch: 442 loss: 0.0461\n",
            "====> Test set loss: 10.5692\n",
            "====> Epoch: 443 loss: 0.0443\n",
            "====> Test set loss: 10.7890\n",
            "====> Epoch: 444 loss: 0.0371\n",
            "====> Test set loss: 10.6189\n",
            "====> Epoch: 445 loss: 0.0294\n",
            "====> Test set loss: 10.6344\n",
            "====> Epoch: 446 loss: 0.0315\n",
            "====> Test set loss: 10.8307\n",
            "====> Epoch: 447 loss: 0.0346\n",
            "====> Test set loss: 10.6434\n",
            "====> Epoch: 448 loss: 0.0350\n",
            "====> Test set loss: 10.7117\n",
            "====> Epoch: 449 loss: 0.0357\n",
            "====> Test set loss: 10.6456\n",
            "====> Epoch: 450 loss: 0.0389\n",
            "====> Test set loss: 10.7631\n",
            "====> Epoch: 451 loss: 0.0410\n",
            "====> Test set loss: 10.6739\n",
            "====> Epoch: 452 loss: 0.0390\n",
            "====> Test set loss: 10.6226\n",
            "====> Epoch: 453 loss: 0.0362\n",
            "====> Test set loss: 10.6733\n",
            "====> Epoch: 454 loss: 0.0302\n",
            "====> Test set loss: 10.6457\n",
            "====> Epoch: 455 loss: 0.0320\n",
            "====> Test set loss: 10.6225\n",
            "====> Epoch: 456 loss: 0.0414\n",
            "====> Test set loss: 10.6547\n",
            "====> Epoch: 457 loss: 0.0442\n",
            "====> Test set loss: 10.7151\n",
            "====> Epoch: 458 loss: 0.0418\n",
            "====> Test set loss: 10.6113\n",
            "====> Epoch: 459 loss: 0.0327\n",
            "====> Test set loss: 10.6873\n",
            "====> Epoch: 460 loss: 0.0297\n",
            "====> Test set loss: 10.6394\n",
            "====> Epoch: 461 loss: 0.0353\n",
            "====> Test set loss: 10.7955\n",
            "====> Epoch: 462 loss: 0.0400\n",
            "====> Test set loss: 10.7069\n",
            "====> Epoch: 463 loss: 0.0390\n",
            "====> Test set loss: 10.6604\n",
            "====> Epoch: 464 loss: 0.0382\n",
            "====> Test set loss: 10.7067\n",
            "====> Epoch: 465 loss: 0.0364\n",
            "====> Test set loss: 10.7801\n",
            "====> Epoch: 466 loss: 0.0369\n",
            "====> Test set loss: 10.6281\n",
            "====> Epoch: 467 loss: 0.0351\n",
            "====> Test set loss: 10.7241\n",
            "====> Epoch: 468 loss: 0.0327\n",
            "====> Test set loss: 10.6465\n",
            "====> Epoch: 469 loss: 0.0344\n",
            "====> Test set loss: 10.6717\n",
            "====> Epoch: 470 loss: 0.0331\n",
            "====> Test set loss: 10.6536\n",
            "====> Epoch: 471 loss: 0.0347\n",
            "====> Test set loss: 10.6020\n",
            "====> Epoch: 472 loss: 0.0372\n",
            "====> Test set loss: 10.6912\n",
            "====> Epoch: 473 loss: 0.0418\n",
            "====> Test set loss: 10.7274\n",
            "====> Epoch: 474 loss: 0.0388\n",
            "====> Test set loss: 10.6132\n",
            "====> Epoch: 475 loss: 0.0362\n",
            "====> Test set loss: 10.6453\n",
            "====> Epoch: 476 loss: 0.0353\n",
            "====> Test set loss: 10.6298\n",
            "====> Epoch: 477 loss: 0.0330\n",
            "====> Test set loss: 10.7297\n",
            "====> Epoch: 478 loss: 0.0331\n",
            "====> Test set loss: 10.6976\n",
            "====> Epoch: 479 loss: 0.0376\n",
            "====> Test set loss: 10.6692\n",
            "====> Epoch: 480 loss: 0.0330\n",
            "====> Test set loss: 10.6588\n",
            "====> Epoch: 481 loss: 0.0375\n",
            "====> Test set loss: 10.6653\n",
            "====> Epoch: 482 loss: 0.0362\n",
            "====> Test set loss: 10.6545\n",
            "====> Epoch: 483 loss: 0.0334\n",
            "====> Test set loss: 10.6224\n",
            "====> Epoch: 484 loss: 0.0307\n",
            "====> Test set loss: 10.7097\n",
            "====> Epoch: 485 loss: 0.0339\n",
            "====> Test set loss: 10.6160\n",
            "====> Epoch: 486 loss: 0.0398\n",
            "====> Test set loss: 10.6854\n",
            "====> Epoch: 487 loss: 0.0393\n",
            "====> Test set loss: 10.6945\n",
            "====> Epoch: 488 loss: 0.0406\n",
            "====> Test set loss: 10.6738\n",
            "====> Epoch: 489 loss: 0.0417\n",
            "====> Test set loss: 10.6772\n",
            "====> Epoch: 490 loss: 0.0371\n",
            "====> Test set loss: 10.7137\n",
            "====> Epoch: 491 loss: 0.0311\n",
            "====> Test set loss: 10.7203\n",
            "====> Epoch: 492 loss: 0.0314\n",
            "====> Test set loss: 10.7380\n",
            "====> Epoch: 493 loss: 0.0328\n",
            "====> Test set loss: 10.6052\n",
            "====> Epoch: 494 loss: 0.0332\n",
            "====> Test set loss: 10.6221\n",
            "====> Epoch: 495 loss: 0.0328\n",
            "====> Test set loss: 10.6169\n",
            "====> Epoch: 496 loss: 0.0337\n",
            "====> Test set loss: 10.5904\n",
            "====> Epoch: 497 loss: 0.0341\n",
            "====> Test set loss: 10.6522\n",
            "====> Epoch: 498 loss: 0.0347\n",
            "====> Test set loss: 10.6463\n",
            "====> Epoch: 499 loss: 0.0359\n",
            "====> Test set loss: 10.7221\n",
            "====> Epoch: 500 loss: 0.0358\n",
            "====> Test set loss: 10.7322\n",
            "====> Test set loss: 10.7322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iZjJWmRK3-w"
      },
      "source": [
        "y3 = y_pred"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "rOW8COQXK6cc",
        "outputId": "88a192f5-c0bf-4187-e8f3-5d6dcf5678d9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa4b16c2e50>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VxOL0-pK7DR",
        "outputId": "9075f8c5-fca0-44af-a687-5000109ac7b6"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o, y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.1440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjhue5WsK79J"
      },
      "source": [
        "#"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CqjBeSeK8Qx",
        "outputId": "cd9b2224-286e-4a69-cc65-b2ddee3cf5e5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 364)\n",
        "        self.fc2 = nn.Linear(364, 182)\n",
        "        self.fc3 = nn.Linear(182, 91)\n",
        "        self.fc4 = nn.Linear(91, 3)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=364, bias=True)\n",
            "  (fc2): Linear(in_features=364, out_features=182, bias=True)\n",
            "  (fc3): Linear(in_features=182, out_features=91, bias=True)\n",
            "  (fc4): Linear(in_features=91, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL7Xw1e1eOse",
        "outputId": "cbeb7f26-c89b-4334-affa-e9d1457d168e"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "349079"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANYqgRBHLErV",
        "outputId": "714e4df6-4e85-4569-b543-75150ffd5444"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = epochs\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 43.8439\n",
            "====> Test set loss: 24.7513\n",
            "====> Epoch: 2 loss: 22.7750\n",
            "====> Test set loss: 22.3718\n",
            "====> Epoch: 3 loss: 17.5295\n",
            "====> Test set loss: 19.2776\n",
            "====> Epoch: 4 loss: 12.5472\n",
            "====> Test set loss: 16.7139\n",
            "====> Epoch: 5 loss: 8.6386\n",
            "====> Test set loss: 15.5457\n",
            "====> Epoch: 6 loss: 5.9462\n",
            "====> Test set loss: 14.0366\n",
            "====> Epoch: 7 loss: 4.0308\n",
            "====> Test set loss: 12.7485\n",
            "====> Epoch: 8 loss: 2.7058\n",
            "====> Test set loss: 12.3735\n",
            "====> Epoch: 9 loss: 1.9154\n",
            "====> Test set loss: 11.8467\n",
            "====> Epoch: 10 loss: 1.3563\n",
            "====> Test set loss: 11.6345\n",
            "====> Epoch: 11 loss: 0.9619\n",
            "====> Test set loss: 11.6208\n",
            "====> Epoch: 12 loss: 0.6509\n",
            "====> Test set loss: 11.6767\n",
            "====> Epoch: 13 loss: 0.4391\n",
            "====> Test set loss: 11.7582\n",
            "====> Epoch: 14 loss: 0.2845\n",
            "====> Test set loss: 11.6964\n",
            "====> Epoch: 15 loss: 0.1846\n",
            "====> Test set loss: 11.7698\n",
            "====> Epoch: 16 loss: 0.1279\n",
            "====> Test set loss: 11.7745\n",
            "====> Epoch: 17 loss: 0.1368\n",
            "====> Test set loss: 11.8706\n",
            "====> Epoch: 18 loss: 0.2360\n",
            "====> Test set loss: 11.9594\n",
            "====> Epoch: 19 loss: 0.3921\n",
            "====> Test set loss: 11.9349\n",
            "====> Epoch: 20 loss: 0.3548\n",
            "====> Test set loss: 11.7946\n",
            "====> Epoch: 21 loss: 0.2584\n",
            "====> Test set loss: 11.7527\n",
            "====> Epoch: 22 loss: 0.1863\n",
            "====> Test set loss: 11.7477\n",
            "====> Epoch: 23 loss: 0.1571\n",
            "====> Test set loss: 11.9078\n",
            "====> Epoch: 24 loss: 0.1457\n",
            "====> Test set loss: 11.9906\n",
            "====> Epoch: 25 loss: 0.1781\n",
            "====> Test set loss: 11.8159\n",
            "====> Epoch: 26 loss: 0.2174\n",
            "====> Test set loss: 11.9728\n",
            "====> Epoch: 27 loss: 0.2650\n",
            "====> Test set loss: 11.7077\n",
            "====> Epoch: 28 loss: 0.2661\n",
            "====> Test set loss: 11.9525\n",
            "====> Epoch: 29 loss: 0.2150\n",
            "====> Test set loss: 11.8041\n",
            "====> Epoch: 30 loss: 0.1626\n",
            "====> Test set loss: 11.7286\n",
            "====> Epoch: 31 loss: 0.1709\n",
            "====> Test set loss: 11.7168\n",
            "====> Epoch: 32 loss: 0.1837\n",
            "====> Test set loss: 11.7508\n",
            "====> Epoch: 33 loss: 0.1888\n",
            "====> Test set loss: 11.8974\n",
            "====> Epoch: 34 loss: 0.2031\n",
            "====> Test set loss: 11.8501\n",
            "====> Epoch: 35 loss: 0.2121\n",
            "====> Test set loss: 11.8440\n",
            "====> Epoch: 36 loss: 0.2058\n",
            "====> Test set loss: 12.0001\n",
            "====> Epoch: 37 loss: 0.1836\n",
            "====> Test set loss: 11.5435\n",
            "====> Epoch: 38 loss: 0.1771\n",
            "====> Test set loss: 11.5790\n",
            "====> Epoch: 39 loss: 0.1727\n",
            "====> Test set loss: 11.7646\n",
            "====> Epoch: 40 loss: 0.1766\n",
            "====> Test set loss: 11.8192\n",
            "====> Epoch: 41 loss: 0.1711\n",
            "====> Test set loss: 11.6160\n",
            "====> Epoch: 42 loss: 0.1619\n",
            "====> Test set loss: 11.7182\n",
            "====> Epoch: 43 loss: 0.1734\n",
            "====> Test set loss: 11.7158\n",
            "====> Epoch: 44 loss: 0.1749\n",
            "====> Test set loss: 11.5772\n",
            "====> Epoch: 45 loss: 0.1720\n",
            "====> Test set loss: 11.6315\n",
            "====> Epoch: 46 loss: 0.1480\n",
            "====> Test set loss: 11.7148\n",
            "====> Epoch: 47 loss: 0.1484\n",
            "====> Test set loss: 11.5945\n",
            "====> Epoch: 48 loss: 0.1621\n",
            "====> Test set loss: 11.7040\n",
            "====> Epoch: 49 loss: 0.1790\n",
            "====> Test set loss: 11.5360\n",
            "====> Epoch: 50 loss: 0.1833\n",
            "====> Test set loss: 11.5195\n",
            "====> Epoch: 51 loss: 0.1599\n",
            "====> Test set loss: 11.6396\n",
            "====> Epoch: 52 loss: 0.1487\n",
            "====> Test set loss: 11.6557\n",
            "====> Epoch: 53 loss: 0.1376\n",
            "====> Test set loss: 11.5189\n",
            "====> Epoch: 54 loss: 0.1520\n",
            "====> Test set loss: 11.7135\n",
            "====> Epoch: 55 loss: 0.1660\n",
            "====> Test set loss: 11.6247\n",
            "====> Epoch: 56 loss: 0.1606\n",
            "====> Test set loss: 11.5745\n",
            "====> Epoch: 57 loss: 0.1548\n",
            "====> Test set loss: 11.6450\n",
            "====> Epoch: 58 loss: 0.1436\n",
            "====> Test set loss: 11.6004\n",
            "====> Epoch: 59 loss: 0.1568\n",
            "====> Test set loss: 11.5514\n",
            "====> Epoch: 60 loss: 0.1468\n",
            "====> Test set loss: 11.4646\n",
            "====> Epoch: 61 loss: 0.1470\n",
            "====> Test set loss: 11.4044\n",
            "====> Epoch: 62 loss: 0.1425\n",
            "====> Test set loss: 11.4714\n",
            "====> Epoch: 63 loss: 0.1307\n",
            "====> Test set loss: 11.5915\n",
            "====> Epoch: 64 loss: 0.1243\n",
            "====> Test set loss: 11.4190\n",
            "====> Epoch: 65 loss: 0.1342\n",
            "====> Test set loss: 11.5067\n",
            "====> Epoch: 66 loss: 0.1428\n",
            "====> Test set loss: 11.4052\n",
            "====> Epoch: 67 loss: 0.1578\n",
            "====> Test set loss: 11.5310\n",
            "====> Epoch: 68 loss: 0.1488\n",
            "====> Test set loss: 11.5810\n",
            "====> Epoch: 69 loss: 0.1430\n",
            "====> Test set loss: 11.5413\n",
            "====> Epoch: 70 loss: 0.1347\n",
            "====> Test set loss: 11.4440\n",
            "====> Epoch: 71 loss: 0.1250\n",
            "====> Test set loss: 11.4084\n",
            "====> Epoch: 72 loss: 0.1282\n",
            "====> Test set loss: 11.4056\n",
            "====> Epoch: 73 loss: 0.1208\n",
            "====> Test set loss: 11.4657\n",
            "====> Epoch: 74 loss: 0.1302\n",
            "====> Test set loss: 11.4734\n",
            "====> Epoch: 75 loss: 0.1406\n",
            "====> Test set loss: 11.3686\n",
            "====> Epoch: 76 loss: 0.1416\n",
            "====> Test set loss: 11.4281\n",
            "====> Epoch: 77 loss: 0.1297\n",
            "====> Test set loss: 11.4395\n",
            "====> Epoch: 78 loss: 0.1213\n",
            "====> Test set loss: 11.3159\n",
            "====> Epoch: 79 loss: 0.1252\n",
            "====> Test set loss: 11.3209\n",
            "====> Epoch: 80 loss: 0.1209\n",
            "====> Test set loss: 11.3612\n",
            "====> Epoch: 81 loss: 0.1211\n",
            "====> Test set loss: 11.3604\n",
            "====> Epoch: 82 loss: 0.1427\n",
            "====> Test set loss: 11.3252\n",
            "====> Epoch: 83 loss: 0.1389\n",
            "====> Test set loss: 11.3349\n",
            "====> Epoch: 84 loss: 0.1325\n",
            "====> Test set loss: 11.3258\n",
            "====> Epoch: 85 loss: 0.1234\n",
            "====> Test set loss: 11.2560\n",
            "====> Epoch: 86 loss: 0.1135\n",
            "====> Test set loss: 11.3693\n",
            "====> Epoch: 87 loss: 0.1084\n",
            "====> Test set loss: 11.2315\n",
            "====> Epoch: 88 loss: 0.1203\n",
            "====> Test set loss: 11.3538\n",
            "====> Epoch: 89 loss: 0.1367\n",
            "====> Test set loss: 11.4548\n",
            "====> Epoch: 90 loss: 0.1261\n",
            "====> Test set loss: 11.3965\n",
            "====> Epoch: 91 loss: 0.1192\n",
            "====> Test set loss: 11.2196\n",
            "====> Epoch: 92 loss: 0.1011\n",
            "====> Test set loss: 11.2372\n",
            "====> Epoch: 93 loss: 0.1062\n",
            "====> Test set loss: 11.3355\n",
            "====> Epoch: 94 loss: 0.1073\n",
            "====> Test set loss: 11.1504\n",
            "====> Epoch: 95 loss: 0.1209\n",
            "====> Test set loss: 11.3124\n",
            "====> Epoch: 96 loss: 0.1232\n",
            "====> Test set loss: 11.1313\n",
            "====> Epoch: 97 loss: 0.1153\n",
            "====> Test set loss: 11.3071\n",
            "====> Epoch: 98 loss: 0.1161\n",
            "====> Test set loss: 11.1663\n",
            "====> Epoch: 99 loss: 0.1077\n",
            "====> Test set loss: 11.2836\n",
            "====> Epoch: 100 loss: 0.1107\n",
            "====> Test set loss: 11.1679\n",
            "====> Epoch: 101 loss: 0.1142\n",
            "====> Test set loss: 11.3057\n",
            "====> Epoch: 102 loss: 0.1163\n",
            "====> Test set loss: 11.1534\n",
            "====> Epoch: 103 loss: 0.1136\n",
            "====> Test set loss: 11.3084\n",
            "====> Epoch: 104 loss: 0.1069\n",
            "====> Test set loss: 11.1878\n",
            "====> Epoch: 105 loss: 0.1001\n",
            "====> Test set loss: 11.1901\n",
            "====> Epoch: 106 loss: 0.1108\n",
            "====> Test set loss: 11.1324\n",
            "====> Epoch: 107 loss: 0.1068\n",
            "====> Test set loss: 11.2358\n",
            "====> Epoch: 108 loss: 0.1094\n",
            "====> Test set loss: 11.2381\n",
            "====> Epoch: 109 loss: 0.1047\n",
            "====> Test set loss: 11.2282\n",
            "====> Epoch: 110 loss: 0.0973\n",
            "====> Test set loss: 11.1474\n",
            "====> Epoch: 111 loss: 0.0979\n",
            "====> Test set loss: 11.1512\n",
            "====> Epoch: 112 loss: 0.0976\n",
            "====> Test set loss: 11.1072\n",
            "====> Epoch: 113 loss: 0.0993\n",
            "====> Test set loss: 11.1393\n",
            "====> Epoch: 114 loss: 0.1026\n",
            "====> Test set loss: 11.0487\n",
            "====> Epoch: 115 loss: 0.1177\n",
            "====> Test set loss: 11.2482\n",
            "====> Epoch: 116 loss: 0.1174\n",
            "====> Test set loss: 11.1502\n",
            "====> Epoch: 117 loss: 0.1032\n",
            "====> Test set loss: 11.1978\n",
            "====> Epoch: 118 loss: 0.0939\n",
            "====> Test set loss: 11.1019\n",
            "====> Epoch: 119 loss: 0.0952\n",
            "====> Test set loss: 11.1429\n",
            "====> Epoch: 120 loss: 0.0981\n",
            "====> Test set loss: 11.1242\n",
            "====> Epoch: 121 loss: 0.1039\n",
            "====> Test set loss: 11.1106\n",
            "====> Epoch: 122 loss: 0.0957\n",
            "====> Test set loss: 11.0363\n",
            "====> Epoch: 123 loss: 0.0970\n",
            "====> Test set loss: 11.1278\n",
            "====> Epoch: 124 loss: 0.1154\n",
            "====> Test set loss: 10.9972\n",
            "====> Epoch: 125 loss: 0.1118\n",
            "====> Test set loss: 11.0929\n",
            "====> Epoch: 126 loss: 0.0983\n",
            "====> Test set loss: 11.0107\n",
            "====> Epoch: 127 loss: 0.0860\n",
            "====> Test set loss: 11.1377\n",
            "====> Epoch: 128 loss: 0.0862\n",
            "====> Test set loss: 11.0287\n",
            "====> Epoch: 129 loss: 0.0883\n",
            "====> Test set loss: 11.0405\n",
            "====> Epoch: 130 loss: 0.1003\n",
            "====> Test set loss: 11.0228\n",
            "====> Epoch: 131 loss: 0.1091\n",
            "====> Test set loss: 11.0600\n",
            "====> Epoch: 132 loss: 0.0942\n",
            "====> Test set loss: 11.0147\n",
            "====> Epoch: 133 loss: 0.0877\n",
            "====> Test set loss: 10.9735\n",
            "====> Epoch: 134 loss: 0.1000\n",
            "====> Test set loss: 11.1898\n",
            "====> Epoch: 135 loss: 0.0967\n",
            "====> Test set loss: 11.0579\n",
            "====> Epoch: 136 loss: 0.0889\n",
            "====> Test set loss: 11.1782\n",
            "====> Epoch: 137 loss: 0.0847\n",
            "====> Test set loss: 10.9990\n",
            "====> Epoch: 138 loss: 0.0941\n",
            "====> Test set loss: 10.9458\n",
            "====> Epoch: 139 loss: 0.0963\n",
            "====> Test set loss: 11.0205\n",
            "====> Epoch: 140 loss: 0.0990\n",
            "====> Test set loss: 11.0266\n",
            "====> Epoch: 141 loss: 0.0905\n",
            "====> Test set loss: 10.9177\n",
            "====> Epoch: 142 loss: 0.0870\n",
            "====> Test set loss: 11.1012\n",
            "====> Epoch: 143 loss: 0.0882\n",
            "====> Test set loss: 10.9661\n",
            "====> Epoch: 144 loss: 0.0859\n",
            "====> Test set loss: 11.0126\n",
            "====> Epoch: 145 loss: 0.0855\n",
            "====> Test set loss: 10.8915\n",
            "====> Epoch: 146 loss: 0.0953\n",
            "====> Test set loss: 11.0099\n",
            "====> Epoch: 147 loss: 0.1030\n",
            "====> Test set loss: 10.9180\n",
            "====> Epoch: 148 loss: 0.0945\n",
            "====> Test set loss: 11.0751\n",
            "====> Epoch: 149 loss: 0.0817\n",
            "====> Test set loss: 10.9564\n",
            "====> Epoch: 150 loss: 0.0781\n",
            "====> Test set loss: 10.9526\n",
            "====> Epoch: 151 loss: 0.0840\n",
            "====> Test set loss: 10.9898\n",
            "====> Epoch: 152 loss: 0.0916\n",
            "====> Test set loss: 10.9272\n",
            "====> Epoch: 153 loss: 0.0857\n",
            "====> Test set loss: 10.8736\n",
            "====> Epoch: 154 loss: 0.0847\n",
            "====> Test set loss: 11.0167\n",
            "====> Epoch: 155 loss: 0.0938\n",
            "====> Test set loss: 10.8891\n",
            "====> Epoch: 156 loss: 0.0862\n",
            "====> Test set loss: 10.9037\n",
            "====> Epoch: 157 loss: 0.0786\n",
            "====> Test set loss: 10.8887\n",
            "====> Epoch: 158 loss: 0.0757\n",
            "====> Test set loss: 10.8259\n",
            "====> Epoch: 159 loss: 0.0775\n",
            "====> Test set loss: 11.0147\n",
            "====> Epoch: 160 loss: 0.0887\n",
            "====> Test set loss: 10.8468\n",
            "====> Epoch: 161 loss: 0.1064\n",
            "====> Test set loss: 10.8921\n",
            "====> Epoch: 162 loss: 0.0946\n",
            "====> Test set loss: 10.9803\n",
            "====> Epoch: 163 loss: 0.0859\n",
            "====> Test set loss: 10.8678\n",
            "====> Epoch: 164 loss: 0.0884\n",
            "====> Test set loss: 11.0279\n",
            "====> Epoch: 165 loss: 0.0882\n",
            "====> Test set loss: 11.0460\n",
            "====> Epoch: 166 loss: 0.0820\n",
            "====> Test set loss: 10.7577\n",
            "====> Epoch: 167 loss: 0.0773\n",
            "====> Test set loss: 10.9026\n",
            "====> Epoch: 168 loss: 0.0774\n",
            "====> Test set loss: 10.9586\n",
            "====> Epoch: 169 loss: 0.0728\n",
            "====> Test set loss: 10.9731\n",
            "====> Epoch: 170 loss: 0.0765\n",
            "====> Test set loss: 10.9890\n",
            "====> Epoch: 171 loss: 0.0872\n",
            "====> Test set loss: 10.8593\n",
            "====> Epoch: 172 loss: 0.0870\n",
            "====> Test set loss: 10.9410\n",
            "====> Epoch: 173 loss: 0.0981\n",
            "====> Test set loss: 10.7028\n",
            "====> Epoch: 174 loss: 0.0847\n",
            "====> Test set loss: 10.9085\n",
            "====> Epoch: 175 loss: 0.0683\n",
            "====> Test set loss: 10.9612\n",
            "====> Epoch: 176 loss: 0.0686\n",
            "====> Test set loss: 10.8476\n",
            "====> Epoch: 177 loss: 0.0709\n",
            "====> Test set loss: 10.7618\n",
            "====> Epoch: 178 loss: 0.0737\n",
            "====> Test set loss: 10.9200\n",
            "====> Epoch: 179 loss: 0.0801\n",
            "====> Test set loss: 10.8708\n",
            "====> Epoch: 180 loss: 0.0742\n",
            "====> Test set loss: 10.8197\n",
            "====> Epoch: 181 loss: 0.0802\n",
            "====> Test set loss: 10.8587\n",
            "====> Epoch: 182 loss: 0.0768\n",
            "====> Test set loss: 10.9385\n",
            "====> Epoch: 183 loss: 0.0720\n",
            "====> Test set loss: 10.8715\n",
            "====> Epoch: 184 loss: 0.0734\n",
            "====> Test set loss: 10.8933\n",
            "====> Epoch: 185 loss: 0.0780\n",
            "====> Test set loss: 10.9330\n",
            "====> Epoch: 186 loss: 0.0813\n",
            "====> Test set loss: 10.9748\n",
            "====> Epoch: 187 loss: 0.0737\n",
            "====> Test set loss: 10.8634\n",
            "====> Epoch: 188 loss: 0.0741\n",
            "====> Test set loss: 10.8773\n",
            "====> Epoch: 189 loss: 0.0830\n",
            "====> Test set loss: 10.8724\n",
            "====> Epoch: 190 loss: 0.0782\n",
            "====> Test set loss: 10.8627\n",
            "====> Epoch: 191 loss: 0.0734\n",
            "====> Test set loss: 10.8008\n",
            "====> Epoch: 192 loss: 0.0651\n",
            "====> Test set loss: 10.7702\n",
            "====> Epoch: 193 loss: 0.0689\n",
            "====> Test set loss: 10.8847\n",
            "====> Epoch: 194 loss: 0.0782\n",
            "====> Test set loss: 10.8393\n",
            "====> Epoch: 195 loss: 0.0830\n",
            "====> Test set loss: 10.9091\n",
            "====> Epoch: 196 loss: 0.0795\n",
            "====> Test set loss: 10.8790\n",
            "====> Epoch: 197 loss: 0.0683\n",
            "====> Test set loss: 10.8913\n",
            "====> Epoch: 198 loss: 0.0724\n",
            "====> Test set loss: 10.7197\n",
            "====> Epoch: 199 loss: 0.0711\n",
            "====> Test set loss: 10.9420\n",
            "====> Epoch: 200 loss: 0.0753\n",
            "====> Test set loss: 10.9768\n",
            "====> Epoch: 201 loss: 0.0759\n",
            "====> Test set loss: 10.8679\n",
            "====> Epoch: 202 loss: 0.0761\n",
            "====> Test set loss: 10.9946\n",
            "====> Epoch: 203 loss: 0.0722\n",
            "====> Test set loss: 10.8181\n",
            "====> Epoch: 204 loss: 0.0700\n",
            "====> Test set loss: 10.8944\n",
            "====> Epoch: 205 loss: 0.0711\n",
            "====> Test set loss: 10.8243\n",
            "====> Epoch: 206 loss: 0.0724\n",
            "====> Test set loss: 10.8771\n",
            "====> Epoch: 207 loss: 0.0746\n",
            "====> Test set loss: 10.8013\n",
            "====> Epoch: 208 loss: 0.0757\n",
            "====> Test set loss: 10.8251\n",
            "====> Epoch: 209 loss: 0.0765\n",
            "====> Test set loss: 10.8201\n",
            "====> Epoch: 210 loss: 0.0648\n",
            "====> Test set loss: 10.7374\n",
            "====> Epoch: 211 loss: 0.0627\n",
            "====> Test set loss: 10.7646\n",
            "====> Epoch: 212 loss: 0.0641\n",
            "====> Test set loss: 10.9130\n",
            "====> Epoch: 213 loss: 0.0733\n",
            "====> Test set loss: 10.7199\n",
            "====> Epoch: 214 loss: 0.0746\n",
            "====> Test set loss: 10.8201\n",
            "====> Epoch: 215 loss: 0.0760\n",
            "====> Test set loss: 10.8003\n",
            "====> Epoch: 216 loss: 0.0725\n",
            "====> Test set loss: 10.7666\n",
            "====> Epoch: 217 loss: 0.0654\n",
            "====> Test set loss: 10.7624\n",
            "====> Epoch: 218 loss: 0.0651\n",
            "====> Test set loss: 10.8305\n",
            "====> Epoch: 219 loss: 0.0649\n",
            "====> Test set loss: 10.8695\n",
            "====> Epoch: 220 loss: 0.0683\n",
            "====> Test set loss: 10.8941\n",
            "====> Epoch: 221 loss: 0.0701\n",
            "====> Test set loss: 10.7828\n",
            "====> Epoch: 222 loss: 0.0727\n",
            "====> Test set loss: 10.7618\n",
            "====> Epoch: 223 loss: 0.0688\n",
            "====> Test set loss: 10.7558\n",
            "====> Epoch: 224 loss: 0.0663\n",
            "====> Test set loss: 10.6741\n",
            "====> Epoch: 225 loss: 0.0709\n",
            "====> Test set loss: 10.6643\n",
            "====> Epoch: 226 loss: 0.0724\n",
            "====> Test set loss: 10.6649\n",
            "====> Epoch: 227 loss: 0.0769\n",
            "====> Test set loss: 10.8498\n",
            "====> Epoch: 228 loss: 0.0753\n",
            "====> Test set loss: 10.8060\n",
            "====> Epoch: 229 loss: 0.0675\n",
            "====> Test set loss: 10.8088\n",
            "====> Epoch: 230 loss: 0.0680\n",
            "====> Test set loss: 10.7224\n",
            "====> Epoch: 231 loss: 0.0650\n",
            "====> Test set loss: 10.8489\n",
            "====> Epoch: 232 loss: 0.0582\n",
            "====> Test set loss: 10.7283\n",
            "====> Epoch: 233 loss: 0.0583\n",
            "====> Test set loss: 10.7760\n",
            "====> Epoch: 234 loss: 0.0601\n",
            "====> Test set loss: 10.7433\n",
            "====> Epoch: 235 loss: 0.0738\n",
            "====> Test set loss: 10.6847\n",
            "====> Epoch: 236 loss: 0.0720\n",
            "====> Test set loss: 10.7420\n",
            "====> Epoch: 237 loss: 0.0637\n",
            "====> Test set loss: 10.7954\n",
            "====> Epoch: 238 loss: 0.0743\n",
            "====> Test set loss: 10.7399\n",
            "====> Epoch: 239 loss: 0.0689\n",
            "====> Test set loss: 10.7380\n",
            "====> Epoch: 240 loss: 0.0640\n",
            "====> Test set loss: 10.8176\n",
            "====> Epoch: 241 loss: 0.0601\n",
            "====> Test set loss: 10.7499\n",
            "====> Epoch: 242 loss: 0.0630\n",
            "====> Test set loss: 10.6662\n",
            "====> Epoch: 243 loss: 0.0652\n",
            "====> Test set loss: 10.8194\n",
            "====> Epoch: 244 loss: 0.0629\n",
            "====> Test set loss: 10.7161\n",
            "====> Epoch: 245 loss: 0.0658\n",
            "====> Test set loss: 10.7496\n",
            "====> Epoch: 246 loss: 0.0707\n",
            "====> Test set loss: 10.7100\n",
            "====> Epoch: 247 loss: 0.0728\n",
            "====> Test set loss: 10.7410\n",
            "====> Epoch: 248 loss: 0.0687\n",
            "====> Test set loss: 10.7983\n",
            "====> Epoch: 249 loss: 0.0579\n",
            "====> Test set loss: 10.6764\n",
            "====> Epoch: 250 loss: 0.0530\n",
            "====> Test set loss: 10.7113\n",
            "====> Epoch: 251 loss: 0.0541\n",
            "====> Test set loss: 10.8899\n",
            "====> Epoch: 252 loss: 0.0615\n",
            "====> Test set loss: 10.7513\n",
            "====> Epoch: 253 loss: 0.0694\n",
            "====> Test set loss: 10.7982\n",
            "====> Epoch: 254 loss: 0.0678\n",
            "====> Test set loss: 10.7319\n",
            "====> Epoch: 255 loss: 0.0615\n",
            "====> Test set loss: 10.7095\n",
            "====> Epoch: 256 loss: 0.0574\n",
            "====> Test set loss: 10.6575\n",
            "====> Epoch: 257 loss: 0.0607\n",
            "====> Test set loss: 10.7091\n",
            "====> Epoch: 258 loss: 0.0668\n",
            "====> Test set loss: 10.7534\n",
            "====> Epoch: 259 loss: 0.0684\n",
            "====> Test set loss: 10.7408\n",
            "====> Epoch: 260 loss: 0.0657\n",
            "====> Test set loss: 10.8159\n",
            "====> Epoch: 261 loss: 0.0624\n",
            "====> Test set loss: 10.7469\n",
            "====> Epoch: 262 loss: 0.0556\n",
            "====> Test set loss: 10.7668\n",
            "====> Epoch: 263 loss: 0.0515\n",
            "====> Test set loss: 10.7311\n",
            "====> Epoch: 264 loss: 0.0601\n",
            "====> Test set loss: 10.6187\n",
            "====> Epoch: 265 loss: 0.0652\n",
            "====> Test set loss: 10.8038\n",
            "====> Epoch: 266 loss: 0.0631\n",
            "====> Test set loss: 10.6725\n",
            "====> Epoch: 267 loss: 0.0626\n",
            "====> Test set loss: 10.7196\n",
            "====> Epoch: 268 loss: 0.0588\n",
            "====> Test set loss: 10.6785\n",
            "====> Epoch: 269 loss: 0.0635\n",
            "====> Test set loss: 10.7380\n",
            "====> Epoch: 270 loss: 0.0646\n",
            "====> Test set loss: 10.6292\n",
            "====> Epoch: 271 loss: 0.0586\n",
            "====> Test set loss: 10.7123\n",
            "====> Epoch: 272 loss: 0.0516\n",
            "====> Test set loss: 10.7030\n",
            "====> Epoch: 273 loss: 0.0543\n",
            "====> Test set loss: 10.6827\n",
            "====> Epoch: 274 loss: 0.0559\n",
            "====> Test set loss: 10.7051\n",
            "====> Epoch: 275 loss: 0.0620\n",
            "====> Test set loss: 10.6900\n",
            "====> Epoch: 276 loss: 0.0653\n",
            "====> Test set loss: 10.6673\n",
            "====> Epoch: 277 loss: 0.0639\n",
            "====> Test set loss: 10.6753\n",
            "====> Epoch: 278 loss: 0.0579\n",
            "====> Test set loss: 10.6330\n",
            "====> Epoch: 279 loss: 0.0569\n",
            "====> Test set loss: 10.6841\n",
            "====> Epoch: 280 loss: 0.0540\n",
            "====> Test set loss: 10.7025\n",
            "====> Epoch: 281 loss: 0.0550\n",
            "====> Test set loss: 10.5842\n",
            "====> Epoch: 282 loss: 0.0590\n",
            "====> Test set loss: 10.7757\n",
            "====> Epoch: 283 loss: 0.0607\n",
            "====> Test set loss: 10.7251\n",
            "====> Epoch: 284 loss: 0.0636\n",
            "====> Test set loss: 10.7792\n",
            "====> Epoch: 285 loss: 0.0583\n",
            "====> Test set loss: 10.5600\n",
            "====> Epoch: 286 loss: 0.0558\n",
            "====> Test set loss: 10.6975\n",
            "====> Epoch: 287 loss: 0.0515\n",
            "====> Test set loss: 10.6473\n",
            "====> Epoch: 288 loss: 0.0557\n",
            "====> Test set loss: 10.6397\n",
            "====> Epoch: 289 loss: 0.0628\n",
            "====> Test set loss: 10.6947\n",
            "====> Epoch: 290 loss: 0.0619\n",
            "====> Test set loss: 10.6010\n",
            "====> Epoch: 291 loss: 0.0603\n",
            "====> Test set loss: 10.6554\n",
            "====> Epoch: 292 loss: 0.0562\n",
            "====> Test set loss: 10.7244\n",
            "====> Epoch: 293 loss: 0.0565\n",
            "====> Test set loss: 10.5872\n",
            "====> Epoch: 294 loss: 0.0540\n",
            "====> Test set loss: 10.6747\n",
            "====> Epoch: 295 loss: 0.0527\n",
            "====> Test set loss: 10.6566\n",
            "====> Epoch: 296 loss: 0.0488\n",
            "====> Test set loss: 10.6493\n",
            "====> Epoch: 297 loss: 0.0578\n",
            "====> Test set loss: 10.6297\n",
            "====> Epoch: 298 loss: 0.0582\n",
            "====> Test set loss: 10.6889\n",
            "====> Epoch: 299 loss: 0.0615\n",
            "====> Test set loss: 10.6664\n",
            "====> Epoch: 300 loss: 0.0591\n",
            "====> Test set loss: 10.7361\n",
            "====> Epoch: 301 loss: 0.0545\n",
            "====> Test set loss: 10.7770\n",
            "====> Epoch: 302 loss: 0.0567\n",
            "====> Test set loss: 10.6509\n",
            "====> Epoch: 303 loss: 0.0493\n",
            "====> Test set loss: 10.5763\n",
            "====> Epoch: 304 loss: 0.0539\n",
            "====> Test set loss: 10.7427\n",
            "====> Epoch: 305 loss: 0.0534\n",
            "====> Test set loss: 10.6483\n",
            "====> Epoch: 306 loss: 0.0574\n",
            "====> Test set loss: 10.5965\n",
            "====> Epoch: 307 loss: 0.0552\n",
            "====> Test set loss: 10.5747\n",
            "====> Epoch: 308 loss: 0.0603\n",
            "====> Test set loss: 10.6311\n",
            "====> Epoch: 309 loss: 0.0580\n",
            "====> Test set loss: 10.7330\n",
            "====> Epoch: 310 loss: 0.0588\n",
            "====> Test set loss: 10.5983\n",
            "====> Epoch: 311 loss: 0.0604\n",
            "====> Test set loss: 10.7080\n",
            "====> Epoch: 312 loss: 0.0512\n",
            "====> Test set loss: 10.4996\n",
            "====> Epoch: 313 loss: 0.0445\n",
            "====> Test set loss: 10.6232\n",
            "====> Epoch: 314 loss: 0.0479\n",
            "====> Test set loss: 10.5791\n",
            "====> Epoch: 315 loss: 0.0544\n",
            "====> Test set loss: 10.6888\n",
            "====> Epoch: 316 loss: 0.0629\n",
            "====> Test set loss: 10.6430\n",
            "====> Epoch: 317 loss: 0.0581\n",
            "====> Test set loss: 10.5976\n",
            "====> Epoch: 318 loss: 0.0509\n",
            "====> Test set loss: 10.6229\n",
            "====> Epoch: 319 loss: 0.0532\n",
            "====> Test set loss: 10.6547\n",
            "====> Epoch: 320 loss: 0.0479\n",
            "====> Test set loss: 10.6132\n",
            "====> Epoch: 321 loss: 0.0506\n",
            "====> Test set loss: 10.5748\n",
            "====> Epoch: 322 loss: 0.0546\n",
            "====> Test set loss: 10.7041\n",
            "====> Epoch: 323 loss: 0.0540\n",
            "====> Test set loss: 10.6054\n",
            "====> Epoch: 324 loss: 0.0525\n",
            "====> Test set loss: 10.5410\n",
            "====> Epoch: 325 loss: 0.0502\n",
            "====> Test set loss: 10.6452\n",
            "====> Epoch: 326 loss: 0.0480\n",
            "====> Test set loss: 10.6250\n",
            "====> Epoch: 327 loss: 0.0522\n",
            "====> Test set loss: 10.5516\n",
            "====> Epoch: 328 loss: 0.0566\n",
            "====> Test set loss: 10.6874\n",
            "====> Epoch: 329 loss: 0.0570\n",
            "====> Test set loss: 10.5298\n",
            "====> Epoch: 330 loss: 0.0560\n",
            "====> Test set loss: 10.6064\n",
            "====> Epoch: 331 loss: 0.0498\n",
            "====> Test set loss: 10.6068\n",
            "====> Epoch: 332 loss: 0.0472\n",
            "====> Test set loss: 10.6131\n",
            "====> Epoch: 333 loss: 0.0480\n",
            "====> Test set loss: 10.5646\n",
            "====> Epoch: 334 loss: 0.0555\n",
            "====> Test set loss: 10.6006\n",
            "====> Epoch: 335 loss: 0.0562\n",
            "====> Test set loss: 10.5323\n",
            "====> Epoch: 336 loss: 0.0571\n",
            "====> Test set loss: 10.6714\n",
            "====> Epoch: 337 loss: 0.0548\n",
            "====> Test set loss: 10.5477\n",
            "====> Epoch: 338 loss: 0.0467\n",
            "====> Test set loss: 10.5688\n",
            "====> Epoch: 339 loss: 0.0475\n",
            "====> Test set loss: 10.6185\n",
            "====> Epoch: 340 loss: 0.0524\n",
            "====> Test set loss: 10.6197\n",
            "====> Epoch: 341 loss: 0.0535\n",
            "====> Test set loss: 10.5243\n",
            "====> Epoch: 342 loss: 0.0538\n",
            "====> Test set loss: 10.5690\n",
            "====> Epoch: 343 loss: 0.0542\n",
            "====> Test set loss: 10.6023\n",
            "====> Epoch: 344 loss: 0.0503\n",
            "====> Test set loss: 10.6092\n",
            "====> Epoch: 345 loss: 0.0488\n",
            "====> Test set loss: 10.6197\n",
            "====> Epoch: 346 loss: 0.0459\n",
            "====> Test set loss: 10.6811\n",
            "====> Epoch: 347 loss: 0.0494\n",
            "====> Test set loss: 10.5183\n",
            "====> Epoch: 348 loss: 0.0604\n",
            "====> Test set loss: 10.5320\n",
            "====> Epoch: 349 loss: 0.0646\n",
            "====> Test set loss: 10.5886\n",
            "====> Epoch: 350 loss: 0.0566\n",
            "====> Test set loss: 10.5384\n",
            "====> Epoch: 351 loss: 0.0465\n",
            "====> Test set loss: 10.5750\n",
            "====> Epoch: 352 loss: 0.0385\n",
            "====> Test set loss: 10.6128\n",
            "====> Epoch: 353 loss: 0.0416\n",
            "====> Test set loss: 10.5917\n",
            "====> Epoch: 354 loss: 0.0478\n",
            "====> Test set loss: 10.5878\n",
            "====> Epoch: 355 loss: 0.0490\n",
            "====> Test set loss: 10.4275\n",
            "====> Epoch: 356 loss: 0.0508\n",
            "====> Test set loss: 10.5167\n",
            "====> Epoch: 357 loss: 0.0563\n",
            "====> Test set loss: 10.5740\n",
            "====> Epoch: 358 loss: 0.0578\n",
            "====> Test set loss: 10.6828\n",
            "====> Epoch: 359 loss: 0.0531\n",
            "====> Test set loss: 10.6212\n",
            "====> Epoch: 360 loss: 0.0478\n",
            "====> Test set loss: 10.4676\n",
            "====> Epoch: 361 loss: 0.0453\n",
            "====> Test set loss: 10.6502\n",
            "====> Epoch: 362 loss: 0.0475\n",
            "====> Test set loss: 10.6716\n",
            "====> Epoch: 363 loss: 0.0545\n",
            "====> Test set loss: 10.5093\n",
            "====> Epoch: 364 loss: 0.0513\n",
            "====> Test set loss: 10.6136\n",
            "====> Epoch: 365 loss: 0.0440\n",
            "====> Test set loss: 10.6090\n",
            "====> Epoch: 366 loss: 0.0443\n",
            "====> Test set loss: 10.5502\n",
            "====> Epoch: 367 loss: 0.0506\n",
            "====> Test set loss: 10.6369\n",
            "====> Epoch: 368 loss: 0.0599\n",
            "====> Test set loss: 10.5860\n",
            "====> Epoch: 369 loss: 0.0505\n",
            "====> Test set loss: 10.5046\n",
            "====> Epoch: 370 loss: 0.0458\n",
            "====> Test set loss: 10.5181\n",
            "====> Epoch: 371 loss: 0.0436\n",
            "====> Test set loss: 10.5626\n",
            "====> Epoch: 372 loss: 0.0500\n",
            "====> Test set loss: 10.5512\n",
            "====> Epoch: 373 loss: 0.0529\n",
            "====> Test set loss: 10.6059\n",
            "====> Epoch: 374 loss: 0.0519\n",
            "====> Test set loss: 10.5575\n",
            "====> Test set loss: 10.5575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCHhgcQWLHj4"
      },
      "source": [
        "y4 = y_pred"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "0bmv6o9SLJyT",
        "outputId": "4e339395-e3b4-420d-a0a0-905f084597a2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1ec030e850>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWhNPgubLKhg",
        "outputId": "a7a33e7a-4486-4a63-f3b7-03a3048164ea"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o, y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.1349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKDQxV7ePjAC"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYR3QtqVLLUy",
        "outputId": "d43eef36-44d5-4b43-eeb6-4c13e5841c04"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(729, 364)\n",
        "        self.fc2 = nn.Linear(364, 182)\n",
        "        self.fc3 = nn.Linear(182, 91)\n",
        "        self.fc4 = nn.Linear(91, 45)\n",
        "        self.fc5 = nn.Linear(45, 3)\n",
        "\n",
        "        #nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc5.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc6.weight)\n",
        "        #nn.init.xavier_uniform_(self.fc7.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x.view(-1, 729))\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data, nn.init.calculate_gain('relu'))\n",
        "        #nn.init.xavier_uniform_(m.weight.data)\n",
        "        #nn.init.xavier_uniform_(m.bias.data)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = DNN().cuda()\n",
        "model.apply(weights_init)\n",
        "print(model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DNN(\n",
            "  (fc1): Linear(in_features=729, out_features=364, bias=True)\n",
            "  (fc2): Linear(in_features=364, out_features=182, bias=True)\n",
            "  (fc3): Linear(in_features=182, out_features=91, bias=True)\n",
            "  (fc4): Linear(in_features=91, out_features=45, bias=True)\n",
            "  (fc5): Linear(in_features=45, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zd4YGXLeR5o",
        "outputId": "7bf061ec-dc0d-4b65-ae73-04aa15e1c37d"
      },
      "source": [
        "# the number of trainable parameter\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "353081"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "raD3iaJPPn1K",
        "outputId": "8cddba5d-779b-438b-d18d-9d2c8e36cd45"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 500\n",
        "\n",
        "# for loss plot\n",
        "tloss = []\n",
        "vloss = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, tloss, e)\n",
        "    test(model, vloss)\n",
        "\n",
        "y_pred = test_pred(model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 loss: 45.1038\n",
            "====> Test set loss: 25.2175\n",
            "====> Epoch: 2 loss: 22.6731\n",
            "====> Test set loss: 22.2429\n",
            "====> Epoch: 3 loss: 16.5611\n",
            "====> Test set loss: 19.4697\n",
            "====> Epoch: 4 loss: 11.0210\n",
            "====> Test set loss: 17.0368\n",
            "====> Epoch: 5 loss: 6.9176\n",
            "====> Test set loss: 14.9611\n",
            "====> Epoch: 6 loss: 4.2835\n",
            "====> Test set loss: 13.9351\n",
            "====> Epoch: 7 loss: 2.7719\n",
            "====> Test set loss: 13.0586\n",
            "====> Epoch: 8 loss: 1.8763\n",
            "====> Test set loss: 12.7011\n",
            "====> Epoch: 9 loss: 1.3123\n",
            "====> Test set loss: 12.7341\n",
            "====> Epoch: 10 loss: 0.9314\n",
            "====> Test set loss: 12.7509\n",
            "====> Epoch: 11 loss: 0.6311\n",
            "====> Test set loss: 12.6601\n",
            "====> Epoch: 12 loss: 0.4202\n",
            "====> Test set loss: 12.7045\n",
            "====> Epoch: 13 loss: 0.3039\n",
            "====> Test set loss: 12.6758\n",
            "====> Epoch: 14 loss: 0.2529\n",
            "====> Test set loss: 12.5784\n",
            "====> Epoch: 15 loss: 0.3119\n",
            "====> Test set loss: 12.9973\n",
            "====> Epoch: 16 loss: 0.4970\n",
            "====> Test set loss: 12.6935\n",
            "====> Epoch: 17 loss: 0.5396\n",
            "====> Test set loss: 12.6682\n",
            "====> Epoch: 18 loss: 0.3846\n",
            "====> Test set loss: 12.5543\n",
            "====> Epoch: 19 loss: 0.2670\n",
            "====> Test set loss: 12.5394\n",
            "====> Epoch: 20 loss: 0.2173\n",
            "====> Test set loss: 12.6022\n",
            "====> Epoch: 21 loss: 0.2043\n",
            "====> Test set loss: 12.6300\n",
            "====> Epoch: 22 loss: 0.2316\n",
            "====> Test set loss: 12.5394\n",
            "====> Epoch: 23 loss: 0.3065\n",
            "====> Test set loss: 12.4577\n",
            "====> Epoch: 24 loss: 0.3697\n",
            "====> Test set loss: 12.7461\n",
            "====> Epoch: 25 loss: 0.3399\n",
            "====> Test set loss: 12.1948\n",
            "====> Epoch: 26 loss: 0.2951\n",
            "====> Test set loss: 12.6960\n",
            "====> Epoch: 27 loss: 0.2511\n",
            "====> Test set loss: 12.2934\n",
            "====> Epoch: 28 loss: 0.2322\n",
            "====> Test set loss: 12.5567\n",
            "====> Epoch: 29 loss: 0.2226\n",
            "====> Test set loss: 12.3888\n",
            "====> Epoch: 30 loss: 0.2428\n",
            "====> Test set loss: 12.6436\n",
            "====> Epoch: 31 loss: 0.2603\n",
            "====> Test set loss: 12.4034\n",
            "====> Epoch: 32 loss: 0.2698\n",
            "====> Test set loss: 12.4166\n",
            "====> Epoch: 33 loss: 0.2900\n",
            "====> Test set loss: 12.3425\n",
            "====> Epoch: 34 loss: 0.2629\n",
            "====> Test set loss: 12.3096\n",
            "====> Epoch: 35 loss: 0.2153\n",
            "====> Test set loss: 12.2757\n",
            "====> Epoch: 36 loss: 0.1966\n",
            "====> Test set loss: 12.3779\n",
            "====> Epoch: 37 loss: 0.2009\n",
            "====> Test set loss: 12.3065\n",
            "====> Epoch: 38 loss: 0.2180\n",
            "====> Test set loss: 12.3424\n",
            "====> Epoch: 39 loss: 0.2305\n",
            "====> Test set loss: 12.2152\n",
            "====> Epoch: 40 loss: 0.2158\n",
            "====> Test set loss: 12.2194\n",
            "====> Epoch: 41 loss: 0.2219\n",
            "====> Test set loss: 12.1336\n",
            "====> Epoch: 42 loss: 0.2096\n",
            "====> Test set loss: 12.2221\n",
            "====> Epoch: 43 loss: 0.2200\n",
            "====> Test set loss: 12.1967\n",
            "====> Epoch: 44 loss: 0.2207\n",
            "====> Test set loss: 12.1581\n",
            "====> Epoch: 45 loss: 0.2056\n",
            "====> Test set loss: 12.3226\n",
            "====> Epoch: 46 loss: 0.2015\n",
            "====> Test set loss: 12.1062\n",
            "====> Epoch: 47 loss: 0.1982\n",
            "====> Test set loss: 12.1459\n",
            "====> Epoch: 48 loss: 0.2010\n",
            "====> Test set loss: 12.1075\n",
            "====> Epoch: 49 loss: 0.1909\n",
            "====> Test set loss: 12.2196\n",
            "====> Epoch: 50 loss: 0.1828\n",
            "====> Test set loss: 12.0386\n",
            "====> Epoch: 51 loss: 0.1750\n",
            "====> Test set loss: 12.1225\n",
            "====> Epoch: 52 loss: 0.1864\n",
            "====> Test set loss: 12.1362\n",
            "====> Epoch: 53 loss: 0.1907\n",
            "====> Test set loss: 12.0082\n",
            "====> Epoch: 54 loss: 0.1905\n",
            "====> Test set loss: 12.2470\n",
            "====> Epoch: 55 loss: 0.1922\n",
            "====> Test set loss: 12.2433\n",
            "====> Epoch: 56 loss: 0.1903\n",
            "====> Test set loss: 12.1964\n",
            "====> Epoch: 57 loss: 0.1863\n",
            "====> Test set loss: 11.9778\n",
            "====> Epoch: 58 loss: 0.1685\n",
            "====> Test set loss: 12.0751\n",
            "====> Epoch: 59 loss: 0.1502\n",
            "====> Test set loss: 11.9765\n",
            "====> Epoch: 60 loss: 0.1467\n",
            "====> Test set loss: 12.0813\n",
            "====> Epoch: 61 loss: 0.1675\n",
            "====> Test set loss: 12.0358\n",
            "====> Epoch: 62 loss: 0.1775\n",
            "====> Test set loss: 11.9994\n",
            "====> Epoch: 63 loss: 0.1917\n",
            "====> Test set loss: 12.0080\n",
            "====> Epoch: 64 loss: 0.1790\n",
            "====> Test set loss: 12.0998\n",
            "====> Epoch: 65 loss: 0.1743\n",
            "====> Test set loss: 11.9702\n",
            "====> Epoch: 66 loss: 0.1634\n",
            "====> Test set loss: 11.9740\n",
            "====> Epoch: 67 loss: 0.1631\n",
            "====> Test set loss: 11.9389\n",
            "====> Epoch: 68 loss: 0.1644\n",
            "====> Test set loss: 11.8654\n",
            "====> Epoch: 69 loss: 0.1573\n",
            "====> Test set loss: 12.2144\n",
            "====> Epoch: 70 loss: 0.1622\n",
            "====> Test set loss: 11.7048\n",
            "====> Epoch: 71 loss: 0.1596\n",
            "====> Test set loss: 11.9589\n",
            "====> Epoch: 72 loss: 0.1483\n",
            "====> Test set loss: 11.9698\n",
            "====> Epoch: 73 loss: 0.1393\n",
            "====> Test set loss: 11.7671\n",
            "====> Epoch: 74 loss: 0.1502\n",
            "====> Test set loss: 11.9816\n",
            "====> Epoch: 75 loss: 0.1713\n",
            "====> Test set loss: 11.8806\n",
            "====> Epoch: 76 loss: 0.1725\n",
            "====> Test set loss: 11.8751\n",
            "====> Epoch: 77 loss: 0.1685\n",
            "====> Test set loss: 12.0114\n",
            "====> Epoch: 78 loss: 0.1424\n",
            "====> Test set loss: 11.8917\n",
            "====> Epoch: 79 loss: 0.1358\n",
            "====> Test set loss: 11.9176\n",
            "====> Epoch: 80 loss: 0.1393\n",
            "====> Test set loss: 11.9571\n",
            "====> Epoch: 81 loss: 0.1449\n",
            "====> Test set loss: 11.8763\n",
            "====> Epoch: 82 loss: 0.1568\n",
            "====> Test set loss: 11.7915\n",
            "====> Epoch: 83 loss: 0.1501\n",
            "====> Test set loss: 12.0761\n",
            "====> Epoch: 84 loss: 0.1313\n",
            "====> Test set loss: 11.9083\n",
            "====> Epoch: 85 loss: 0.1395\n",
            "====> Test set loss: 12.0029\n",
            "====> Epoch: 86 loss: 0.1504\n",
            "====> Test set loss: 11.8239\n",
            "====> Epoch: 87 loss: 0.1416\n",
            "====> Test set loss: 11.9133\n",
            "====> Epoch: 88 loss: 0.1403\n",
            "====> Test set loss: 11.9594\n",
            "====> Epoch: 89 loss: 0.1406\n",
            "====> Test set loss: 11.6811\n",
            "====> Epoch: 90 loss: 0.1402\n",
            "====> Test set loss: 11.9244\n",
            "====> Epoch: 91 loss: 0.1461\n",
            "====> Test set loss: 11.9855\n",
            "====> Epoch: 92 loss: 0.1358\n",
            "====> Test set loss: 11.7836\n",
            "====> Epoch: 93 loss: 0.1342\n",
            "====> Test set loss: 11.8996\n",
            "====> Epoch: 94 loss: 0.1404\n",
            "====> Test set loss: 11.6536\n",
            "====> Epoch: 95 loss: 0.1488\n",
            "====> Test set loss: 11.8172\n",
            "====> Epoch: 96 loss: 0.1409\n",
            "====> Test set loss: 11.8230\n",
            "====> Epoch: 97 loss: 0.1290\n",
            "====> Test set loss: 11.9057\n",
            "====> Epoch: 98 loss: 0.1242\n",
            "====> Test set loss: 11.9665\n",
            "====> Epoch: 99 loss: 0.1266\n",
            "====> Test set loss: 11.8449\n",
            "====> Epoch: 100 loss: 0.1254\n",
            "====> Test set loss: 11.8411\n",
            "====> Epoch: 101 loss: 0.1405\n",
            "====> Test set loss: 11.7508\n",
            "====> Epoch: 102 loss: 0.1360\n",
            "====> Test set loss: 11.7578\n",
            "====> Epoch: 103 loss: 0.1376\n",
            "====> Test set loss: 11.8987\n",
            "====> Epoch: 104 loss: 0.1404\n",
            "====> Test set loss: 11.7620\n",
            "====> Epoch: 105 loss: 0.1194\n",
            "====> Test set loss: 11.7295\n",
            "====> Epoch: 106 loss: 0.1042\n",
            "====> Test set loss: 11.8283\n",
            "====> Epoch: 107 loss: 0.1030\n",
            "====> Test set loss: 11.8505\n",
            "====> Epoch: 108 loss: 0.1255\n",
            "====> Test set loss: 11.7126\n",
            "====> Epoch: 109 loss: 0.1318\n",
            "====> Test set loss: 11.8624\n",
            "====> Epoch: 110 loss: 0.1325\n",
            "====> Test set loss: 11.8265\n",
            "====> Epoch: 111 loss: 0.1260\n",
            "====> Test set loss: 11.8617\n",
            "====> Epoch: 112 loss: 0.1210\n",
            "====> Test set loss: 11.7146\n",
            "====> Epoch: 113 loss: 0.1135\n",
            "====> Test set loss: 11.9017\n",
            "====> Epoch: 114 loss: 0.1051\n",
            "====> Test set loss: 11.6429\n",
            "====> Epoch: 115 loss: 0.1093\n",
            "====> Test set loss: 11.6876\n",
            "====> Epoch: 116 loss: 0.1124\n",
            "====> Test set loss: 11.5451\n",
            "====> Epoch: 117 loss: 0.1120\n",
            "====> Test set loss: 11.7110\n",
            "====> Epoch: 118 loss: 0.1117\n",
            "====> Test set loss: 11.7739\n",
            "====> Epoch: 119 loss: 0.1151\n",
            "====> Test set loss: 11.7705\n",
            "====> Epoch: 120 loss: 0.1205\n",
            "====> Test set loss: 11.7930\n",
            "====> Epoch: 121 loss: 0.1333\n",
            "====> Test set loss: 11.8459\n",
            "====> Epoch: 122 loss: 0.1388\n",
            "====> Test set loss: 11.8041\n",
            "====> Epoch: 123 loss: 0.1163\n",
            "====> Test set loss: 11.5672\n",
            "====> Epoch: 124 loss: 0.1054\n",
            "====> Test set loss: 11.7420\n",
            "====> Epoch: 125 loss: 0.0993\n",
            "====> Test set loss: 11.6164\n",
            "====> Epoch: 126 loss: 0.1084\n",
            "====> Test set loss: 11.7505\n",
            "====> Epoch: 127 loss: 0.1191\n",
            "====> Test set loss: 11.7049\n",
            "====> Epoch: 128 loss: 0.1146\n",
            "====> Test set loss: 11.6588\n",
            "====> Epoch: 129 loss: 0.1156\n",
            "====> Test set loss: 11.7348\n",
            "====> Epoch: 130 loss: 0.1157\n",
            "====> Test set loss: 11.6557\n",
            "====> Epoch: 131 loss: 0.1057\n",
            "====> Test set loss: 11.6886\n",
            "====> Epoch: 132 loss: 0.0994\n",
            "====> Test set loss: 11.6036\n",
            "====> Epoch: 133 loss: 0.1037\n",
            "====> Test set loss: 11.6708\n",
            "====> Epoch: 134 loss: 0.1084\n",
            "====> Test set loss: 11.6215\n",
            "====> Epoch: 135 loss: 0.1096\n",
            "====> Test set loss: 11.6850\n",
            "====> Epoch: 136 loss: 0.0989\n",
            "====> Test set loss: 11.6627\n",
            "====> Epoch: 137 loss: 0.0923\n",
            "====> Test set loss: 11.5683\n",
            "====> Epoch: 138 loss: 0.1034\n",
            "====> Test set loss: 11.7111\n",
            "====> Epoch: 139 loss: 0.1175\n",
            "====> Test set loss: 11.5777\n",
            "====> Epoch: 140 loss: 0.1208\n",
            "====> Test set loss: 11.5912\n",
            "====> Epoch: 141 loss: 0.1074\n",
            "====> Test set loss: 11.5576\n",
            "====> Epoch: 142 loss: 0.1004\n",
            "====> Test set loss: 11.6157\n",
            "====> Epoch: 143 loss: 0.0890\n",
            "====> Test set loss: 11.6413\n",
            "====> Epoch: 144 loss: 0.0901\n",
            "====> Test set loss: 11.6572\n",
            "====> Epoch: 145 loss: 0.1087\n",
            "====> Test set loss: 11.6428\n",
            "====> Epoch: 146 loss: 0.1199\n",
            "====> Test set loss: 11.6453\n",
            "====> Epoch: 147 loss: 0.1039\n",
            "====> Test set loss: 11.6169\n",
            "====> Epoch: 148 loss: 0.0921\n",
            "====> Test set loss: 11.6925\n",
            "====> Epoch: 149 loss: 0.0891\n",
            "====> Test set loss: 11.5151\n",
            "====> Epoch: 150 loss: 0.0924\n",
            "====> Test set loss: 11.4804\n",
            "====> Epoch: 151 loss: 0.1065\n",
            "====> Test set loss: 11.6194\n",
            "====> Epoch: 152 loss: 0.1195\n",
            "====> Test set loss: 11.6602\n",
            "====> Epoch: 153 loss: 0.1154\n",
            "====> Test set loss: 11.3917\n",
            "====> Epoch: 154 loss: 0.1000\n",
            "====> Test set loss: 11.5176\n",
            "====> Epoch: 155 loss: 0.0872\n",
            "====> Test set loss: 11.6680\n",
            "====> Epoch: 156 loss: 0.0858\n",
            "====> Test set loss: 11.5006\n",
            "====> Epoch: 157 loss: 0.0889\n",
            "====> Test set loss: 11.5426\n",
            "====> Epoch: 158 loss: 0.0894\n",
            "====> Test set loss: 11.6075\n",
            "====> Epoch: 159 loss: 0.1059\n",
            "====> Test set loss: 11.5613\n",
            "====> Epoch: 160 loss: 0.1107\n",
            "====> Test set loss: 11.5695\n",
            "====> Epoch: 161 loss: 0.0977\n",
            "====> Test set loss: 11.7846\n",
            "====> Epoch: 162 loss: 0.0866\n",
            "====> Test set loss: 11.5726\n",
            "====> Epoch: 163 loss: 0.0878\n",
            "====> Test set loss: 11.5745\n",
            "====> Epoch: 164 loss: 0.0941\n",
            "====> Test set loss: 11.5094\n",
            "====> Epoch: 165 loss: 0.1056\n",
            "====> Test set loss: 11.5536\n",
            "====> Epoch: 166 loss: 0.0921\n",
            "====> Test set loss: 11.6913\n",
            "====> Epoch: 167 loss: 0.0925\n",
            "====> Test set loss: 11.7157\n",
            "====> Epoch: 168 loss: 0.1011\n",
            "====> Test set loss: 11.6839\n",
            "====> Epoch: 169 loss: 0.0933\n",
            "====> Test set loss: 11.5899\n",
            "====> Epoch: 170 loss: 0.0964\n",
            "====> Test set loss: 11.5360\n",
            "====> Epoch: 171 loss: 0.0915\n",
            "====> Test set loss: 11.6367\n",
            "====> Epoch: 172 loss: 0.0818\n",
            "====> Test set loss: 11.6484\n",
            "====> Epoch: 173 loss: 0.0917\n",
            "====> Test set loss: 11.5807\n",
            "====> Epoch: 174 loss: 0.0987\n",
            "====> Test set loss: 11.4820\n",
            "====> Epoch: 175 loss: 0.0969\n",
            "====> Test set loss: 11.4777\n",
            "====> Epoch: 176 loss: 0.0914\n",
            "====> Test set loss: 11.4744\n",
            "====> Epoch: 177 loss: 0.0858\n",
            "====> Test set loss: 11.7285\n",
            "====> Epoch: 178 loss: 0.0879\n",
            "====> Test set loss: 11.5320\n",
            "====> Epoch: 179 loss: 0.0888\n",
            "====> Test set loss: 11.6083\n",
            "====> Epoch: 180 loss: 0.0792\n",
            "====> Test set loss: 11.4767\n",
            "====> Epoch: 181 loss: 0.0774\n",
            "====> Test set loss: 11.4969\n",
            "====> Epoch: 182 loss: 0.0861\n",
            "====> Test set loss: 11.5136\n",
            "====> Epoch: 183 loss: 0.0876\n",
            "====> Test set loss: 11.6344\n",
            "====> Epoch: 184 loss: 0.0953\n",
            "====> Test set loss: 11.6556\n",
            "====> Epoch: 185 loss: 0.0922\n",
            "====> Test set loss: 11.6304\n",
            "====> Epoch: 186 loss: 0.0826\n",
            "====> Test set loss: 11.6327\n",
            "====> Epoch: 187 loss: 0.0798\n",
            "====> Test set loss: 11.4443\n",
            "====> Epoch: 188 loss: 0.0798\n",
            "====> Test set loss: 11.4948\n",
            "====> Epoch: 189 loss: 0.0880\n",
            "====> Test set loss: 11.4807\n",
            "====> Epoch: 190 loss: 0.0972\n",
            "====> Test set loss: 11.3600\n",
            "====> Epoch: 191 loss: 0.0932\n",
            "====> Test set loss: 11.5025\n",
            "====> Epoch: 192 loss: 0.0807\n",
            "====> Test set loss: 11.5032\n",
            "====> Epoch: 193 loss: 0.0800\n",
            "====> Test set loss: 11.5058\n",
            "====> Epoch: 194 loss: 0.0862\n",
            "====> Test set loss: 11.4614\n",
            "====> Epoch: 195 loss: 0.0863\n",
            "====> Test set loss: 11.5616\n",
            "====> Epoch: 196 loss: 0.0804\n",
            "====> Test set loss: 11.3681\n",
            "====> Epoch: 197 loss: 0.0772\n",
            "====> Test set loss: 11.6087\n",
            "====> Epoch: 198 loss: 0.0844\n",
            "====> Test set loss: 11.5415\n",
            "====> Epoch: 199 loss: 0.0962\n",
            "====> Test set loss: 11.3920\n",
            "====> Epoch: 200 loss: 0.0900\n",
            "====> Test set loss: 11.4910\n",
            "====> Epoch: 201 loss: 0.0881\n",
            "====> Test set loss: 11.4893\n",
            "====> Epoch: 202 loss: 0.0802\n",
            "====> Test set loss: 11.5573\n",
            "====> Epoch: 203 loss: 0.0707\n",
            "====> Test set loss: 11.4629\n",
            "====> Epoch: 204 loss: 0.0650\n",
            "====> Test set loss: 11.3983\n",
            "====> Epoch: 205 loss: 0.0757\n",
            "====> Test set loss: 11.4314\n",
            "====> Epoch: 206 loss: 0.1030\n",
            "====> Test set loss: 11.4339\n",
            "====> Epoch: 207 loss: 0.1148\n",
            "====> Test set loss: 11.3425\n",
            "====> Epoch: 208 loss: 0.0957\n",
            "====> Test set loss: 11.4468\n",
            "====> Epoch: 209 loss: 0.0679\n",
            "====> Test set loss: 11.4660\n",
            "====> Epoch: 210 loss: 0.0613\n",
            "====> Test set loss: 11.4287\n",
            "====> Epoch: 211 loss: 0.0658\n",
            "====> Test set loss: 11.4148\n",
            "====> Epoch: 212 loss: 0.0705\n",
            "====> Test set loss: 11.5139\n",
            "====> Epoch: 213 loss: 0.0787\n",
            "====> Test set loss: 11.2985\n",
            "====> Epoch: 214 loss: 0.0816\n",
            "====> Test set loss: 11.4519\n",
            "====> Epoch: 215 loss: 0.0820\n",
            "====> Test set loss: 11.4664\n",
            "====> Epoch: 216 loss: 0.0786\n",
            "====> Test set loss: 11.5779\n",
            "====> Epoch: 217 loss: 0.0761\n",
            "====> Test set loss: 11.4548\n",
            "====> Epoch: 218 loss: 0.0773\n",
            "====> Test set loss: 11.3596\n",
            "====> Epoch: 219 loss: 0.0821\n",
            "====> Test set loss: 11.3793\n",
            "====> Epoch: 220 loss: 0.0932\n",
            "====> Test set loss: 11.4086\n",
            "====> Epoch: 221 loss: 0.0796\n",
            "====> Test set loss: 11.3264\n",
            "====> Epoch: 222 loss: 0.0721\n",
            "====> Test set loss: 11.3364\n",
            "====> Epoch: 223 loss: 0.0656\n",
            "====> Test set loss: 11.4233\n",
            "====> Epoch: 224 loss: 0.0630\n",
            "====> Test set loss: 11.3392\n",
            "====> Epoch: 225 loss: 0.0871\n",
            "====> Test set loss: 11.5336\n",
            "====> Epoch: 226 loss: 0.0986\n",
            "====> Test set loss: 11.2469\n",
            "====> Epoch: 227 loss: 0.0834\n",
            "====> Test set loss: 11.4481\n",
            "====> Epoch: 228 loss: 0.0710\n",
            "====> Test set loss: 11.3209\n",
            "====> Epoch: 229 loss: 0.0664\n",
            "====> Test set loss: 11.3910\n",
            "====> Epoch: 230 loss: 0.0627\n",
            "====> Test set loss: 11.4711\n",
            "====> Epoch: 231 loss: 0.0663\n",
            "====> Test set loss: 11.3597\n",
            "====> Epoch: 232 loss: 0.0741\n",
            "====> Test set loss: 11.4485\n",
            "====> Epoch: 233 loss: 0.0853\n",
            "====> Test set loss: 11.3477\n",
            "====> Epoch: 234 loss: 0.0819\n",
            "====> Test set loss: 11.5211\n",
            "====> Epoch: 235 loss: 0.0795\n",
            "====> Test set loss: 11.3772\n",
            "====> Epoch: 236 loss: 0.0723\n",
            "====> Test set loss: 11.5419\n",
            "====> Epoch: 237 loss: 0.0719\n",
            "====> Test set loss: 11.4682\n",
            "====> Epoch: 238 loss: 0.0746\n",
            "====> Test set loss: 11.4132\n",
            "====> Epoch: 239 loss: 0.0757\n",
            "====> Test set loss: 11.5648\n",
            "====> Epoch: 240 loss: 0.0713\n",
            "====> Test set loss: 11.3607\n",
            "====> Epoch: 241 loss: 0.0706\n",
            "====> Test set loss: 11.4250\n",
            "====> Epoch: 242 loss: 0.0747\n",
            "====> Test set loss: 11.4730\n",
            "====> Epoch: 243 loss: 0.0940\n",
            "====> Test set loss: 11.4070\n",
            "====> Epoch: 244 loss: 0.0868\n",
            "====> Test set loss: 11.3135\n",
            "====> Epoch: 245 loss: 0.0667\n",
            "====> Test set loss: 11.4433\n",
            "====> Epoch: 246 loss: 0.0655\n",
            "====> Test set loss: 11.3770\n",
            "====> Epoch: 247 loss: 0.0727\n",
            "====> Test set loss: 11.5652\n",
            "====> Epoch: 248 loss: 0.0789\n",
            "====> Test set loss: 11.3955\n",
            "====> Epoch: 249 loss: 0.0676\n",
            "====> Test set loss: 11.5448\n",
            "====> Epoch: 250 loss: 0.0642\n",
            "====> Test set loss: 11.3740\n",
            "====> Epoch: 251 loss: 0.0682\n",
            "====> Test set loss: 11.4940\n",
            "====> Epoch: 252 loss: 0.0701\n",
            "====> Test set loss: 11.3994\n",
            "====> Epoch: 253 loss: 0.0701\n",
            "====> Test set loss: 11.3759\n",
            "====> Epoch: 254 loss: 0.0721\n",
            "====> Test set loss: 11.3041\n",
            "====> Epoch: 255 loss: 0.0690\n",
            "====> Test set loss: 11.2736\n",
            "====> Epoch: 256 loss: 0.0702\n",
            "====> Test set loss: 11.4140\n",
            "====> Epoch: 257 loss: 0.0750\n",
            "====> Test set loss: 11.3847\n",
            "====> Epoch: 258 loss: 0.0693\n",
            "====> Test set loss: 11.5338\n",
            "====> Epoch: 259 loss: 0.0666\n",
            "====> Test set loss: 11.2628\n",
            "====> Epoch: 260 loss: 0.0661\n",
            "====> Test set loss: 11.4490\n",
            "====> Epoch: 261 loss: 0.0640\n",
            "====> Test set loss: 11.3389\n",
            "====> Epoch: 262 loss: 0.0643\n",
            "====> Test set loss: 11.3640\n",
            "====> Epoch: 263 loss: 0.0767\n",
            "====> Test set loss: 11.4696\n",
            "====> Epoch: 264 loss: 0.0761\n",
            "====> Test set loss: 11.5492\n",
            "====> Epoch: 265 loss: 0.0646\n",
            "====> Test set loss: 11.3286\n",
            "====> Epoch: 266 loss: 0.0658\n",
            "====> Test set loss: 11.4179\n",
            "====> Epoch: 267 loss: 0.0644\n",
            "====> Test set loss: 11.4158\n",
            "====> Epoch: 268 loss: 0.0613\n",
            "====> Test set loss: 11.4221\n",
            "====> Epoch: 269 loss: 0.0628\n",
            "====> Test set loss: 11.3453\n",
            "====> Epoch: 270 loss: 0.0761\n",
            "====> Test set loss: 11.2816\n",
            "====> Epoch: 271 loss: 0.0832\n",
            "====> Test set loss: 11.3593\n",
            "====> Epoch: 272 loss: 0.0718\n",
            "====> Test set loss: 11.3893\n",
            "====> Epoch: 273 loss: 0.0572\n",
            "====> Test set loss: 11.4358\n",
            "====> Epoch: 274 loss: 0.0532\n",
            "====> Test set loss: 11.2697\n",
            "====> Epoch: 275 loss: 0.0585\n",
            "====> Test set loss: 11.4830\n",
            "====> Epoch: 276 loss: 0.0631\n",
            "====> Test set loss: 11.3440\n",
            "====> Epoch: 277 loss: 0.0671\n",
            "====> Test set loss: 11.3941\n",
            "====> Epoch: 278 loss: 0.0728\n",
            "====> Test set loss: 11.2651\n",
            "====> Epoch: 279 loss: 0.0751\n",
            "====> Test set loss: 11.3601\n",
            "====> Epoch: 280 loss: 0.0755\n",
            "====> Test set loss: 11.4168\n",
            "====> Epoch: 281 loss: 0.0689\n",
            "====> Test set loss: 11.3473\n",
            "====> Epoch: 282 loss: 0.0642\n",
            "====> Test set loss: 11.3749\n",
            "====> Epoch: 283 loss: 0.0574\n",
            "====> Test set loss: 11.3747\n",
            "====> Epoch: 284 loss: 0.0546\n",
            "====> Test set loss: 11.3069\n",
            "====> Epoch: 285 loss: 0.0657\n",
            "====> Test set loss: 11.3760\n",
            "====> Epoch: 286 loss: 0.0689\n",
            "====> Test set loss: 11.4098\n",
            "====> Epoch: 287 loss: 0.0638\n",
            "====> Test set loss: 11.3933\n",
            "====> Epoch: 288 loss: 0.0597\n",
            "====> Test set loss: 11.3547\n",
            "====> Epoch: 289 loss: 0.0674\n",
            "====> Test set loss: 11.2046\n",
            "====> Epoch: 290 loss: 0.0850\n",
            "====> Test set loss: 11.3908\n",
            "====> Epoch: 291 loss: 0.0813\n",
            "====> Test set loss: 11.2915\n",
            "====> Epoch: 292 loss: 0.0625\n",
            "====> Test set loss: 11.2504\n",
            "====> Epoch: 293 loss: 0.0493\n",
            "====> Test set loss: 11.3465\n",
            "====> Epoch: 294 loss: 0.0484\n",
            "====> Test set loss: 11.2237\n",
            "====> Epoch: 295 loss: 0.0565\n",
            "====> Test set loss: 11.3787\n",
            "====> Epoch: 296 loss: 0.0726\n",
            "====> Test set loss: 11.2540\n",
            "====> Epoch: 297 loss: 0.0748\n",
            "====> Test set loss: 11.2814\n",
            "====> Epoch: 298 loss: 0.0595\n",
            "====> Test set loss: 11.4205\n",
            "====> Epoch: 299 loss: 0.0592\n",
            "====> Test set loss: 11.1911\n",
            "====> Epoch: 300 loss: 0.0637\n",
            "====> Test set loss: 11.3558\n",
            "====> Epoch: 301 loss: 0.0644\n",
            "====> Test set loss: 11.3003\n",
            "====> Epoch: 302 loss: 0.0683\n",
            "====> Test set loss: 11.2503\n",
            "====> Epoch: 303 loss: 0.0573\n",
            "====> Test set loss: 11.2960\n",
            "====> Epoch: 304 loss: 0.0550\n",
            "====> Test set loss: 11.2956\n",
            "====> Epoch: 305 loss: 0.0531\n",
            "====> Test set loss: 11.2153\n",
            "====> Epoch: 306 loss: 0.0625\n",
            "====> Test set loss: 11.3943\n",
            "====> Epoch: 307 loss: 0.0709\n",
            "====> Test set loss: 11.2933\n",
            "====> Epoch: 308 loss: 0.0668\n",
            "====> Test set loss: 11.3511\n",
            "====> Epoch: 309 loss: 0.0578\n",
            "====> Test set loss: 11.1820\n",
            "====> Epoch: 310 loss: 0.0552\n",
            "====> Test set loss: 11.2363\n",
            "====> Epoch: 311 loss: 0.0555\n",
            "====> Test set loss: 11.3318\n",
            "====> Epoch: 312 loss: 0.0584\n",
            "====> Test set loss: 11.3474\n",
            "====> Epoch: 313 loss: 0.0645\n",
            "====> Test set loss: 11.1988\n",
            "====> Epoch: 314 loss: 0.0635\n",
            "====> Test set loss: 11.4146\n",
            "====> Epoch: 315 loss: 0.0646\n",
            "====> Test set loss: 11.3771\n",
            "====> Epoch: 316 loss: 0.0603\n",
            "====> Test set loss: 11.3487\n",
            "====> Epoch: 317 loss: 0.0567\n",
            "====> Test set loss: 11.3051\n",
            "====> Epoch: 318 loss: 0.0616\n",
            "====> Test set loss: 11.2582\n",
            "====> Epoch: 319 loss: 0.0640\n",
            "====> Test set loss: 11.3080\n",
            "====> Epoch: 320 loss: 0.0665\n",
            "====> Test set loss: 11.2855\n",
            "====> Epoch: 321 loss: 0.0638\n",
            "====> Test set loss: 11.2030\n",
            "====> Epoch: 322 loss: 0.0580\n",
            "====> Test set loss: 11.3265\n",
            "====> Epoch: 323 loss: 0.0508\n",
            "====> Test set loss: 11.3590\n",
            "====> Epoch: 324 loss: 0.0515\n",
            "====> Test set loss: 11.1546\n",
            "====> Epoch: 325 loss: 0.0542\n",
            "====> Test set loss: 11.2832\n",
            "====> Epoch: 326 loss: 0.0576\n",
            "====> Test set loss: 11.3030\n",
            "====> Epoch: 327 loss: 0.0622\n",
            "====> Test set loss: 11.3043\n",
            "====> Epoch: 328 loss: 0.0682\n",
            "====> Test set loss: 11.2796\n",
            "====> Epoch: 329 loss: 0.0752\n",
            "====> Test set loss: 11.2344\n",
            "====> Epoch: 330 loss: 0.0653\n",
            "====> Test set loss: 11.3563\n",
            "====> Epoch: 331 loss: 0.0568\n",
            "====> Test set loss: 11.3238\n",
            "====> Epoch: 332 loss: 0.0498\n",
            "====> Test set loss: 11.2891\n",
            "====> Epoch: 333 loss: 0.0480\n",
            "====> Test set loss: 11.1717\n",
            "====> Epoch: 334 loss: 0.0554\n",
            "====> Test set loss: 11.2228\n",
            "====> Epoch: 335 loss: 0.0622\n",
            "====> Test set loss: 11.3601\n",
            "====> Epoch: 336 loss: 0.0695\n",
            "====> Test set loss: 11.2997\n",
            "====> Epoch: 337 loss: 0.0615\n",
            "====> Test set loss: 11.2941\n",
            "====> Epoch: 338 loss: 0.0581\n",
            "====> Test set loss: 11.0970\n",
            "====> Epoch: 339 loss: 0.0529\n",
            "====> Test set loss: 11.2362\n",
            "====> Epoch: 340 loss: 0.0535\n",
            "====> Test set loss: 11.1545\n",
            "====> Epoch: 341 loss: 0.0517\n",
            "====> Test set loss: 11.1713\n",
            "====> Epoch: 342 loss: 0.0502\n",
            "====> Test set loss: 11.2215\n",
            "====> Epoch: 343 loss: 0.0548\n",
            "====> Test set loss: 11.2095\n",
            "====> Epoch: 344 loss: 0.0619\n",
            "====> Test set loss: 11.2956\n",
            "====> Epoch: 345 loss: 0.0687\n",
            "====> Test set loss: 11.3015\n",
            "====> Epoch: 346 loss: 0.0653\n",
            "====> Test set loss: 11.2487\n",
            "====> Epoch: 347 loss: 0.0592\n",
            "====> Test set loss: 11.2811\n",
            "====> Epoch: 348 loss: 0.0507\n",
            "====> Test set loss: 11.2700\n",
            "====> Epoch: 349 loss: 0.0481\n",
            "====> Test set loss: 11.1535\n",
            "====> Epoch: 350 loss: 0.0485\n",
            "====> Test set loss: 11.2185\n",
            "====> Epoch: 351 loss: 0.0548\n",
            "====> Test set loss: 11.3154\n",
            "====> Epoch: 352 loss: 0.0597\n",
            "====> Test set loss: 11.2583\n",
            "====> Epoch: 353 loss: 0.0592\n",
            "====> Test set loss: 11.2977\n",
            "====> Epoch: 354 loss: 0.0525\n",
            "====> Test set loss: 11.2062\n",
            "====> Epoch: 355 loss: 0.0526\n",
            "====> Test set loss: 11.2439\n",
            "====> Epoch: 356 loss: 0.0572\n",
            "====> Test set loss: 11.1160\n",
            "====> Epoch: 357 loss: 0.0563\n",
            "====> Test set loss: 11.1571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7152d1477002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b712eb611429>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_losses, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "n2_80ow2Pp-r",
        "outputId": "20d0ab70-bcf4-4859-b2a2-92953786396b"
      },
      "source": [
        "y5 = y_pred"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1df15f889851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "8mYAg-EQPrc_",
        "outputId": "0dd3492f-a483-434b-eabe-8e5b2cf656dc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(np.array(vloss), label=\"valid\")\n",
        "plt.plot(np.array(tloss), label=\"train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa4b13e5e90>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE3aIczLPsqC",
        "outputId": "da8d3785-cf01-4cb3-a653-60a707864a03"
      },
      "source": [
        "# RMSE\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda().float(), y.cuda()       \n",
        "        o = model(x)\n",
        "        loss = torch.sqrt(loss_function(o, y))\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "print('====> Test set loss: {:.4f}'.format(test_loss / len(test_loader)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 3.2243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmmle83YPtXu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}